#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†è„šæœ¬ - å°†å„ç§æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºLLaMA Factoryæ”¯æŒçš„æ ¼å¼
ä½œè€…: DCUå®æˆ˜é¡¹ç›®ç»„
"""

import json
import pandas as pd
import argparse
import os
import re
from typing import List, Dict, Optional
from pathlib import Path
from sklearn.model_selection import train_test_split
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼è½¬æ¢ä¸ºAlpacaæ ¼å¼"""
    
    def __init__(self, quality_threshold: float = 0.8):
        self.quality_threshold = quality_threshold
        self.alpaca_format = {
            "instruction": "",
            "input": "",
            "output": ""
        }
    
    def csv_to_alpaca(self, csv_file: str, instruction_col: str, 
                     input_col: Optional[str] = None, output_col: str = None) -> List[Dict]:
        """
        å°†CSVæ–‡ä»¶è½¬æ¢ä¸ºAlpacaæ ¼å¼
        
        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            instruction_col: æŒ‡ä»¤åˆ—å
            input_col: è¾“å…¥åˆ—åï¼ˆå¯é€‰ï¼‰
            output_col: è¾“å‡ºåˆ—å
            
        Returns:
            è½¬æ¢åçš„Alpacaæ ¼å¼æ•°æ®åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¤„ç†CSVæ–‡ä»¶: {csv_file}")
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(csv_file, encoding='gbk')
        
        logger.info(f"CSVæ–‡ä»¶åŒ…å« {len(df)} è¡Œæ•°æ®")
        
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = [instruction_col]
        if output_col:
            required_cols.append(output_col)
        
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"CSVæ–‡ä»¶ä¸­ç¼ºå°‘åˆ—: {missing_cols}")
        
        alpaca_data = []
        skipped_count = 0
        
        for idx, row in df.iterrows():
            try:
                # å¤„ç†æŒ‡ä»¤
                instruction = str(row[instruction_col]).strip()
                if pd.isna(row[instruction_col]) or len(instruction) < 5:
                    skipped_count += 1
                    continue
                
                # å¤„ç†è¾“å…¥ï¼ˆå¯é€‰ï¼‰
                input_text = ""
                if input_col and input_col in df.columns and pd.notna(row[input_col]):
                    input_text = str(row[input_col]).strip()
                
                # å¤„ç†è¾“å‡º
                output_text = ""
                if output_col and pd.notna(row[output_col]):
                    output_text = str(row[output_col]).strip()
                
                # åˆ›å»ºAlpacaæ ¼å¼æ•°æ®
                item = {
                    "instruction": instruction,
                    "input": input_text,
                    "output": output_text
                }
                
                alpaca_data.append(item)
                
            except Exception as e:
                logger.warning(f"å¤„ç†ç¬¬ {idx} è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
                skipped_count += 1
                continue
        
        logger.info(f"è½¬æ¢å®Œæˆ: {len(alpaca_data)} æ¡æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡ {skipped_count} æ¡")
        return alpaca_data
    
    def validate_data(self, data: List[Dict]) -> List[Dict]:
        """
        æ•°æ®è´¨é‡éªŒè¯
        
        Args:
            data: å¾…éªŒè¯çš„æ•°æ®åˆ—è¡¨
            
        Returns:
            éªŒè¯é€šè¿‡çš„æ•°æ®åˆ—è¡¨
        """
        logger.info("å¼€å§‹æ•°æ®è´¨é‡éªŒè¯...")
        
        validated_data = []
        validation_stats = {
            "missing_fields": 0,
            "too_short": 0,
            "low_quality": 0,
            "duplicates": 0,
            "passed": 0
        }
        
        seen_instructions = set()
        
        for item in data:
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            if not all(key in item for key in ["instruction", "output"]):
                validation_stats["missing_fields"] += 1
                continue
            
            # é•¿åº¦æ£€æŸ¥
            if len(item["instruction"]) < 5 or len(item["output"]) < 5:
                validation_stats["too_short"] += 1
                continue
            
            # å»é‡æ£€æŸ¥
            instruction_key = item["instruction"].lower().strip()
            if instruction_key in seen_instructions:
                validation_stats["duplicates"] += 1
                continue
            seen_instructions.add(instruction_key)
            
            # å†…å®¹è´¨é‡æ£€æŸ¥
            if not self._quality_check(item):
                validation_stats["low_quality"] += 1
                continue
            
            validated_data.append(item)
            validation_stats["passed"] += 1
        
        # æ‰“å°éªŒè¯ç»Ÿè®¡
        logger.info("æ•°æ®éªŒè¯å®Œæˆ:")
        for key, count in validation_stats.items():
            logger.info(f"  {key}: {count}")
        
        logger.info(f"éªŒè¯é€šè¿‡ç‡: {validation_stats['passed']}/{len(data)} ({validation_stats['passed']/len(data)*100:.1f}%)")
        
        return validated_data
    
    def _quality_check(self, item: Dict) -> bool:
        """
        å†…å®¹è´¨é‡æ£€æŸ¥
        
        Args:
            item: å•æ¡æ•°æ®
            
        Returns:
            æ˜¯å¦é€šè¿‡è´¨é‡æ£€æŸ¥
        """
        text = item["instruction"] + " " + item.get("input", "") + " " + item["output"]
        
        # å»é™¤é‡å¤å­—ç¬¦è¿‡å¤šçš„å†…å®¹
        if len(text) > 0 and len(set(text)) / len(text) < 0.3:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯
        words = text.split()
        if len(words) < 10:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šç‰¹æ®Šå­—ç¬¦
        special_char_ratio = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text)) / len(text)
        if special_char_ratio > 0.3:
            return False
        
        # æ£€æŸ¥æŒ‡ä»¤å’Œè¾“å‡ºçš„ç›¸å…³æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        instruction_words = set(item["instruction"].lower().split())
        output_words = set(item["output"].lower().split())
        
        # å¦‚æœæŒ‡ä»¤å’Œè¾“å‡ºå®Œå…¨æ²¡æœ‰å…±åŒè¯æ±‡ï¼ˆé™¤äº†åœç”¨è¯ï¼‰ï¼Œå¯èƒ½è´¨é‡æœ‰é—®é¢˜
        stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬", "è¿™", "é‚£", "with", "the", "a", "an", "and", "or", "but", "is", "are", "was", "were"}
        instruction_meaningful = instruction_words - stop_words
        output_meaningful = output_words - stop_words
        
        if len(instruction_meaningful) > 3 and len(output_meaningful) > 3:
            overlap = len(instruction_meaningful & output_meaningful)
            if overlap == 0 and len(item["output"]) < 50:  # çŸ­å›ç­”ä¸”æ— å…³è”æ€§
                return False
        
        return True
    
    def augment_data(self, data: List[Dict], augment_ratio: float = 0.2) -> List[Dict]:
        """
        æ•°æ®å¢å¼º
        
        Args:
            data: åŸå§‹æ•°æ®
            augment_ratio: å¢å¼ºæ¯”ä¾‹
            
        Returns:
            å¢å¼ºåçš„æ•°æ®
        """
        if augment_ratio <= 0:
            return data
        
        logger.info(f"å¼€å§‹æ•°æ®å¢å¼ºï¼Œå¢å¼ºæ¯”ä¾‹: {augment_ratio}")
        
        augmented_data = data.copy()
        num_to_augment = int(len(data) * augment_ratio)
        
        import random
        random.seed(42)
        
        for i in range(num_to_augment):
            original_item = random.choice(data)
            
            # ç®€å•çš„æ•°æ®å¢å¼ºï¼šåŒä¹‰è¯æ›¿æ¢ã€å¥å¼å˜åŒ–ç­‰
            augmented_item = {
                "instruction": self._augment_text(original_item["instruction"]),
                "input": self._augment_text(original_item.get("input", "")),
                "output": original_item["output"]  # ä¿æŒè¾“å‡ºä¸å˜
            }
            
            augmented_data.append(augmented_item)
        
        logger.info(f"æ•°æ®å¢å¼ºå®Œæˆ: {len(data)} -> {len(augmented_data)}")
        return augmented_data
    
    def _augment_text(self, text: str) -> str:
        """
        æ–‡æœ¬å¢å¼ºï¼ˆç®€å•å®ç°ï¼‰
        """
        if not text or len(text) < 10:
            return text
        
        # ç®€å•çš„åŒä¹‰è¯æ›¿æ¢è¡¨
        synonyms = {
            "è¯·": "éº»çƒ¦",
            "å¸®æˆ‘": "ååŠ©æˆ‘", 
            "å¦‚ä½•": "æ€æ ·",
            "ä»€ä¹ˆ": "å•¥",
            "æ€ä¹ˆ": "å¦‚ä½•",
            "ä»‹ç»": "è¯´æ˜",
            "è§£é‡Š": "é˜è¿°"
        }
        
        result = text
        for original, replacement in synonyms.items():
            if original in result:
                result = result.replace(original, replacement, 1)  # åªæ›¿æ¢ä¸€æ¬¡
                break
        
        return result
    
    def split_dataset(self, data: List[Dict], test_size: float = 0.1, 
                     val_size: float = 0.1, random_state: int = 42) -> Dict[str, List[Dict]]:
        """
        æ•°æ®é›†åˆ†å‰²
        
        Args:
            data: å®Œæ•´æ•°æ®é›†
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            val_size: éªŒè¯é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
            
        Returns:
            åˆ†å‰²åçš„æ•°æ®é›†å­—å…¸
        """
        logger.info(f"å¼€å§‹æ•°æ®é›†åˆ†å‰²: è®­ç»ƒé›† {1-test_size-val_size:.1%}, éªŒè¯é›† {val_size:.1%}, æµ‹è¯•é›† {test_size:.1%}")
        
        # è®­ç»ƒé›†å’Œä¸´æ—¶é›†åˆ†å‰²
        train_data, temp_data = train_test_split(
            data, 
            test_size=(test_size + val_size), 
            random_state=random_state,
            shuffle=True
        )
        
        # éªŒè¯é›†å’Œæµ‹è¯•é›†åˆ†å‰²
        if val_size > 0:
            val_data, test_data = train_test_split(
                temp_data, 
                test_size=(test_size/(test_size + val_size)), 
                random_state=random_state
            )
        else:
            val_data = []
            test_data = temp_data
        
        result = {
            "train": train_data,
            "validation": val_data,
            "test": test_data
        }
        
        # æ‰“å°åˆ†å‰²ç»Ÿè®¡
        for split_name, split_data in result.items():
            logger.info(f"  {split_name}: {len(split_data)} æ¡è®°å½•")
        
        return result
    
    def save_dataset(self, data: Dict[str, List[Dict]], output_dir: str):
        """
        ä¿å­˜æ•°æ®é›†
        
        Args:
            data: æ•°æ®é›†å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        for split_name, split_data in data.items():
            if len(split_data) == 0:
                continue
                
            output_file = os.path.join(output_dir, f"{split_name}.json")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¿å­˜ {split_name} æ•°æ®é›†ï¼š{len(split_data)} æ¡è®°å½• -> {output_file}")
        
        # ä¿å­˜æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        stats_file = os.path.join(output_dir, "dataset_stats.json")
        stats = {
            "total_samples": sum(len(split_data) for split_data in data.values()),
            "splits": {name: len(split_data) for name, split_data in data.items()},
            "sample_data": {name: split_data[:3] if split_data else [] for name, split_data in data.items()}
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜æ•°æ®ç»Ÿè®¡ä¿¡æ¯ -> {stats_file}")

def main():
    parser = argparse.ArgumentParser(description="æ•°æ®å¤„ç†å·¥å…· - CSVè½¬Alpacaæ ¼å¼")
    parser.add_argument("--input", required=True, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--instruction_col", required=True, help="æŒ‡ä»¤åˆ—å")
    parser.add_argument("--input_col", help="è¾“å…¥åˆ—åï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--output_col", required=True, help="è¾“å‡ºåˆ—å")
    parser.add_argument("--test_size", type=float, default=0.1, help="æµ‹è¯•é›†æ¯”ä¾‹")
    parser.add_argument("--val_size", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--quality_threshold", type=float, default=0.8, help="è´¨é‡é˜ˆå€¼")
    parser.add_argument("--augment_ratio", type=float, default=0.0, help="æ•°æ®å¢å¼ºæ¯”ä¾‹")
    parser.add_argument("--no_validation", action="store_true", help="è·³è¿‡æ•°æ®éªŒè¯")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    processor = DataProcessor(quality_threshold=args.quality_threshold)
    
    try:
        # è½¬æ¢æ ¼å¼
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®è½¬æ¢...")
        alpaca_data = processor.csv_to_alpaca(
            args.input, 
            args.instruction_col, 
            args.input_col, 
            args.output_col
        )
        
        if len(alpaca_data) == 0:
            logger.error("æ²¡æœ‰æˆåŠŸè½¬æ¢ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥åˆ—åå’Œæ•°æ®æ ¼å¼")
            return
        
        # éªŒè¯æ•°æ®
        if not args.no_validation:
            logger.info("âœ… å¼€å§‹æ•°æ®éªŒè¯...")
            validated_data = processor.validate_data(alpaca_data)
            
            if len(validated_data) == 0:
                logger.error("æ²¡æœ‰æ•°æ®é€šè¿‡éªŒè¯ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")
                return
        else:
            validated_data = alpaca_data
        
        # æ•°æ®å¢å¼º
        if args.augment_ratio > 0:
            logger.info("ğŸ”„ å¼€å§‹æ•°æ®å¢å¼º...")
            validated_data = processor.augment_data(validated_data, args.augment_ratio)
        
        # åˆ†å‰²æ•°æ®é›†
        logger.info("ğŸ“Š å¼€å§‹æ•°æ®é›†åˆ†å‰²...")
        dataset_splits = processor.split_dataset(
            validated_data, 
            args.test_size, 
            args.val_size
        )
        
        # ä¿å­˜æ•°æ®é›†
        logger.info("ğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        processor.save_dataset(dataset_splits, args.output)
        
        logger.info("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
        total_samples = sum(len(split_data) for split_data in dataset_splits.values())
        logger.info(f"å¤„ç†ç»“æœæ‘˜è¦:")
        logger.info(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        logger.info(f"  è®­ç»ƒé›†: {len(dataset_splits['train'])}")
        logger.info(f"  éªŒè¯é›†: {len(dataset_splits['validation'])}")
        logger.info(f"  æµ‹è¯•é›†: {len(dataset_splits['test'])}")
        logger.info(f"  è¾“å‡ºç›®å½•: {args.output}")
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 