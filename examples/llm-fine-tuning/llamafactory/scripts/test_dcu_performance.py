#!/usr/bin/env python3
"""
DCU k100-AI æ€§èƒ½æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•æµ·å…‰DCU k100-AIåŠ é€Ÿå¡çš„è®¡ç®—æ€§èƒ½å’Œæ˜¾å­˜å¸¦å®½
"""

import torch
import time
import gc
import argparse
import sys
from typing import List, Dict, Any
import numpy as np

def check_dcu_availability() -> bool:
    """æ£€æŸ¥DCUæ˜¯å¦å¯ç”¨"""
    if not torch.cuda.is_available():
        print("âŒ DCUä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é©±åŠ¨å®‰è£…")
        return False
    
    device_count = torch.cuda.device_count()
    print(f"âœ… å‘ç° {device_count} ä¸ªDCUè®¾å¤‡")
    
    for i in range(device_count):
        device_name = torch.cuda.get_device_name(i)
        device_props = torch.cuda.get_device_properties(i)
        memory_gb = device_props.total_memory / (1024**3)
        print(f"  DCU {i}: {device_name}")
        print(f"    æ˜¾å­˜å®¹é‡: {memory_gb:.1f} GB")
        print(f"    è®¡ç®—èƒ½åŠ›: {device_props.major}.{device_props.minor}")
    
    return True

def test_matrix_performance(device_id: int = 0, sizes: List[int] = None) -> Dict[str, Any]:
    """æµ‹è¯•çŸ©é˜µè¿ç®—æ€§èƒ½"""
    if sizes is None:
        sizes = [1024, 2048, 4096, 8192]
    
    print(f"\nğŸ§® DCU {device_id} çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•:")
    print("=" * 60)
    
    results = {}
    device = f'cuda:{device_id}'
    
    for size in sizes:
        try:
            # ç”ŸæˆéšæœºçŸ©é˜µ
            a = torch.randn(size, size, device=device, dtype=torch.float16)
            b = torch.randn(size, size, device=device, dtype=torch.float16)
            
            # é¢„çƒ­GPU
            for _ in range(10):
                _ = torch.mm(a, b)
            torch.cuda.synchronize()
            
            # æ€§èƒ½æµ‹è¯•
            iterations = 100
            start_time = time.time()
            for _ in range(iterations):
                c = torch.mm(a, b)
            torch.cuda.synchronize()
            end_time = time.time()
            
            elapsed = end_time - start_time
            # è®¡ç®—TFLOPS: æ¯æ¬¡çŸ©é˜µä¹˜æ³•çš„è¿ç®—é‡ä¸º 2*size^3
            flops = 2 * size**3 * iterations
            tflops = flops / (elapsed * 1e12)
            
            results[size] = {
                'elapsed_time': elapsed,
                'tflops': tflops,
                'memory_used': torch.cuda.memory_allocated(device_id) / (1024**3)
            }
            
            print(f"  {size:4d}x{size:4d}: {elapsed:6.3f}s, {tflops:6.2f} TFLOPS, "
                  f"æ˜¾å­˜: {results[size]['memory_used']:.2f} GB")
            
            # æ¸…ç†å†…å­˜
            del a, b, c
            gc.collect()
            torch.cuda.empty_cache()
            
        except torch.cuda.OutOfMemoryError:
            print(f"  {size:4d}x{size:4d}: âŒ æ˜¾å­˜ä¸è¶³")
            results[size] = {'error': 'OutOfMemoryError'}
        except Exception as e:
            print(f"  {size:4d}x{size:4d}: âŒ é”™è¯¯: {str(e)}")
            results[size] = {'error': str(e)}
    
    return results

def test_memory_bandwidth(device_id: int = 0, sizes_mb: List[int] = None) -> Dict[str, Any]:
    """æµ‹è¯•æ˜¾å­˜å¸¦å®½"""
    if sizes_mb is None:
        sizes_mb = [64, 128, 256, 512, 1024, 2048]
    
    print(f"\nğŸ’¾ DCU {device_id} æ˜¾å­˜å¸¦å®½æµ‹è¯•:")
    print("=" * 60)
    
    results = {}
    device = f'cuda:{device_id}'
    
    for size_mb in sizes_mb:
        try:
            elements = size_mb * 1024 * 1024 // 4  # float32
            data = torch.randn(elements, device=device, dtype=torch.float32)
            
            # é¢„çƒ­
            for _ in range(10):
                _ = data.sum()
            torch.cuda.synchronize()
            
            # å¸¦å®½æµ‹è¯•
            iterations = 100
            start_time = time.time()
            for _ in range(iterations):
                result = data.sum()
            torch.cuda.synchronize()
            end_time = time.time()
            
            elapsed = end_time - start_time
            # è®¡ç®—å¸¦å®½: æ¯æ¬¡æ“ä½œè¯»å–size_mbæ•°æ®
            bandwidth_gbps = (size_mb * iterations) / elapsed / 1024
            
            results[size_mb] = {
                'elapsed_time': elapsed,
                'bandwidth_gbps': bandwidth_gbps,
                'memory_used': torch.cuda.memory_allocated(device_id) / (1024**3)
            }
            
            print(f"  {size_mb:4d}MB: {bandwidth_gbps:6.2f} GB/s, "
                  f"æ˜¾å­˜: {results[size_mb]['memory_used']:.2f} GB")
            
            del data, result
            gc.collect()
            torch.cuda.empty_cache()
            
        except torch.cuda.OutOfMemoryError:
            print(f"  {size_mb:4d}MB: âŒ æ˜¾å­˜ä¸è¶³")
            results[size_mb] = {'error': 'OutOfMemoryError'}
        except Exception as e:
            print(f"  {size_mb:4d}MB: âŒ é”™è¯¯: {str(e)}")
            results[size_mb] = {'error': str(e)}
    
    return results

def test_mixed_precision(device_id: int = 0) -> Dict[str, Any]:
    """æµ‹è¯•æ··åˆç²¾åº¦æ€§èƒ½"""
    print(f"\nğŸ¯ DCU {device_id} æ··åˆç²¾åº¦æµ‹è¯•:")
    print("=" * 60)
    
    results = {}
    device = f'cuda:{device_id}'
    size = 4096
    iterations = 50
    
    # æµ‹è¯•ä¸åŒç²¾åº¦
    dtypes = [
        ('FP32', torch.float32),
        ('FP16', torch.float16),
        ('BF16', torch.bfloat16),
    ]
    
    for dtype_name, dtype in dtypes:
        try:
            # ç”ŸæˆçŸ©é˜µ
            a = torch.randn(size, size, device=device, dtype=dtype)
            b = torch.randn(size, size, device=device, dtype=dtype)
            
            # é¢„çƒ­
            for _ in range(5):
                _ = torch.mm(a, b)
            torch.cuda.synchronize()
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            for _ in range(iterations):
                c = torch.mm(a, b)
            torch.cuda.synchronize()
            end_time = time.time()
            
            elapsed = end_time - start_time
            flops = 2 * size**3 * iterations
            tflops = flops / (elapsed * 1e12)
            memory_used = torch.cuda.memory_allocated(device_id) / (1024**3)
            
            results[dtype_name] = {
                'elapsed_time': elapsed,
                'tflops': tflops,
                'memory_used': memory_used
            }
            
            print(f"  {dtype_name}: {elapsed:6.3f}s, {tflops:6.2f} TFLOPS, "
                  f"æ˜¾å­˜: {memory_used:.2f} GB")
            
            del a, b, c
            gc.collect()
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"  {dtype_name}: âŒ é”™è¯¯: {str(e)}")
            results[dtype_name] = {'error': str(e)}
    
    return results

def benchmark_llm_workload(device_id: int = 0) -> Dict[str, Any]:
    """æ¨¡æ‹ŸLLMå·¥ä½œè´Ÿè½½æ€§èƒ½æµ‹è¯•"""
    print(f"\nğŸ¤– DCU {device_id} LLMå·¥ä½œè´Ÿè½½æ¨¡æ‹Ÿ:")
    print("=" * 60)
    
    device = f'cuda:{device_id}'
    results = {}
    
    # æ¨¡æ‹Ÿä¸åŒçš„LLMå‚æ•°é…ç½®
    configs = [
        {'name': '3Bæ¨¡å‹', 'seq_len': 2048, 'hidden_size': 2048, 'batch_size': 8},
        {'name': '7Bæ¨¡å‹', 'seq_len': 2048, 'hidden_size': 4096, 'batch_size': 4},
        {'name': '14Bæ¨¡å‹', 'seq_len': 2048, 'hidden_size': 5120, 'batch_size': 2},
    ]
    
    for config in configs:
        try:
            seq_len = config['seq_len']
            hidden_size = config['hidden_size']
            batch_size = config['batch_size']
            
            # æ¨¡æ‹Ÿæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—
            query = torch.randn(batch_size, seq_len, hidden_size, device=device, dtype=torch.bfloat16)
            key = torch.randn(batch_size, seq_len, hidden_size, device=device, dtype=torch.bfloat16)
            value = torch.randn(batch_size, seq_len, hidden_size, device=device, dtype=torch.bfloat16)
            
            # é¢„çƒ­
            for _ in range(5):
                # æ³¨æ„åŠ›è®¡ç®—
                scores = torch.matmul(query, key.transpose(-2, -1)) / (hidden_size ** 0.5)
                attn_weights = torch.softmax(scores, dim=-1)
                output = torch.matmul(attn_weights, value)
            torch.cuda.synchronize()
            
            # æ€§èƒ½æµ‹è¯•
            iterations = 20
            start_time = time.time()
            for _ in range(iterations):
                scores = torch.matmul(query, key.transpose(-2, -1)) / (hidden_size ** 0.5)
                attn_weights = torch.softmax(scores, dim=-1)
                output = torch.matmul(attn_weights, value)
            torch.cuda.synchronize()
            end_time = time.time()
            
            elapsed = end_time - start_time
            tokens_per_second = (batch_size * seq_len * iterations) / elapsed
            memory_used = torch.cuda.memory_allocated(device_id) / (1024**3)
            
            results[config['name']] = {
                'elapsed_time': elapsed,
                'tokens_per_second': tokens_per_second,
                'memory_used': memory_used
            }
            
            print(f"  {config['name']:8s}: {tokens_per_second:6.0f} tok/s, "
                  f"æ˜¾å­˜: {memory_used:.2f} GB")
            
            del query, key, value, scores, attn_weights, output
            gc.collect()
            torch.cuda.empty_cache()
            
        except torch.cuda.OutOfMemoryError:
            print(f"  {config['name']:8s}: âŒ æ˜¾å­˜ä¸è¶³")
            results[config['name']] = {'error': 'OutOfMemoryError'}
        except Exception as e:
            print(f"  {config['name']:8s}: âŒ é”™è¯¯: {str(e)}")
            results[config['name']] = {'error': str(e)}
    
    return results

def generate_report(all_results: Dict[str, Any], device_id: int = 0):
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    print(f"\nğŸ“Š DCU {device_id} æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    print("=" * 80)
    
    # è®¾å¤‡ä¿¡æ¯
    device_name = torch.cuda.get_device_name(device_id)
    device_props = torch.cuda.get_device_properties(device_id)
    memory_gb = device_props.total_memory / (1024**3)
    
    print(f"è®¾å¤‡åç§°: {device_name}")
    print(f"æ˜¾å­˜å®¹é‡: {memory_gb:.1f} GB")
    print(f"è®¡ç®—èƒ½åŠ›: {device_props.major}.{device_props.minor}")
    
    # å³°å€¼æ€§èƒ½
    if 'matrix_results' in all_results:
        matrix_results = all_results['matrix_results']
        max_tflops = max([r.get('tflops', 0) for r in matrix_results.values() if 'tflops' in r])
        print(f"å³°å€¼ç®—åŠ›: {max_tflops:.2f} TFLOPS")
    
    if 'bandwidth_results' in all_results:
        bandwidth_results = all_results['bandwidth_results']
        max_bandwidth = max([r.get('bandwidth_gbps', 0) for r in bandwidth_results.values() if 'bandwidth_gbps' in r])
        print(f"å³°å€¼å¸¦å®½: {max_bandwidth:.2f} GB/s")
    
    # LLMæ€§èƒ½
    if 'llm_results' in all_results:
        print(f"\nLLMå·¥ä½œè´Ÿè½½æ€§èƒ½:")
        for model_name, result in all_results['llm_results'].items():
            if 'tokens_per_second' in result:
                print(f"  {model_name}: {result['tokens_per_second']:.0f} tokens/s")
    
    # æ¨èé…ç½®
    print(f"\nğŸ’¡ åŸºäºæµ‹è¯•ç»“æœçš„æ¨èé…ç½®:")
    print(f"  - æ¨èç²¾åº¦: BF16 (DCU k100-AIåŸç”Ÿæ”¯æŒ)")
    print(f"  - æ¨èbatch_size: 4-8 (æ ¹æ®æ¨¡å‹å¤§å°)")
    print(f"  - æ¨èsequence_length: 2048")
    print(f"  - æ˜¾å­˜åˆ©ç”¨å»ºè®®: ä½¿ç”¨80-85%æ˜¾å­˜ä»¥ä¿æŒç¨³å®šæ€§")

def main():
    parser = argparse.ArgumentParser(description='DCU k100-AI æ€§èƒ½æµ‹è¯•å·¥å…·')
    parser.add_argument('--device', type=int, default=0, help='DCUè®¾å¤‡ID (é»˜è®¤: 0)')
    parser.add_argument('--matrix-only', action='store_true', help='ä»…è¿è¡ŒçŸ©é˜µæ€§èƒ½æµ‹è¯•')
    parser.add_argument('--bandwidth-only', action='store_true', help='ä»…è¿è¡Œå¸¦å®½æµ‹è¯•')
    parser.add_argument('--llm-only', action='store_true', help='ä»…è¿è¡ŒLLMå·¥ä½œè´Ÿè½½æµ‹è¯•')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    print("ğŸš€ DCU k100-AI æ€§èƒ½æµ‹è¯•å·¥å…·")
    print("=" * 80)
    
    # æ£€æŸ¥DCUå¯ç”¨æ€§
    if not check_dcu_availability():
        sys.exit(1)
    
    device_id = args.device
    if device_id >= torch.cuda.device_count():
        print(f"âŒ è®¾å¤‡ID {device_id} è¶…å‡ºèŒƒå›´")
        sys.exit(1)
    
    torch.cuda.set_device(device_id)
    all_results = {}
    
    try:
        # æ ¹æ®å‚æ•°è¿è¡Œä¸åŒæµ‹è¯•
        if args.matrix_only:
            sizes = [2048, 4096] if args.quick else [1024, 2048, 4096, 8192]
            all_results['matrix_results'] = test_matrix_performance(device_id, sizes)
        elif args.bandwidth_only:
            sizes = [256, 512, 1024] if args.quick else [64, 128, 256, 512, 1024, 2048]
            all_results['bandwidth_results'] = test_memory_bandwidth(device_id, sizes)
        elif args.llm_only:
            all_results['llm_results'] = benchmark_llm_workload(device_id)
        else:
            # å®Œæ•´æµ‹è¯•
            sizes = [2048, 4096] if args.quick else [1024, 2048, 4096, 8192]
            bandwidth_sizes = [256, 512, 1024] if args.quick else [128, 256, 512, 1024]
            
            all_results['matrix_results'] = test_matrix_performance(device_id, sizes)
            all_results['bandwidth_results'] = test_memory_bandwidth(device_id, bandwidth_sizes)
            all_results['mixed_precision_results'] = test_mixed_precision(device_id)
            all_results['llm_results'] = benchmark_llm_workload(device_id)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_report(all_results, device_id)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 