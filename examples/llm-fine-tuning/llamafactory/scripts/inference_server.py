#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLaMA Factoryæ¨¡å‹æ¨ç†æœåŠ¡
ä½œè€…: DCUå®æˆ˜é¡¹ç›®ç»„
"""

import os
import sys
import json
import asyncio
import argparse
import logging
import time
import torch
from pathlib import Path
from typing import List, Optional, Dict, Any, AsyncGenerator
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from transformers import AutoModelForCausalLM, AutoTokenizer
import psutil
import GPUtil

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
inference_engine = None

# APIè¯·æ±‚/å“åº”æ¨¡å‹
class ChatRequest(BaseModel):
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    history: Optional[List[List[str]]] = Field(default=[], description="å¯¹è¯å†å²")
    max_length: Optional[int] = Field(default=2048, description="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: Optional[float] = Field(default=0.9, ge=0.0, le=1.0, description="top-på‚æ•°")
    do_sample: Optional[bool] = Field(default=True, description="æ˜¯å¦é‡‡æ ·")

class ChatResponse(BaseModel):
    response: str = Field(..., description="æ¨¡å‹å›å¤")
    history: List[List[str]] = Field(..., description="æ›´æ–°åçš„å¯¹è¯å†å²")
    timestamp: float = Field(..., description="å“åº”æ—¶é—´æˆ³")
    inference_time: float = Field(..., description="æ¨ç†è€—æ—¶(ç§’)")

class BatchRequest(BaseModel):
    messages: List[str] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    max_length: Optional[int] = Field(default=2048, description="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    temperature: Optional[float] = Field(default=0.7, description="æ¸©åº¦å‚æ•°")
    top_p: Optional[float] = Field(default=0.9, description="top-på‚æ•°")

class BatchResponse(BaseModel):
    responses: List[str] = Field(..., description="å›å¤åˆ—è¡¨")
    total_time: float = Field(..., description="æ€»è€—æ—¶(ç§’)")
    avg_time_per_request: float = Field(..., description="å¹³å‡å•æ¡è€—æ—¶(ç§’)")

class ModelInfo(BaseModel):
    model_name: str = Field(..., description="æ¨¡å‹åç§°")
    model_path: str = Field(..., description="æ¨¡å‹è·¯å¾„")
    device: str = Field(..., description="è¿è¡Œè®¾å¤‡")
    memory_usage: Dict[str, float] = Field(..., description="å†…å­˜ä½¿ç”¨æƒ…å†µ")
    total_requests: int = Field(..., description="æ€»è¯·æ±‚æ•°")
    avg_response_time: float = Field(..., description="å¹³å‡å“åº”æ—¶é—´")

class HealthResponse(BaseModel):
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    uptime: float = Field(..., description="è¿è¡Œæ—¶é—´(ç§’)")
    model_loaded: bool = Field(..., description="æ¨¡å‹æ˜¯å¦å·²åŠ è½½")
    system_info: Dict[str, float] = Field(..., description="ç³»ç»Ÿä¿¡æ¯")

class InferenceEngine:
    """æ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: str, device: str = "auto", precision: str = "fp16"):
        self.model_path = model_path
        self.device = device
        self.precision = precision
        self.model = None
        self.tokenizer = None
        self.load_time = 0
        
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        start_time = time.time()
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            use_fast=False
        )
        
        # è®¾ç½®æ¨¡å‹ç²¾åº¦
        torch_dtype = torch.float16 if self.precision == "fp16" else torch.float32
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch_dtype,
            device_map=self.device,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        self.model.eval()
        
        self.load_time = time.time() - start_time
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {self.load_time:.2f}ç§’")
        print(f"æ¨¡å‹è®¾å¤‡: {self.get_model_device()}")
        print(f"æ¨¡å‹ç²¾åº¦: {self.precision}")
    
    def get_model_device(self) -> str:
        """è·å–æ¨¡å‹è®¾å¤‡ä¿¡æ¯"""
        if hasattr(self.model, 'hf_device_map'):
            devices = set(self.model.hf_device_map.values())
            return f"GPU: {list(devices)}" if 'cuda' in str(devices) else "CPU"
        return "Unknown"
    
    def get_model_info(self) -> ModelInfo:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        # ä¼°ç®—æ¨¡å‹å¤§å°
        param_count = sum(p.numel() for p in self.model.parameters())
        model_size = f"{param_count / 1e9:.1f}B parameters"
        
        return ModelInfo(
            model_name=os.path.basename(self.model_path),
            model_path=self.model_path,
            device=self.get_model_device(),
            memory_usage={},
            total_requests=0,
            avg_response_time=0.0
        )
    
    def chat(self, message: str, history: List[List[str]] = None,
             max_length: int = 2048, temperature: float = 0.7,
             top_p: float = 0.9, top_k: int = 50, 
             repetition_penalty: float = 1.1) -> tuple:
        """èŠå¤©æ¨ç†"""
        if history is None:
            history = []
        
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰chatæ–¹æ³•
            if hasattr(self.model, 'chat'):
                # ä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„chatæ–¹æ³•
                response, updated_history = self.model.chat(
                    self.tokenizer,
                    message,
                    history=history,
                    max_length=max_length,
                    temperature=temperature,
                    top_p=top_p
                )
            else:
                # æ‰‹åŠ¨æ„å»ºå¯¹è¯
                response, updated_history = self._manual_chat(
                    message, history, max_length, temperature, top_p, top_k, repetition_penalty
                )
            
            # è®¡ç®—tokenä½¿ç”¨é‡
            prompt_tokens = len(self.tokenizer.encode(message))
            completion_tokens = len(self.tokenizer.encode(response))
            
            usage = {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
                "inference_time": time.time() - start_time
            }
            
            return response, updated_history, usage
            
        except Exception as e:
            print(f"æ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            raise e
    
    def _manual_chat(self, message: str, history: List[List[str]],
                    max_length: int, temperature: float, top_p: float,
                    top_k: int, repetition_penalty: float) -> tuple:
        """æ‰‹åŠ¨èŠå¤©å®ç°"""
        # æ„å»ºå¯¹è¯å†å²
        conversation = ""
        for user_msg, assistant_msg in history:
            conversation += f"ç”¨æˆ·: {user_msg}\nåŠ©æ‰‹: {assistant_msg}\n"
        conversation += f"ç”¨æˆ·: {message}\nåŠ©æ‰‹: "
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(
            conversation,
            return_tensors="pt",
            max_length=max_length,
            truncation=True
        )
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        # æ›´æ–°å†å²
        updated_history = history + [[message, response]]
        
        return response, updated_history

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    global inference_engine
    
    model_path = os.environ.get("MODEL_PATH", "/path/to/your/model")
    device = os.environ.get("DEVICE", "auto")
    precision = os.environ.get("PRECISION", "fp16")
    
    if not os.path.exists(model_path):
        print(f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ MODEL_PATH æˆ–åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®šæ¨¡å‹è·¯å¾„")
        sys.exit(1)
    
    inference_engine = InferenceEngine(model_path, device, precision)
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    if inference_engine and inference_engine.model:
        del inference_engine.model
        del inference_engine.tokenizer
        torch.cuda.empty_cache()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LLaMA Factory æ¨ç†æœåŠ¡",
    description="åŸºäºLLaMA Factoryçš„å¤§æ¨¡å‹æ¨ç†APIæœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨ç†æœåŠ¡å®ä¾‹
inference_service: Optional[InferenceEngine] = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    global inference_service
    
    if inference_service is None:
        logger.error("æ¨ç†æœåŠ¡æœªåˆå§‹åŒ–")
        return
    
    try:
        inference_service.load_model()
        logger.info("ğŸš€ æ¨ç†æœåŠ¡å¯åŠ¨æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨ç†æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """èŠå¤©å¯¹è¯æ¥å£"""
    if inference_service is None or inference_service.model is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªåˆå§‹åŒ–")
    
    try:
        response, history, usage = inference_service.chat(
            request.message,
            request.history,
            request.max_length,
            request.temperature,
            request.top_p,
            request.do_sample
        )
        
        return ChatResponse(
            response=response,
            history=history,
            timestamp=time.time(),
            inference_time=usage["inference_time"]
        )
        
    except Exception as e:
        logger.error(f"èŠå¤©æ¥å£é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """æµå¼èŠå¤©æ¥å£"""
    if inference_service is None or inference_service.model is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªåˆå§‹åŒ–")
    
    async def generate():
        try:
            async for chunk in inference_service.chat_stream(
                request.message,
                request.history,
                request.max_length,
                request.temperature,
                request.top_p
            ):
                yield f"data: {json.dumps({'chunk': chunk}, ensure_ascii=False)}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"
        finally:
            yield "data: [DONE]\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

@app.post("/batch", response_model=BatchResponse)
async def batch_inference_endpoint(request: BatchRequest):
    """æ‰¹é‡æ¨ç†æ¥å£"""
    if inference_service is None or inference_service.model is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªåˆå§‹åŒ–")
    
    try:
        responses, total_time = await inference_service.batch_inference(
            request.messages,
            max_length=request.max_length,
            temperature=request.temperature,
            top_p=request.top_p
        )
        
        return BatchResponse(
            responses=responses,
            total_time=total_time,
            avg_time_per_request=total_time / len(request.messages)
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ¨ç†é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/info", response_model=ModelInfo)
async def model_info_endpoint():
    """æ¨¡å‹ä¿¡æ¯æ¥å£"""
    if inference_service is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªåˆå§‹åŒ–")
    
    try:
        info = inference_service.get_model_info()
        return ModelInfo(**info)
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    try:
        system_info = {}
        uptime = 0.0
        model_loaded = False
        
        if inference_service is not None:
            system_info = inference_service.get_system_info()
            uptime = system_info.get("uptime", 0.0)
            model_loaded = inference_service.model is not None
        
        return HealthResponse(
            status="healthy" if model_loaded else "loading",
            uptime=uptime,
            model_loaded=model_loaded,
            system_info=system_info
        )
        
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥é”™è¯¯: {e}")
        return HealthResponse(
            status="error",
            uptime=0.0,
            model_loaded=False,
            system_info={}
        )

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "LLaMA Factory æ¨ç†æœåŠ¡",
        "docs": "/docs",
        "health": "/health",
        "info": "/info"
    }

def create_client_example():
    """åˆ›å»ºå®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹"""
    example_code = '''
import requests
import json

# æ¨ç†æœåŠ¡åŸºç¡€URL
BASE_URL = "http://localhost:8000"

class LLaMAClient:
    def __init__(self, base_url: str = BASE_URL):
        self.base_url = base_url
        self.history = []
    
    def chat(self, message: str, **kwargs) -> str:
        """å•è½®å¯¹è¯"""
        url = f"{self.base_url}/chat"
        payload = {
            "message": message,
            "history": self.history,
            **kwargs
        }
        
        try:
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            self.history = result["history"]
            return result["response"]
            
        except requests.exceptions.RequestException as e:
            return f"è¯·æ±‚å¤±è´¥: {e}"
    
    def batch_inference(self, messages: list, **kwargs) -> list:
        """æ‰¹é‡æ¨ç†"""
        url = f"{self.base_url}/batch"
        payload = {
            "messages": messages,
            **kwargs
        }
        
        try:
            response = requests.post(url, json=payload, timeout=300)
            response.raise_for_status()
            
            result = response.json()
            return result["responses"]
            
        except requests.exceptions.RequestException as e:
            return [f"è¯·æ±‚å¤±è´¥: {e}"] * len(messages)
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        url = f"{self.base_url}/info"
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}
    
    def reset_history(self):
        """é‡ç½®å¯¹è¯å†å²"""
        self.history = []

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    client = LLaMAClient()
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    info = client.get_model_info()
    print(f"æ¨¡å‹ä¿¡æ¯: {info}")
    
    # å•è½®å¯¹è¯
    response = client.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
    print(f"å›å¤: {response}")
    
    # æ‰¹é‡æ¨ç†
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
        "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]
    
    answers = client.batch_inference(questions)
    for q, a in zip(questions, answers):
        print(f"é—®é¢˜: {q}")
        print(f"å›ç­”: {a}\\n")
'''
    
    with open("client_example.py", "w", encoding="utf-8") as f:
        f.write(example_code)
    
    print("å®¢æˆ·ç«¯ç¤ºä¾‹ä»£ç å·²ä¿å­˜åˆ°: client_example.py")

def main():
    parser = argparse.ArgumentParser(description="LLaMA Factory æ¨ç†æœåŠ¡å™¨")
    parser.add_argument("--model_path", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--host", default="0.0.0.0", help="æœåŠ¡ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--device", default="auto", choices=["auto", "cuda", "cpu"], help="è¿è¡Œè®¾å¤‡")
    parser.add_argument("--workers", type=int, default=1, help="å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--create_client", action="store_true", help="åˆ›å»ºå®¢æˆ·ç«¯ç¤ºä¾‹ä»£ç ")
    parser.add_argument("--log_level", default="info", choices=["debug", "info", "warning", "error"], help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level.upper()))
    
    # åˆ›å»ºå®¢æˆ·ç«¯ç¤ºä¾‹
    if args.create_client:
        create_client_example()
        return
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)
    
    # åˆå§‹åŒ–æ¨ç†æœåŠ¡
    global inference_service
    inference_service = InferenceEngine(args.model_path, args.device)
    
    logger.info(f"å¯åŠ¨æ¨ç†æœåŠ¡...")
    logger.info(f"  æ¨¡å‹è·¯å¾„: {args.model_path}")
    logger.info(f"  æœåŠ¡åœ°å€: http://{args.host}:{args.port}")
    logger.info(f"  è¿è¡Œè®¾å¤‡: {args.device}")
    
    # å¯åŠ¨æœåŠ¡
    try:
        uvicorn.run(
            app,
            host=args.host,
            port=args.port,
            workers=args.workers,
            log_level=args.log_level
        )
    except KeyboardInterrupt:
        logger.info("æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 