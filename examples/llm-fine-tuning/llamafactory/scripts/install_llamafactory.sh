#!/bin/bash
# LLaMA Factory ä¸€é”®å®‰è£…è„šæœ¬
# æ”¯æŒGPU/CPUç¯å¢ƒè‡ªåŠ¨æ£€æµ‹å’Œé…ç½®

set -e

echo "ğŸš€ å¼€å§‹å®‰è£…LLaMA Factoryç¯å¢ƒ..."

# æ£€æŸ¥GPUç¯å¢ƒ
if command -v nvidia-smi &> /dev/null; then
    echo "âœ“ æ£€æµ‹åˆ°NVIDIA GPU"
    nvidia-smi
    GPU_AVAILABLE=true
else
    echo "âš ï¸ æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼"
    GPU_AVAILABLE=false
fi

# æ£€æŸ¥Pythonç‰ˆæœ¬
PYTHON_VERSION=$(python3 -c "import sys; print('.'.join(map(str, sys.version_info[:2])))")
echo "Pythonç‰ˆæœ¬: $PYTHON_VERSION"

if [[ $(echo "$PYTHON_VERSION < 3.8" | bc -l) -eq 1 ]]; then
    echo "âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦Python 3.8+ï¼Œå½“å‰ç‰ˆæœ¬: $PYTHON_VERSION"
    exit 1
fi

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
echo "ğŸ“¦ åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ..."
python3 -m venv llamafactory_env
source llamafactory_env/bin/activate

# å‡çº§pip
echo "â¬†ï¸ å‡çº§pip..."
pip install --upgrade pip setuptools wheel

# å®‰è£…PyTorch
echo "ğŸ”§ å®‰è£…PyTorch..."
if [ "$GPU_AVAILABLE" = true ]; then
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
else
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
fi

# å…‹éš†LLaMA Factoryä»“åº“
echo "ğŸ“¥ å…‹éš†LLaMA Factoryä»“åº“..."
if [ ! -d "LLaMA-Factory" ]; then
    git clone https://github.com/hiyouga/LLaMA-Factory.git
fi
cd LLaMA-Factory

# å®‰è£…LLaMA Factory
echo "ğŸ› ï¸ å®‰è£…LLaMA Factory..."
pip install -e .

# å®‰è£…é¢å¤–ä¾èµ–
echo "ğŸ“š å®‰è£…é¢å¤–ä¾èµ–..."
pip install datasets transformers accelerate peft trl
pip install bitsandbytes  # ç”¨äºé‡åŒ–è®­ç»ƒ
pip install wandb  # ç”¨äºå®éªŒè·Ÿè¸ª
pip install gradio  # WebUIç•Œé¢
pip install fastapi uvicorn  # APIæœåŠ¡
pip install rouge-score nltk  # è¯„ä¼°å·¥å…·

# ä¸‹è½½NLTKæ•°æ®
echo "ğŸ“Š ä¸‹è½½NLTKæ•°æ®..."
python3 -c "import nltk; nltk.download('punkt')"

# éªŒè¯å®‰è£…
echo "âœ… éªŒè¯å®‰è£…..."
python3 -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')"
python3 -c "import transformers; print(f'Transformersç‰ˆæœ¬: {transformers.__version__}')"

if [ "$GPU_AVAILABLE" = true ]; then
    python3 -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
    python3 -c "import torch; print(f'GPUæ•°é‡: {torch.cuda.device_count()}')"
    if python3 -c "import torch; exit(0 if torch.cuda.is_available() else 1)"; then
        echo "âœ… GPUç¯å¢ƒé…ç½®æˆåŠŸ"
    else
        echo "âš ï¸ GPUç¯å¢ƒé…ç½®å¯èƒ½æœ‰é—®é¢˜ï¼Œä½†åŸºç¡€å®‰è£…å®Œæˆ"
    fi
fi

# åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶
echo "ğŸ“„ åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶..."
mkdir -p examples/configs

cat > examples/configs/lora_config.yaml << 'EOF'
# LoRAå¾®è°ƒé…ç½®ç¤ºä¾‹
model_name_or_path: "THUDM/chatglm3-6b"
dataset: "alpaca_zh"
template: "chatglm3"
finetuning_type: "lora"
lora_target: "query_key_value"
output_dir: "./saves/ChatGLM3-6B/lora/train"

# LoRAé…ç½®
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.1
modules_to_save: "embed_tokens,lm_head"

# è®­ç»ƒå‚æ•°
stage: "sft"
do_train: true
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
lr_scheduler_type: "cosine"
logging_steps: 10
save_steps: 1000
learning_rate: 5.0e-5
num_train_epochs: 3.0
max_grad_norm: 1.0
quantization_bit: 4
use_unsloth: true
val_size: 0.1
evaluation_strategy: "steps"
eval_steps: 500
load_best_model_at_end: true
plot_loss: true
EOF

cat > examples/configs/qlora_config.yaml << 'EOF'
# QLoRAå¾®è°ƒé…ç½®ç¤ºä¾‹
model_name_or_path: "meta-llama/Llama-2-7b-chat-hf"
dataset: "alpaca_en"
template: "llama2"
finetuning_type: "lora"
lora_target: "q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj"
output_dir: "./saves/Llama2-7B/qlora/train"

# QLoRAé…ç½®
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.1
quantization_bit: 4
use_double_quant: true
quant_type: "nf4"
compute_dtype: "bfloat16"

# è®­ç»ƒå‚æ•°
stage: "sft"
do_train: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
lr_scheduler_type: "cosine"
logging_steps: 10
save_steps: 1000
learning_rate: 2.0e-5
num_train_epochs: 3.0
max_grad_norm: 1.0
val_size: 0.1
evaluation_strategy: "steps"
eval_steps: 500
load_best_model_at_end: true
plot_loss: true
EOF

# åˆ›å»ºå¯åŠ¨è„šæœ¬
cat > start_webui.sh << 'EOF'
#!/bin/bash
# å¯åŠ¨WebUIçš„ä¾¿æ·è„šæœ¬
source llamafactory_env/bin/activate
cd LLaMA-Factory
python src/train_web.py
EOF

chmod +x start_webui.sh

cat > requirements.txt << 'EOF'
torch>=2.0.0
transformers>=4.35.0
datasets>=2.12.0
accelerate>=0.24.0
peft>=0.6.0
trl>=0.7.0
bitsandbytes>=0.41.0
wandb>=0.15.0
gradio>=3.50.0
fastapi>=0.104.0
uvicorn>=0.24.0
rouge-score>=0.1.2
nltk>=3.8.0
pyyaml>=6.0
pandas>=1.5.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
EOF

echo "ğŸ‰ LLaMA Factoryå®‰è£…å®Œæˆï¼"
echo ""
echo "ğŸ“ åç»­æ­¥éª¤ï¼š"
echo "1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ: source llamafactory_env/bin/activate"
echo "2. å¯åŠ¨WebUI: ./start_webui.sh"
echo "3. æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ: cd LLaMA-Factory && llamafactory-cli train examples/configs/lora_config.yaml"
echo ""
echo "ğŸ“‚ é…ç½®æ–‡ä»¶ä½ç½®:"
echo "   - examples/configs/lora_config.yaml"
echo "   - examples/configs/qlora_config.yaml"
echo ""
echo "ğŸŒ WebUIè®¿é—®åœ°å€: http://localhost:7860"
echo ""

# æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
echo "ğŸ’» ç³»ç»Ÿä¿¡æ¯:"
echo "   - Python: $PYTHON_VERSION"
if [ "$GPU_AVAILABLE" = true ]; then
    echo "   - GPU: å·²æ£€æµ‹åˆ°"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | head -1 | awk '{print "   - GPUå‹å·: " $0}'
else
    echo "   - GPU: æœªæ£€æµ‹åˆ°"
fi
echo "   - å®‰è£…ç›®å½•: $(pwd)"
echo ""
echo "âœ¨ å®‰è£…å®Œæˆï¼å¼€å§‹æ‚¨çš„å¤§æ¨¡å‹å¾®è°ƒä¹‹æ—…å§ï¼" 