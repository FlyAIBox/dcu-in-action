#!/usr/bin/env python3
"""
DCUæ€§èƒ½åŸºå‡†æµ‹è¯•ç¨‹åº
å…¨é¢æµ‹è¯•DCUåœ¨å„ç§è®¡ç®—ä»»åŠ¡ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import argparse
import json
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from common.utils.logger import get_logger
from common.dcu.performance_profiler import DCUProfiler

logger = get_logger(__name__)

class BenchmarkSuite:
    """DCUæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, device='cuda:0', warmup_runs=3, test_runs=10):
        self.device = torch.device(device)
        self.warmup_runs = warmup_runs
        self.test_runs = test_runs
        self.results = {}
        
        print(f"ğŸš€ åˆå§‹åŒ–DCUåŸºå‡†æµ‹è¯• (è®¾å¤‡: {device})")
        print(f"   é¢„çƒ­æ¬¡æ•°: {warmup_runs}, æµ‹è¯•æ¬¡æ•°: {test_runs}")
    
    def warmup(self, func, *args, **kwargs):
        """é¢„çƒ­å‡½æ•°"""
        for _ in range(self.warmup_runs):
            func(*args, **kwargs)
        torch.cuda.synchronize()
    
    def benchmark(self, func, *args, **kwargs):
        """åŸºå‡†æµ‹è¯•å‡½æ•°"""
        # é¢„çƒ­
        self.warmup(func, *args, **kwargs)
        
        # æµ‹è¯•
        start_time = time.time()
        for _ in range(self.test_runs):
            result = func(*args, **kwargs)
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / self.test_runs
        return avg_time, result
    
    def test_matrix_operations(self):
        """æµ‹è¯•çŸ©é˜µè¿ç®—æ€§èƒ½"""
        print("\nğŸ“Š çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•")
        print("-" * 50)
        
        sizes = [512, 1024, 2048, 4096, 8192]
        operations = {
            'matmul': lambda a, b: torch.mm(a, b),
            'add': lambda a, b: torch.add(a, b),
            'mul': lambda a, b: torch.mul(a, b),
            'transpose': lambda a, b: torch.transpose(a, 0, 1),
        }
        
        results = {}
        
        for size in sizes:
            print(f"\nçŸ©é˜µå¤§å°: {size}x{size}")
            a = torch.randn(size, size, device=self.device, dtype=torch.float32)
            b = torch.randn(size, size, device=self.device, dtype=torch.float32)
            
            size_results = {}
            
            for op_name, op_func in operations.items():
                try:
                    avg_time, _ = self.benchmark(op_func, a, b)
                    
                    if op_name == 'matmul':
                        flops = 2 * size**3
                        gflops = flops / avg_time / 1e9
                        print(f"   {op_name:10s}: {avg_time*1000:8.2f} ms ({gflops:6.1f} GFLOPS)")
                        size_results[op_name] = {'time': avg_time, 'gflops': gflops}
                    else:
                        print(f"   {op_name:10s}: {avg_time*1000:8.2f} ms")
                        size_results[op_name] = {'time': avg_time}
                        
                except Exception as e:
                    print(f"   {op_name:10s}: å¤±è´¥ ({e})")
                    size_results[op_name] = {'error': str(e)}
            
            results[size] = size_results
        
        self.results['matrix_operations'] = results
    
    def test_neural_network_operations(self):
        """æµ‹è¯•ç¥ç»ç½‘ç»œæ“ä½œæ€§èƒ½"""
        print("\nğŸ§  ç¥ç»ç½‘ç»œæ“ä½œæ€§èƒ½æµ‹è¯•")
        print("-" * 50)
        
        batch_sizes = [1, 8, 32, 128]
        seq_lengths = [128, 512, 1024, 2048]
        hidden_sizes = [768, 1024, 2048, 4096]
        
        results = {}
        
        for batch_size in batch_sizes:
            for seq_len in seq_lengths:
                for hidden_size in hidden_sizes:
                    if batch_size * seq_len * hidden_size > 100000000:  # è·³è¿‡è¿‡å¤§çš„é…ç½®
                        continue
                    
                    print(f"\né…ç½®: batch={batch_size}, seq_len={seq_len}, hidden={hidden_size}")
                    
                    # åˆ›å»ºè¾“å…¥æ•°æ®
                    x = torch.randn(batch_size, seq_len, hidden_size, device=self.device)
                    
                    # æµ‹è¯•ä¸åŒçš„ç¥ç»ç½‘ç»œå±‚
                    layers = {
                        'linear': nn.Linear(hidden_size, hidden_size).to(self.device),
                        'layernorm': nn.LayerNorm(hidden_size).to(self.device),
                        'gelu': nn.GELU(),
                        'softmax': nn.Softmax(dim=-1),
                    }
                    
                    config_results = {}
                    
                    for layer_name, layer in layers.items():
                        try:
                            avg_time, _ = self.benchmark(layer, x)
                            print(f"   {layer_name:10s}: {avg_time*1000:8.2f} ms")
                            config_results[layer_name] = {'time': avg_time}
                        except Exception as e:
                            print(f"   {layer_name:10s}: å¤±è´¥ ({e})")
                            config_results[layer_name] = {'error': str(e)}
                    
                    key = f"b{batch_size}_s{seq_len}_h{hidden_size}"
                    results[key] = config_results
        
        self.results['neural_network_operations'] = results
    
    def test_memory_bandwidth(self):
        """æµ‹è¯•æ˜¾å­˜å¸¦å®½"""
        print("\nğŸ’¾ æ˜¾å­˜å¸¦å®½æµ‹è¯•")
        print("-" * 50)
        
        sizes_mb = [1, 10, 100, 1000, 5000]  # MB
        results = {}
        
        for size_mb in sizes_mb:
            size_elements = size_mb * 1024 * 1024 // 4  # float32 = 4 bytes
            
            print(f"\næ•°æ®å¤§å°: {size_mb} MB")
            
            # åˆ›å»ºæ•°æ®
            data = torch.randn(size_elements, device='cpu')
            
            # æµ‹è¯• CPU -> DCU ä¼ è¾“
            try:
                start_time = time.time()
                for _ in range(self.test_runs):
                    gpu_data = data.to(self.device)
                torch.cuda.synchronize()
                end_time = time.time()
                
                avg_time = (end_time - start_time) / self.test_runs
                bandwidth = size_mb / avg_time / 1024  # GB/s
                print(f"   CPU->DCU: {avg_time*1000:8.2f} ms ({bandwidth:6.1f} GB/s)")
                
                # æµ‹è¯• DCU -> CPU ä¼ è¾“
                start_time = time.time()
                for _ in range(self.test_runs):
                    cpu_data = gpu_data.to('cpu')
                torch.cuda.synchronize()
                end_time = time.time()
                
                avg_time = (end_time - start_time) / self.test_runs
                bandwidth = size_mb / avg_time / 1024  # GB/s
                print(f"   DCU->CPU: {avg_time*1000:8.2f} ms ({bandwidth:6.1f} GB/s)")
                
                results[size_mb] = {
                    'cpu_to_dcu': {'time': avg_time, 'bandwidth': bandwidth},
                    'dcu_to_cpu': {'time': avg_time, 'bandwidth': bandwidth}
                }
                
            except Exception as e:
                print(f"   ä¼ è¾“å¤±è´¥: {e}")
                results[size_mb] = {'error': str(e)}
        
        self.results['memory_bandwidth'] = results
    
    def test_transformer_attention(self):
        """æµ‹è¯•Transformeræ³¨æ„åŠ›æœºåˆ¶æ€§èƒ½"""
        print("\nğŸ” Transformeræ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•")
        print("-" * 50)
        
        configs = [
            {'batch': 1, 'seq_len': 512, 'heads': 8, 'dim': 64},
            {'batch': 4, 'seq_len': 1024, 'heads': 12, 'dim': 64},
            {'batch': 8, 'seq_len': 2048, 'heads': 16, 'dim': 64},
        ]
        
        results = {}
        
        for config in configs:
            batch_size = config['batch']
            seq_len = config['seq_len']
            num_heads = config['heads']
            head_dim = config['dim']
            
            print(f"\né…ç½®: batch={batch_size}, seq_len={seq_len}, heads={num_heads}, dim={head_dim}")
            
            # åˆ›å»ºè¾“å…¥
            hidden_size = num_heads * head_dim
            x = torch.randn(batch_size, seq_len, hidden_size, device=self.device)
            
            # åˆ›å»ºæ³¨æ„åŠ›å±‚
            attention = nn.MultiheadAttention(
                embed_dim=hidden_size,
                num_heads=num_heads,
                batch_first=True
            ).to(self.device)
            
            try:
                # æµ‹è¯•æ³¨æ„åŠ›è®¡ç®—
                def attention_forward():
                    return attention(x, x, x)
                
                avg_time, _ = self.benchmark(attention_forward)
                
                # è®¡ç®—ç†è®ºFLOPS
                flops = 4 * batch_size * seq_len * seq_len * hidden_size
                gflops = flops / avg_time / 1e9
                
                print(f"   æ³¨æ„åŠ›è®¡ç®—: {avg_time*1000:8.2f} ms ({gflops:6.1f} GFLOPS)")
                
                config_key = f"b{batch_size}_s{seq_len}_h{num_heads}_d{head_dim}"
                results[config_key] = {
                    'time': avg_time,
                    'gflops': gflops,
                    'config': config
                }
                
            except Exception as e:
                print(f"   æ³¨æ„åŠ›è®¡ç®—å¤±è´¥: {e}")
                config_key = f"b{batch_size}_s{seq_len}_h{num_heads}_d{head_dim}"
                results[config_key] = {'error': str(e)}
        
        self.results['transformer_attention'] = results
    
    def test_mixed_precision(self):
        """æµ‹è¯•æ··åˆç²¾åº¦æ€§èƒ½"""
        print("\nâš¡ æ··åˆç²¾åº¦æ€§èƒ½æµ‹è¯•")
        print("-" * 50)
        
        size = 2048
        dtypes = [torch.float32, torch.float16]
        
        results = {}
        
        for dtype in dtypes:
            print(f"\næ•°æ®ç±»å‹: {dtype}")
            
            a = torch.randn(size, size, device=self.device, dtype=dtype)
            b = torch.randn(size, size, device=self.device, dtype=dtype)
            
            # çŸ©é˜µä¹˜æ³•æµ‹è¯•
            avg_time, _ = self.benchmark(torch.mm, a, b)
            flops = 2 * size**3
            gflops = flops / avg_time / 1e9
            
            print(f"   çŸ©é˜µä¹˜æ³•: {avg_time*1000:8.2f} ms ({gflops:6.1f} GFLOPS)")
            
            results[str(dtype)] = {
                'time': avg_time,
                'gflops': gflops
            }
        
        self.results['mixed_precision'] = results
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹DCUæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        if torch.cuda.is_available():
            props = torch.cuda.get_device_properties(self.device)
            print(f"è®¾å¤‡: {props.name}")
            print(f"æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
            print(f"è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_functions = [
            self.test_matrix_operations,
            self.test_neural_network_operations,
            self.test_memory_bandwidth,
            self.test_transformer_attention,
            self.test_mixed_precision,
        ]
        
        for test_func in test_functions:
            try:
                test_func()
            except Exception as e:
                logger.error(f"æµ‹è¯• {test_func.__name__} å¤±è´¥: {e}", exc_info=True)
                print(f"âŒ æµ‹è¯• {test_func.__name__} å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary()
    
    def generate_summary(self):
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        print("\nğŸ“‹ æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"dcu_benchmark_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
        # æ˜¾ç¤ºå…³é”®æ€§èƒ½æŒ‡æ ‡
        if 'matrix_operations' in self.results:
            matrix_results = self.results['matrix_operations']
            if '2048' in matrix_results and 'matmul' in matrix_results['2048']:
                gflops = matrix_results['2048']['matmul'].get('gflops', 0)
                print(f"ğŸ”¥ çŸ©é˜µä¹˜æ³•æ€§èƒ½ (2048x2048): {gflops:.1f} GFLOPS")
        
        print("\nğŸ¯ æ€§èƒ½å»ºè®®:")
        print("   - å¦‚æœæ€§èƒ½ä½äºé¢„æœŸï¼Œè¯·æ£€æŸ¥DCUé©±åŠ¨ç‰ˆæœ¬")
        print("   - è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæå‡æ€§èƒ½")
        print("   - ä¼˜åŒ–æ‰¹æ¬¡å¤§å°ä»¥å……åˆ†åˆ©ç”¨DCUèµ„æº")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='DCUæ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--device', default='cuda:0', help='DCUè®¾å¤‡ID')
    parser.add_argument('--warmup', type=int, default=3, help='é¢„çƒ­æ¬¡æ•°')
    parser.add_argument('--runs', type=int, default=10, help='æµ‹è¯•æ¬¡æ•°')
    parser.add_argument('--profile', action='store_true', help='å¯ç”¨æ€§èƒ½åˆ†æ')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥DCUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ DCUè®¾å¤‡ä¸å¯ç”¨ï¼è¯·æ£€æŸ¥é©±åŠ¨å®‰è£…ã€‚")
        sys.exit(1)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark = BenchmarkSuite(
        device=args.device,
        warmup_runs=args.warmup,
        test_runs=args.runs
    )
    
    # è¿è¡Œæµ‹è¯•
    if args.profile:
        with DCUProfiler() as profiler:
            benchmark.run_all_tests()
        profiler.generate_report('dcu_benchmark_profile.html')
        print(f"ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: dcu_benchmark_profile.html")
    else:
        benchmark.run_all_tests()

if __name__ == "__main__":
    main() 