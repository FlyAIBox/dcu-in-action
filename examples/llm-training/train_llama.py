#!/usr/bin/env python3
"""
LLaMAæ¨¡å‹åœ¨æµ·å…‰DCUä¸Šçš„è®­ç»ƒç¤ºä¾‹
æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰é«˜çº§åŠŸèƒ½
"""

import os
import sys
import json
import time
import math
import argparse
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, Dict, List

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.cuda.amp import autocast, GradScaler

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    get_linear_schedule_with_warmup
)
from datasets import Dataset, load_dataset
import wandb

@dataclass
class ModelArguments:
    """æ¨¡å‹ç›¸å…³å‚æ•°"""
    model_name_or_path: Optional[str] = field(
        default="meta-llama/Llama-2-7b-hf",
        metadata={"help": "é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "æ¨¡å‹ç¼“å­˜ç›®å½•"}
    )
    use_flash_attention: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨Flash Attention"}
    )

@dataclass
class DataArguments:
    """æ•°æ®ç›¸å…³å‚æ•°"""
    dataset_name: Optional[str] = field(
        default="wikitext",
        metadata={"help": "æ•°æ®é›†åç§°"}
    )
    dataset_config: Optional[str] = field(
        default="wikitext-2-raw-v1",
        metadata={"help": "æ•°æ®é›†é…ç½®"}
    )
    max_seq_length: int = field(
        default=1024,
        metadata={"help": "æœ€å¤§åºåˆ—é•¿åº¦"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "æ•°æ®é¢„å¤„ç†è¿›ç¨‹æ•°"}
    )

@dataclass
class TrainingArguments:
    """è®­ç»ƒç›¸å…³å‚æ•°"""
    output_dir: str = field(
        default="./llama_output",
        metadata={"help": "è¾“å‡ºç›®å½•"}
    )
    num_train_epochs: int = field(
        default=3,
        metadata={"help": "è®­ç»ƒè½®æ•°"}
    )
    per_device_train_batch_size: int = field(
        default=4,
        metadata={"help": "æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°"}
    )
    gradient_accumulation_steps: int = field(
        default=4,
        metadata={"help": "æ¢¯åº¦ç´¯ç§¯æ­¥æ•°"}
    )
    learning_rate: float = field(
        default=2e-5,
        metadata={"help": "å­¦ä¹ ç‡"}
    )
    weight_decay: float = field(
        default=0.01,
        metadata={"help": "æƒé‡è¡°å‡"}
    )
    warmup_steps: int = field(
        default=500,
        metadata={"help": "é¢„çƒ­æ­¥æ•°"}
    )
    logging_steps: int = field(
        default=10,
        metadata={"help": "æ—¥å¿—è®°å½•é—´éš”"}
    )
    save_steps: int = field(
        default=500,
        metadata={"help": "æ¨¡å‹ä¿å­˜é—´éš”"}
    )
    eval_steps: int = field(
        default=500,
        metadata={"help": "è¯„ä¼°é—´éš”"}
    )
    fp16: bool = field(
        default=True,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨FP16æ··åˆç²¾åº¦"}
    )
    dataloader_num_workers: int = field(
        default=4,
        metadata={"help": "æ•°æ®åŠ è½½å™¨è¿›ç¨‹æ•°"}
    )
    remove_unused_columns: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ç§»é™¤æœªä½¿ç”¨çš„åˆ—"}
    )
    report_to: Optional[str] = field(
        default="wandb",
        metadata={"help": "å®éªŒè·Ÿè¸ªå·¥å…·"}
    )

class DCUTrainer:
    """DCUè®­ç»ƒå™¨"""
    
    def __init__(self, model_args, data_args, training_args):
        self.model_args = model_args
        self.data_args = data_args
        self.training_args = training_args
        
        # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
        self.setup_distributed()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
        
    def setup_distributed(self):
        """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
        if 'WORLD_SIZE' in os.environ:
            self.distributed = True
            self.local_rank = int(os.environ['LOCAL_RANK'])
            self.world_size = int(os.environ['WORLD_SIZE'])
            self.rank = int(os.environ['RANK'])
            
            # åˆå§‹åŒ–åˆ†å¸ƒå¼
            dist.init_process_group(backend='nccl')
            torch.cuda.set_device(self.local_rank)
            
            print(f"åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ: rank={self.rank}, local_rank={self.local_rank}, world_size={self.world_size}")
        else:
            self.distributed = False
            self.local_rank = 0
            self.world_size = 1
            self.rank = 0
    
    def load_tokenizer_and_model(self):
        """åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹"""
        print("ğŸ”„ åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹...")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_args.model_name_or_path,
            cache_dir=self.model_args.cache_dir,
            use_fast=True
        )
        
        # è®¾ç½®padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_args.model_name_or_path,
            cache_dir=self.model_args.cache_dir,
            torch_dtype=torch.float16 if self.training_args.fp16 else torch.float32,
            device_map=None,  # æ‰‹åŠ¨ç®¡ç†è®¾å¤‡
        )
        
        # ç§»åŠ¨æ¨¡å‹åˆ°å½“å‰è®¾å¤‡
        self.model = self.model.cuda(self.local_rank)
        
        # åˆ†å¸ƒå¼åŒ…è£…
        if self.distributed:
            self.model = DDP(self.model, device_ids=[self.local_rank])
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {self.model_args.model_name_or_path}")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        if self.rank == 0:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
    
    def prepare_dataset(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        print("ğŸ”„ å‡†å¤‡æ•°æ®é›†...")
        
        # åŠ è½½æ•°æ®é›†
        if self.data_args.dataset_name == "wikitext":
            dataset = load_dataset(
                self.data_args.dataset_name,
                self.data_args.dataset_config,
                cache_dir=self.model_args.cache_dir
            )
        else:
            # æ”¯æŒæœ¬åœ°æ•°æ®é›†
            dataset = load_dataset(
                "text",
                data_files=self.data_args.dataset_name,
                cache_dir=self.model_args.cache_dir
            )
        
        # æ•°æ®é¢„å¤„ç†å‡½æ•°
        def tokenize_function(examples):
            # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=self.data_args.max_seq_length,
                return_special_tokens_mask=True,
            )
            return tokenized
        
        # åˆ†ç»„å‡½æ•°ï¼Œå°†çŸ­æ–‡æœ¬åˆå¹¶åˆ°æœ€å¤§é•¿åº¦
        def group_texts(examples):
            concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
            total_length = len(concatenated_examples[list(examples.keys())[0]])
            
            # ä¸¢å¼ƒä¸è¶³ä¸€ä¸ªå®Œæ•´å—çš„æ•°æ®
            total_length = (total_length // self.data_args.max_seq_length) * self.data_args.max_seq_length
            
            result = {
                k: [t[i:i + self.data_args.max_seq_length] 
                    for i in range(0, total_length, self.data_args.max_seq_length)]
                for k, t in concatenated_examples.items()
            }
            
            # æ·»åŠ æ ‡ç­¾ï¼ˆè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­æ ‡ç­¾å°±æ˜¯input_idsï¼‰
            result["labels"] = result["input_ids"].copy()
            return result
        
        # åº”ç”¨åˆ†è¯
        tokenized_datasets = dataset.map(
            tokenize_function,
            batched=True,
            num_proc=self.data_args.preprocessing_num_workers,
            remove_columns=dataset["train"].column_names,
            desc="Tokenizing texts",
        )
        
        # åˆ†ç»„æ–‡æœ¬
        grouped_datasets = tokenized_datasets.map(
            group_texts,
            batched=True,
            num_proc=self.data_args.preprocessing_num_workers,
            desc="Grouping texts",
        )
        
        self.train_dataset = grouped_datasets["train"]
        self.eval_dataset = grouped_datasets["validation"] if "validation" in grouped_datasets else None
        
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: è®­ç»ƒæ ·æœ¬ {len(self.train_dataset)}")
        if self.eval_dataset:
            print(f"   éªŒè¯æ ·æœ¬: {len(self.eval_dataset)}")
    
    def create_optimizer_and_scheduler(self):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # åˆ›å»ºä¼˜åŒ–å™¨
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                "weight_decay": self.training_args.weight_decay,
            },
            {
                "params": [p for n, p in self.model.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        
        self.optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters,
            lr=self.training_args.learning_rate
        )
        
        # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
        num_training_steps = (
            len(self.train_dataset) // 
            (self.training_args.per_device_train_batch_size * self.world_size * self.training_args.gradient_accumulation_steps)
        ) * self.training_args.num_train_epochs
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=self.training_args.warmup_steps,
            num_training_steps=num_training_steps
        )
        
        # æ··åˆç²¾åº¦
        if self.training_args.fp16:
            self.scaler = GradScaler()
        
        print(f"âœ… ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºå®Œæˆ")
        print(f"   æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps}")
        print(f"   é¢„çƒ­æ­¥æ•°: {self.training_args.warmup_steps}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_sampler = DistributedSampler(self.train_dataset) if self.distributed else None
        train_dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.training_args.per_device_train_batch_size,
            sampler=train_sampler,
            num_workers=self.training_args.dataloader_num_workers,
            pin_memory=True,
            collate_fn=DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer, 
                mlm=False
            )
        )
        
        # åˆå§‹åŒ–æ—¥å¿—
        if self.rank == 0 and self.training_args.report_to == "wandb":
            wandb.init(
                project="dcu-llama-training",
                config={
                    **vars(self.model_args),
                    **vars(self.data_args),
                    **vars(self.training_args)
                }
            )
        
        # è®­ç»ƒå¾ªç¯
        global_step = 0
        self.model.train()
        
        for epoch in range(self.training_args.num_train_epochs):
            if self.distributed:
                train_sampler.set_epoch(epoch)
            
            epoch_loss = 0
            epoch_start_time = time.time()
            
            for step, batch in enumerate(train_dataloader):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch = {k: v.cuda(self.local_rank, non_blocking=True) 
                        for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                if self.training_args.fp16:
                    with autocast():
                        outputs = self.model(**batch)
                        loss = outputs.loss / self.training_args.gradient_accumulation_steps
                else:
                    outputs = self.model(**batch)
                    loss = outputs.loss / self.training_args.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                if self.training_args.fp16:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                epoch_loss += loss.item()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (step + 1) % self.training_args.gradient_accumulation_steps == 0:
                    if self.training_args.fp16:
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()
                    
                    self.lr_scheduler.step()
                    self.optimizer.zero_grad()
                    global_step += 1
                    
                    # æ—¥å¿—è®°å½•
                    if global_step % self.training_args.logging_steps == 0:
                        lr = self.lr_scheduler.get_last_lr()[0]
                        
                        if self.rank == 0:
                            print(f"Epoch {epoch+1}, Step {global_step}, Loss: {loss.item():.4f}, LR: {lr:.2e}")
                            
                            if self.training_args.report_to == "wandb":
                                wandb.log({
                                    "train/loss": loss.item(),
                                    "train/learning_rate": lr,
                                    "train/epoch": epoch + 1,
                                    "train/global_step": global_step
                                })
                    
                    # ä¿å­˜æ¨¡å‹
                    if global_step % self.training_args.save_steps == 0:
                        self.save_model(global_step)
            
            # æ¯ä¸ªepochç»“æŸæ—¶çš„æ—¥å¿—
            epoch_time = time.time() - epoch_start_time
            avg_loss = epoch_loss / len(train_dataloader)
            
            if self.rank == 0:
                print(f"Epoch {epoch+1} å®Œæˆ")
                print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
                print(f"  ç”¨æ—¶: {epoch_time:.2f}s")
                
                if self.training_args.report_to == "wandb":
                    wandb.log({
                        "train/epoch_loss": avg_loss,
                        "train/epoch_time": epoch_time,
                        "train/epoch": epoch + 1
                    })
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model("final")
        
        if self.rank == 0:
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    def save_model(self, step):
        """ä¿å­˜æ¨¡å‹"""
        if self.rank == 0:
            output_dir = Path(self.training_args.output_dir) / f"checkpoint-{step}"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
            if self.distributed:
                self.model.module.save_pretrained(output_dir)
            else:
                self.model.save_pretrained(output_dir)
            
            self.tokenizer.save_pretrained(output_dir)
            
            # ä¿å­˜è®­ç»ƒå‚æ•°
            with open(output_dir / "training_args.json", "w") as f:
                json.dump(vars(self.training_args), f, indent=2)
            
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")

def main():
    parser = argparse.ArgumentParser(description="LLaMAæ¨¡å‹DCUè®­ç»ƒ")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_name_or_path", type=str, default="meta-llama/Llama-2-7b-hf")
    parser.add_argument("--cache_dir", type=str, default=None)
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--dataset_name", type=str, default="wikitext")
    parser.add_argument("--dataset_config", type=str, default="wikitext-2-raw-v1")
    parser.add_argument("--max_seq_length", type=int, default=1024)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./llama_output")
    parser.add_argument("--num_train_epochs", type=int, default=3)
    parser.add_argument("--per_device_train_batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--warmup_steps", type=int, default=500)
    parser.add_argument("--logging_steps", type=int, default=10)
    parser.add_argument("--save_steps", type=int, default=500)
    parser.add_argument("--fp16", action="store_true", default=True)
    parser.add_argument("--report_to", type=str, default="none")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    model_args = ModelArguments(
        model_name_or_path=args.model_name_or_path,
        cache_dir=args.cache_dir
    )
    
    data_args = DataArguments(
        dataset_name=args.dataset_name,
        dataset_config=args.dataset_config,
        max_seq_length=args.max_seq_length
    )
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        fp16=args.fp16,
        report_to=args.report_to
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DCUTrainer(model_args, data_args, training_args)
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    trainer.load_tokenizer_and_model()
    trainer.prepare_dataset()
    trainer.create_optimizer_and_scheduler()
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == "__main__":
    main() 