#!/usr/bin/env python3
"""
ChatGLMæ¨¡å‹åœ¨æµ·å…‰DCUä¸Šçš„æ¨ç†ç¤ºä¾‹
æ”¯æŒChatGLM2-6Bå’ŒChatGLM3-6Bæ¨¡å‹
"""

import torch
import time
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM

class ChatGLMInference:
    def __init__(self, model_name="THUDM/chatglm3-6b", device="auto"):
        """
        åˆå§‹åŒ–ChatGLMæ¨ç†å¼•æ“
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡ç±»å‹ï¼Œ"auto"ä¸ºè‡ªåŠ¨é€‰æ‹©
        """
        self.model_name = model_name
        self.device = device
        self.tokenizer = None
        self.model = None
        self.chat_history = []
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map=self.device,
                trust_remote_code=True
            )
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
            # æ‰“å°æ¨¡å‹ä¿¡æ¯
            total_params = sum(p.numel() for p in self.model.parameters())
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params/1e9:.2f}B")
            
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"ğŸ’¾ æ˜¾å­˜å ç”¨: {memory_used:.2f} GB")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def generate_response(self, prompt, max_length=2048, temperature=0.7, top_p=0.95):
        """
        ç”Ÿæˆå›å¤
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleusé‡‡æ ·å‚æ•°
            
        Returns:
            str: ç”Ÿæˆçš„å›å¤
        """
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(prompt, return_tensors="pt")
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # è®°å½•æ¨ç†æ—¶é—´
            start_time = time.time()
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # è®¡ç®—æ¨ç†æ—¶é—´å’Œé€Ÿåº¦
            inference_time = time.time() - start_time
            output_tokens = len(outputs[0]) - len(inputs['input_ids'][0])
            tokens_per_second = output_tokens / inference_time
            
            print(f"â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.2f}s")
            print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} tokens/s")
            
            return response, {
                'inference_time': inference_time,
                'tokens_per_second': tokens_per_second,
                'output_tokens': output_tokens
            }
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return None, None
    
    def chat(self, user_input, history_length=10):
        """
        å¯¹è¯æ¨¡å¼ï¼Œç»´æŠ¤å¯¹è¯å†å²
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            history_length: ä¿ç•™çš„å†å²å¯¹è¯è½®æ•°
            
        Returns:
            str: åŠ©æ‰‹å›å¤
        """
        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²
        self.chat_history.append(f"ç”¨æˆ·: {user_input}")
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™æœ€è¿‘çš„å¯¹è¯ï¼‰
        recent_history = self.chat_history[-history_length*2:]  # æ¯è½®åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹
        context = "\n".join(recent_history)
        
        # æ„å»ºæç¤º
        prompt = f"{context}\nåŠ©æ‰‹: "
        
        # ç”Ÿæˆå›å¤
        full_response, stats = self.generate_response(prompt)
        
        if full_response:
            # æå–åŠ©æ‰‹å›å¤éƒ¨åˆ†
            assistant_reply = full_response.split("åŠ©æ‰‹:")[-1].strip()
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.chat_history.append(f"åŠ©æ‰‹: {assistant_reply}")
            
            return assistant_reply, stats
        
        return None, None
    
    def batch_inference(self, prompts, batch_size=4):
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            list: å›å¤åˆ—è¡¨
        """
        results = []
        
        for i in range(0, len(prompts), batch_size):
            batch = prompts[i:i+batch_size]
            
            # æ‰¹é‡ç¼–ç 
            inputs = self.tokenizer(
                batch, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    pad_token_id=self.tokenizer.eos_token_id,
                    do_sample=True,
                    temperature=0.7
                )
            
            # è§£ç ç»“æœ
            batch_results = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
            results.extend(batch_results)
            
            print(f"âœ… å®Œæˆæ‰¹æ¬¡ {i//batch_size + 1}/{(len(prompts)-1)//batch_size + 1}")
        
        return results
    
    def benchmark(self, test_prompts=None, iterations=5):
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            test_prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
            iterations: æµ‹è¯•è¿­ä»£æ¬¡æ•°
        """
        if test_prompts is None:
            test_prompts = [
                "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
                "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
                "Pythonç¼–ç¨‹è¯­è¨€æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "æè¿°æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯",
                "ä»€ä¹ˆæ˜¯æµ·å…‰DCUåŠ é€Ÿå¡ï¼Ÿ"
            ]
        
        print(f"ğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({iterations}è½®)...")
        
        total_time = 0
        total_tokens = 0
        
        for i, prompt in enumerate(test_prompts):
            print(f"\næµ‹è¯• {i+1}/{len(test_prompts)}: {prompt[:30]}...")
            
            iter_times = []
            iter_tokens = []
            
            for j in range(iterations):
                response, stats = self.generate_response(prompt)
                if stats:
                    iter_times.append(stats['inference_time'])
                    iter_tokens.append(stats['output_tokens'])
            
            # è®¡ç®—å¹³å‡å€¼
            avg_time = sum(iter_times) / len(iter_times)
            avg_tokens = sum(iter_tokens) / len(iter_tokens)
            avg_speed = avg_tokens / avg_time
            
            total_time += avg_time
            total_tokens += avg_tokens
            
            print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}s")
            print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_speed:.1f} tokens/s")
        
        # æ€»ä½“ç»Ÿè®¡
        overall_speed = total_tokens / total_time
        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"   æ€»æ¨ç†æ—¶é—´: {total_time:.2f}s")
        print(f"   æ€»ç”Ÿæˆtoken: {total_tokens:.0f}")
        print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {overall_speed:.1f} tokens/s")

def interactive_chat(model_name="THUDM/chatglm3-6b"):
    """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
    print("ğŸ¤– ChatGLMäº¤äº’å¼å¯¹è¯æ¨¡å¼")
    print("è¾“å…¥ 'exit', 'quit' æˆ– 'é€€å‡º' æ¥ç»“æŸå¯¹è¯\n")
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    engine = ChatGLMInference(model_name)
    
    while True:
        user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()
        
        if user_input.lower() in ['exit', 'quit', 'é€€å‡º', '']:
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
        
        # ç”Ÿæˆå›å¤
        reply, stats = engine.chat(user_input)
        
        if reply:
            print(reply)
            if stats:
                print(f"   (â±ï¸ {stats['inference_time']:.1f}s, ğŸš€ {stats['tokens_per_second']:.1f} tokens/s)")
        else:
            print("æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯ã€‚")
        
        print()

def main():
    parser = argparse.ArgumentParser(description="ChatGLMæ¨¡å‹æ¨ç†")
    parser.add_argument("--model", default="THUDM/chatglm3-6b", help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--mode", choices=["chat", "benchmark", "test"], default="chat", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--prompt", type=str, help="å•æ¬¡æ¨ç†çš„æç¤º")
    
    args = parser.parse_args()
    
    if args.mode == "chat":
        interactive_chat(args.model)
    elif args.mode == "benchmark":
        engine = ChatGLMInference(args.model)
        engine.benchmark()
    elif args.mode == "test":
        if args.prompt:
            engine = ChatGLMInference(args.model)
            response, stats = engine.generate_response(args.prompt)
            print(f"å›å¤: {response}")
            if stats:
                print(f"ç»Ÿè®¡: {stats}")
        else:
            print("æµ‹è¯•æ¨¡å¼éœ€è¦æä¾› --prompt å‚æ•°")

if __name__ == "__main__":
    main() 