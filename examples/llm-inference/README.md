# ğŸš€ æµ·å…‰DCU K100-AIå¤§æ¨¡å‹æ¨ç†åŸºå‡†æµ‹è¯•ç³»ç»Ÿ

> **ç‰ˆæœ¬**ï¼šv2.0   **æœ€åæ›´æ–°**ï¼š2024-12-19
>
> æœ¬æ–‡æ¡£æä¾›æµ·å…‰DCU K100-AIåŠ é€Ÿå¡ä¸Šè¿›è¡Œå¤§æ¨¡å‹æ¨ç†æµ‹è¯„çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæ¶µç›–**vLLMã€SGLangã€Xinference**ä¸‰å¤§æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒ**DeepSeekã€Qwen**ç­‰ä¸»æµå¼€æºæ¨¡å‹çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

### æ ¸å¿ƒåŠŸèƒ½
- **ğŸ”§ å¤šæ¡†æ¶æ”¯æŒ**ï¼švLLMã€SGLangã€Xinference
- **ğŸ“Š å…¨é¢è¯„ä¼°**ï¼šå•å¡/8å¡ååé‡ã€å¹¶å‘èƒ½åŠ›ã€å»¶è¿Ÿåˆ†æ
- **âš¡ æ€§èƒ½ä¼˜åŒ–**ï¼šè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºè®®ã€ç¡¬ä»¶ç›‘æ§
- **ğŸ“ˆ å¯è§†åŒ–æŠ¥å‘Š**ï¼šè¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Š
- **ğŸ› ï¸ è‡ªåŠ¨åŒ–æµ‹è¯•**ï¼šå®Œæ•´çš„CI/CDæµ‹è¯•æµç¨‹

### æµ‹è¯•ç»´åº¦
| ç»´åº¦ | æŒ‡æ ‡ | è¯´æ˜ |
|------|------|------|
| **ååé‡** | tokens/s | æ¯ç§’å¤„ç†çš„tokenæ•°é‡ |
| **å»¶è¿Ÿ** | ms | P50/P90/P99å“åº”æ—¶é—´ |
| **å¹¶å‘èƒ½åŠ›** | concurrent requests | æœ€å¤§æ”¯æŒå¹¶å‘æ•° |
| **èµ„æºåˆ©ç”¨ç‡** | GPU/CPU/Memory % | ç¡¬ä»¶èµ„æºä½¿ç”¨æƒ…å†µ |
| **åŠŸè€—æ•ˆç‡** | tokens/J | æ¯ç„¦è€³èƒ½å¤„ç†çš„tokenæ•° |

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    subgraph "ç¡¬ä»¶å±‚"
        A[æµ·å…‰DCU K100-AI Ã— 8]
        B[64GB HBM per GPU]
        C[ROCm 6.3.8+]
    end
    
    subgraph "æ¨ç†æ¡†æ¶å±‚"
        D[vLLM Engine]
        E[SGLang Engine]
        F[Xinference Engine]
    end
    
    subgraph "æ¨¡å‹å±‚"
        G[DeepSeek Models]
        H[Qwen Models]
        I[Custom Models]
    end
    
    subgraph "æµ‹è¯•ç³»ç»Ÿ"
        J[Benchmark Controller]
        K[Performance Monitor]
        L[Report Generator]
    end
    
    A --> D
    A --> E
    A --> F
    D --> G
    E --> H
    F --> I
    J --> K
    K --> L
```

---

## ğŸ“‹ æµ‹è¯•ç¯å¢ƒè§„èŒƒ

### ç¡¬ä»¶é…ç½®
| ç»„ä»¶ | è§„æ ¼ | è¦æ±‚ |
|------|------|------|
| **åŠ é€Ÿå¡** | æµ·å…‰DCU K100-AI Ã— 8 | 64GB HBM, PCIe 4.0 Ã—16 |
| **CPU** | 2Ã—64C AMD EPYC 7T83 | NUMAä¼˜åŒ– |
| **å†…å­˜** | 512GB DDR4-3200 | ECCæ”¯æŒ |
| **å­˜å‚¨** | NVMe SSD â‰¥2TB | é«˜IOPS |
| **ç½‘ç»œ** | 100GbE | åˆ†å¸ƒå¼æµ‹è¯• |

### è½¯ä»¶ç¯å¢ƒ
| ç»„ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **OS** | Ubuntu 22.04.4 LTS | å†…æ ¸5.15.0+ |
| **ROCm** | 6.3.8+ | é©±åŠ¨å…¼å®¹æ€§ |
| **Python** | 3.10+ | è™šæ‹Ÿç¯å¢ƒéš”ç¦» |
| **Docker** | 20.10+ | å®¹å™¨åŒ–éƒ¨ç½² |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/dcu-in-action.git
cd dcu-in-action/examples/llm-inference

# ä¸€é”®ç¯å¢ƒé…ç½®
./scripts/setup_environment.sh

# éªŒè¯DCUç¯å¢ƒ
./scripts/check_dcu_status.sh
```

### 2. è¿è¡ŒåŸºå‡†æµ‹è¯•
```bash
# å•å¡æµ‹è¯•
./scripts/run_single_gpu_benchmark.sh

# 8å¡æµ‹è¯•
./scripts/run_multi_gpu_benchmark.sh

# ç”ŸæˆæŠ¥å‘Š
python tools/generate_comprehensive_report.py
```

---

## ğŸ“Š åŸºå‡†æµ‹è¯•æ–¹æ³•å­¦

### æµ‹è¯•åŸåˆ™
1. **å¯é‡ç°æ€§**ï¼šå›ºå®šè½¯ä»¶ç‰ˆæœ¬ã€æµ‹è¯•å‚æ•°
2. **å…¬å¹³æ€§**ï¼šç»Ÿä¸€æµ‹è¯•æ¡ä»¶ã€é¢„çƒ­æœºåˆ¶
3. **å…¨é¢æ€§**ï¼šå¤šç»´åº¦æŒ‡æ ‡ã€å¤šåœºæ™¯è¦†ç›–
4. **æƒå¨æ€§**ï¼šå‚è€ƒMLPerfæ ‡å‡†ã€è¡Œä¸šæœ€ä½³å®è·µ

### æµ‹è¯•åœºæ™¯
| åœºæ™¯ | è¾“å…¥é•¿åº¦ | è¾“å‡ºé•¿åº¦ | å¹¶å‘æ•° | ç”¨é€” |
|------|----------|----------|--------|------|
| **çŸ­æ–‡æœ¬** | 64 tokens | 64 tokens | 1-32 | å¯¹è¯åº”ç”¨ |
| **ä¸­ç­‰æ–‡æœ¬** | 256 tokens | 256 tokens | 1-64 | å†…å®¹ç”Ÿæˆ |
| **é•¿æ–‡æœ¬** | 1024 tokens | 512 tokens | 1-16 | æ–‡æ¡£åˆ†æ |
| **æé™å‹æµ‹** | 128 tokens | 128 tokens | 1-512 | æ€§èƒ½ä¸Šé™ |

### è¯„ä¼°æŒ‡æ ‡

#### æ ¸å¿ƒæŒ‡æ ‡
- **ååé‡ (Throughput)**ï¼š`æ€»tokenæ•° / æ€»æ—¶é—´`
- **å»¶è¿Ÿ (Latency)**ï¼šè¯·æ±‚å“åº”æ—¶é—´åˆ†å¸ƒ
- **é¦–tokenå»¶è¿Ÿ (TTFT)**ï¼šTime To First Token
- **tokené—´å»¶è¿Ÿ (ITL)**ï¼šInter-Token Latency

#### æ‰©å±•æŒ‡æ ‡
- **GPUåˆ©ç”¨ç‡**ï¼šè®¡ç®—èµ„æºä½¿ç”¨æ•ˆç‡
- **å†…å­˜ä½¿ç”¨ç‡**ï¼šæ˜¾å­˜å ç”¨æƒ…å†µ
- **åŠŸè€—æ•ˆç‡**ï¼štokens/J
- **æˆæœ¬æ•ˆç‡**ï¼štokens/$/hour

---

## ğŸ”§ æ¡†æ¶é…ç½®ä¸ä¼˜åŒ–

### vLLM é…ç½®
```bash
# å¯åŠ¨vLLMæœåŠ¡
python -m vllm.entrypoints.openai.api_server \
  --model /path/to/model \
  --dtype float16 \
  --tensor-parallel-size 8 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 4096 \
  --swap-space 16 \
  --port 8000
```

### SGLang é…ç½®
```bash
# å¯åŠ¨SGLangæœåŠ¡
python -m sglang.launch_server \
  --model-path /path/to/model \
  --tokenizer-path /path/to/tokenizer \
  --tp-size 8 \
  --mem-fraction-static 0.8 \
  --port 8001
```

### Xinference é…ç½®
```bash
# å¯åŠ¨XinferenceæœåŠ¡
xinference-local --host 0.0.0.0 --port 8002
# æ³¨å†Œæ¨¡å‹
curl -X POST "http://localhost:8002/v1/models" \
  -H "Content-Type: application/json" \
  -d '{"model_name": "custom-model", "model_path": "/path/to/model"}'
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è½¯ä»¶å±‚ä¼˜åŒ–

#### 1. å†…å­˜ä¼˜åŒ–
```bash
# Flash Attention 2.0
export VLLM_ATTENTION_BACKEND=FLASHINFER

# åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–
--max-batch-total-tokens 16384
--max-batch-prefill-tokens 8192
```

#### 2. å¹¶è¡Œä¼˜åŒ–
```bash
# å¼ é‡å¹¶è¡Œ
--tensor-parallel-size 8

# æµæ°´çº¿å¹¶è¡Œï¼ˆè¶…å¤§æ¨¡å‹ï¼‰
--pipeline-parallel-size 2
```

#### 3. é‡åŒ–ä¼˜åŒ–
```bash
# AWQé‡åŒ–ï¼ˆæ¨èï¼‰
--quantization awq

# GPTQé‡åŒ–
--quantization gptq
```

### ç¡¬ä»¶å±‚ä¼˜åŒ–

#### 1. NUMAä¼˜åŒ–
```bash
# CPUç»‘æ ¸
numactl --cpunodebind=0 --membind=0 python inference_server.py

# GPUæ‹“æ‰‘ä¼˜åŒ–
export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
```

#### 2. PCIeä¼˜åŒ–
```bash
# æ£€æŸ¥PCIeå¸¦å®½
lspci -vvv | grep -A3 "K100"

# å¼€å¯Large BAR
echo 1 > /sys/bus/pci/devices/*/enable
```

#### 3. åŠŸè€—ä¼˜åŒ–
```bash
# è®¾ç½®åŠŸè€—æ¨¡å¼
rocm-smi --setpoweroverdrive 15

# æ¸©åº¦ç›‘æ§
watch -n 1 'rocm-smi --showtemp'
```

---

## ğŸ› ï¸ è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹

### æµ‹è¯•è„šæœ¬ç»“æ„
```
examples/llm-inference/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_environment.sh          # ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ run_single_gpu_benchmark.sh   # å•å¡æµ‹è¯•
â”‚   â”œâ”€â”€ run_multi_gpu_benchmark.sh    # å¤šå¡æµ‹è¯•
â”‚   â”œâ”€â”€ check_dcu_status.sh          # ç¡¬ä»¶æ£€æŸ¥
â”‚   â””â”€â”€ cleanup.sh                   # æ¸…ç†ç¯å¢ƒ
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ benchmark_controller.py       # æµ‹è¯•æ§åˆ¶å™¨
â”‚   â”œâ”€â”€ performance_monitor.py        # æ€§èƒ½ç›‘æ§
â”‚   â””â”€â”€ load_generator.py            # è´Ÿè½½ç”Ÿæˆå™¨
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ generate_comprehensive_report.py  # æŠ¥å‘Šç”Ÿæˆ
â”‚   â”œâ”€â”€ analyze_performance.py            # æ€§èƒ½åˆ†æ
â”‚   â””â”€â”€ visualize_results.py             # ç»“æœå¯è§†åŒ–
â””â”€â”€ configs/
    â”œâ”€â”€ vllm_config.yaml              # vLLMé…ç½®
    â”œâ”€â”€ sglang_config.yaml            # SGLangé…ç½®
    â””â”€â”€ xinference_config.yaml        # Xinferenceé…ç½®
```

---

## ğŸ“Š ç»“æœåˆ†æä¸æŠ¥å‘Š

### æ€§èƒ½åŸºå‡†æ•°æ®ç¤ºä¾‹

#### å•å¡æ€§èƒ½ (DCU K100-AI)
| æ¨¡å‹ | æ¡†æ¶ | å¹¶å‘ | ååé‡ (tokens/s) | P50å»¶è¿Ÿ (ms) | GPUåˆ©ç”¨ç‡ (%) |
|------|------|------|------------------|--------------|---------------|
| DeepSeek-7B | vLLM | 32 | 1,880 | 47 | 85 |
| DeepSeek-7B | SGLang | 32 | 1,750 | 55 | 82 |
| DeepSeek-7B | Xinference | 32 | 1,650 | 62 | 78 |
| Qwen-7B | vLLM | 32 | 1,920 | 45 | 87 |

#### 8å¡æ€§èƒ½ (æ•´æœº)
| æ¨¡å‹ | æ¡†æ¶ | å¹¶å‘ | ååé‡ (tokens/s) | æ‰©å±•æ•ˆç‡ (%) | åŠŸè€— (W) |
|------|------|------|------------------|--------------|----------|
| DeepSeek-7B | vLLM | 256 | 14,720 | 98.0 | 1,760 |
| DeepSeek-7B | SGLang | 256 | 13,600 | 97.1 | 1,680 |
| Qwen-7B | vLLM | 256 | 15,040 | 98.2 | 1,800 |

### å¯è§†åŒ–æŠ¥å‘Š
- **æ€§èƒ½è¶‹åŠ¿å›¾**ï¼šååé‡vså¹¶å‘æ•°
- **å»¶è¿Ÿåˆ†å¸ƒå›¾**ï¼šP50/P90/P99å»¶è¿Ÿåˆ†æ
- **èµ„æºåˆ©ç”¨ç‡**ï¼šGPU/CPU/å†…å­˜ä½¿ç”¨æƒ…å†µ
- **åŠŸè€—æ•ˆç‡**ï¼šæ€§èƒ½åŠŸè€—æ¯”å¯¹æ¯”

---

## ğŸ” æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **HIPé”™è¯¯** | `hipErrorNoBinaryForGpu` | å‡çº§ROCmç‰ˆæœ¬ï¼Œæ£€æŸ¥GPUå…¼å®¹æ€§ |
| **OOMé”™è¯¯** | æ˜¾å­˜ä¸è¶³ | å‡å°batch sizeï¼Œå¯ç”¨swap-space |
| **æ€§èƒ½ä¸‹é™** | ååé‡ä½äºé¢„æœŸ | æ£€æŸ¥æ¸©åº¦ã€åŠŸè€—è®¾ç½®ã€NUMAé…ç½® |
| **å»¶è¿Ÿè¿‡é«˜** | å“åº”æ—¶é—´å¼‚å¸¸ | ä¼˜åŒ–å¹¶å‘è®¾ç½®ï¼Œæ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ |

### ç›‘æ§è„šæœ¬
```bash
# å®æ—¶ç›‘æ§DCUçŠ¶æ€
watch -n 2 'rocm-smi -a'

# æ€§èƒ½è®¡æ•°å™¨ç›‘æ§
rocm-smi --showpids --showcompute --showtemp --showfan

# ç³»ç»Ÿèµ„æºç›‘æ§
htop && iotop && nethogs
```

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

### æŠ€æœ¯è§„èŒƒ
- [æµ·å…‰DCU K100-AIæŠ€æœ¯ç™½çš®ä¹¦](docs/dcu-k100-ai-whitepaper.pdf)
- [ROCmå¼€å‘è€…æŒ‡å—](https://rocmdocs.amd.com/)
- [vLLMæ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://docs.vllm.ai/en/latest/performance/)

### æœ€ä½³å®è·µ
- [MLPerfæ¨ç†åŸºå‡†æµ‹è¯•æ ‡å‡†](https://mlcommons.org/en/inference/)
- [å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯](docs/llm-inference-optimization.md)
- [DCUé›†ç¾¤éƒ¨ç½²æŒ‡å—](docs/dcu-cluster-deployment.md)

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPRæ¥æ”¹è¿›æœ¬æµ‹è¯•ç³»ç»Ÿï¼š

1. **Bugä¿®å¤**ï¼šæ€§èƒ½å¼‚å¸¸ã€ç¯å¢ƒé…ç½®é—®é¢˜
2. **åŠŸèƒ½å¢å¼º**ï¼šæ–°æ¡†æ¶æ”¯æŒã€ä¼˜åŒ–ç®—æ³•
3. **æ–‡æ¡£å®Œå–„**ï¼šä½¿ç”¨æŒ‡å—ã€æœ€ä½³å®è·µ

è¯¦è§ [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦è§ [LICENSE](../../LICENSE) æ–‡ä»¶ã€‚

---

**ğŸ¯ æµ‹è¯•ç›®æ ‡ï¼š**
- å•å¡ååé‡ > 1,800 tokens/s
- 8å¡çº¿æ€§æ‰©å±•æ•ˆç‡ > 95%
- P99å»¶è¿Ÿ < 100ms
- GPUåˆ©ç”¨ç‡ > 85%

**ğŸ“ æŠ€æœ¯æ”¯æŒï¼š** support@dcu-ai.com 