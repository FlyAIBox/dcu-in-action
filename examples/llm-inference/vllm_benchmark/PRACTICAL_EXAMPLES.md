# ğŸ› ï¸ vLLMåŸºå‡†æµ‹è¯•æ¡†æ¶å®æˆ˜ç¤ºä¾‹

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†vLLMåŸºå‡†æµ‹è¯•æ¡†æ¶çš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ï¼Œæ¶µç›–ä»åŸºç¡€æµ‹è¯•åˆ°é«˜çº§åˆ†æçš„å®Œæ•´æµç¨‹ï¼Œå¸®åŠ©åˆå­¦è€…å¿«é€Ÿä¸Šæ‰‹å¹¶æŒæ¡æœ€ä½³å®è·µã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºç¡€æ€§èƒ½æµ‹è¯•

```bash
# 1. å¯åŠ¨vLLMæœåŠ¡
vllm serve microsoft/DialoGPT-medium \
  --host 0.0.0.0 \
  --port 8000 \
  --swap-space 16 \
  --disable-log-requests

# 2. è¿è¡ŒåŸºç¡€æµ‹è¯•
python3 benchmark_serving.py \
  --backend vllm \
  --model microsoft/DialoGPT-medium \
  --dataset-name random \
  --num-prompts 100 \
  --request-rate 10 \
  --random-input-len 512 \
  --random-output-len 256
```

**é¢„æœŸè¾“å‡º**ï¼š
```
============= Serving Benchmark Result =============
Successful requests:                100
Benchmark duration (s):            12.50
Total input tokens:                 51200
Total generated tokens:             25600
Request throughput (req/s):         8.00
Output token throughput (tok/s):    2048.00
Total Token throughput (tok/s):     6144.00

------------------ Time to First Token ------------------
Mean TTFT (ms):                     125.30
Median TTFT (ms):                   120.50
P99 TTFT (ms):                      180.20
```

---

## ğŸ“Š ä¸åŒæµ‹è¯•åœºæ™¯ç¤ºä¾‹

### ç¤ºä¾‹2ï¼šå¹¶å‘æ€§èƒ½æµ‹è¯•

```bash
# æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«çš„æ€§èƒ½è¡¨ç°
for concurrency in 1 4 8 16 32; do
  echo "Testing concurrency: $concurrency"
  python3 benchmark_serving.py \
    --backend vllm \
    --model your_model_name \
    --dataset-name sharegpt \
    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
    --num-prompts $((concurrency * 10)) \
    --max-concurrency $concurrency \
    --request-rate inf \
    --save-result \
    --result-filename "concurrency_${concurrency}.json"
done
```

### ç¤ºä¾‹3ï¼šä¸åŒè¾“å…¥é•¿åº¦æµ‹è¯•

```bash
# æµ‹è¯•ä¸åŒè¾“å…¥é•¿åº¦å¯¹æ€§èƒ½çš„å½±å“
for input_len in 256 512 1024 2048; do
  python3 benchmark_serving.py \
    --backend vllm \
    --model your_model_name \
    --dataset-name random \
    --num-prompts 50 \
    --random-input-len $input_len \
    --random-output-len 256 \
    --request-rate 5 \
    --save-result \
    --result-filename "input_len_${input_len}.json"
done
```

### ç¤ºä¾‹4ï¼šæµé‡æ¨¡æ‹Ÿæµ‹è¯•

```bash
# æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è®¿é—®æ¨¡å¼
python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 1000 \
  --request-rate 20 \
  --burstiness 0.8 \
  --max-concurrency 16 \
  --save-result \
  --metadata environment=production load_test=true
```

---

## ğŸ”§ é«˜çº§é…ç½®ç¤ºä¾‹

### ç¤ºä¾‹5ï¼šSLAå’ŒGoodputæµ‹è¯•

```bash
# æµ‹è¯•æœåŠ¡è´¨é‡æŒ‡æ ‡
python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 500 \
  --request-rate 15 \
  --goodput ttft:200 tpot:50 e2el:5000 \
  --percentile-metrics ttft,tpot,itl,e2el \
  --metric-percentiles 50,90,95,99 \
  --save-result
```

**Goodputé…ç½®è¯´æ˜**ï¼š
- `ttft:200`ï¼šé¦–ä¸ªtokenæ—¶é—´ä¸è¶…è¿‡200ms
- `tpot:50`ï¼šæ¯tokenæ—¶é—´ä¸è¶…è¿‡50ms  
- `e2el:5000`ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿä¸è¶…è¿‡5000ms

### ç¤ºä¾‹6ï¼šå¤šæ¨¡æ€æµ‹è¯•

```bash
# æµ‹è¯•è§†è§‰å¯¹è¯æ¨¡å‹
python3 benchmark_serving.py \
  --backend openai-chat \
  --model llava-v1.6-vicuna-7b \
  --dataset-name hf \
  --dataset-path lmms-lab/VisionArena \
  --num-prompts 100 \
  --request-rate 2 \
  --save-result \
  --save-detailed
```

### ç¤ºä¾‹7ï¼šæ€§èƒ½åˆ†ææ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ€§èƒ½åˆ†æ
export VLLM_TORCH_PROFILER_DIR=/tmp/vllm_profiles

python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name random \
  --num-prompts 50 \
  --random-input-len 1024 \
  --random-output-len 512 \
  --request-rate 5 \
  --profile \
  --save-result \
  --save-detailed
```

---

## ğŸ“ˆ æ‰¹é‡æµ‹è¯•å’Œç»“æœåˆ†æ

### ç¤ºä¾‹8ï¼šä½¿ç”¨combos.yamlæ‰¹é‡æµ‹è¯•

**é…ç½®æ–‡ä»¶ (combos.yaml)**ï¼š
```yaml
model: "microsoft/DialoGPT-medium"
base_url: "http://localhost:8000"
tokenizer: "microsoft/DialoGPT-medium"

# æµ‹è¯•ä¸åŒçš„è¾“å…¥è¾“å‡ºé•¿åº¦ç»„åˆ
input_output:
  - [256, 128]    # çŸ­å¯¹è¯
  - [512, 256]    # ä¸­ç­‰å¯¹è¯
  - [1024, 512]   # é•¿å¯¹è¯
  - [2048, 1024]  # è¶…é•¿å¯¹è¯

# æµ‹è¯•ä¸åŒçš„å¹¶å‘çº§åˆ«
concurrency_prompts:
  - [1, 20]       # å•ç”¨æˆ·
  - [4, 80]       # å°å›¢é˜Ÿ
  - [8, 160]      # ä¸­ç­‰è´Ÿè½½
  - [16, 320]     # é«˜è´Ÿè½½
  - [32, 640]     # æé™è´Ÿè½½
```

**è¿è¡Œæ‰¹é‡æµ‹è¯•**ï¼š
```bash
# æ‰§è¡Œæ‰€æœ‰é…ç½®ç»„åˆçš„æµ‹è¯•
python3 run_sweep.py

# èšåˆæ‰€æœ‰ç»“æœ
python3 aggregate_result.py

# ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
python3 visualize.py --all
```

### ç¤ºä¾‹9ï¼šç»“æœåˆ†æå’Œå¯è§†åŒ–

```bash
# ç”Ÿæˆååé‡åˆ†æå›¾
python3 visualize.py --throughput

# ç”Ÿæˆå»¶è¿Ÿåˆ†æå›¾  
python3 visualize.py --latency

# ç”Ÿæˆäº¤äº’å¼ä»ªè¡¨æ¿
python3 visualize.py --interactive

# ç”Ÿæˆå®Œæ•´æ€§èƒ½æŠ¥å‘Š
python3 visualize.py --report

# è‡ªå®šä¹‰æ•°æ®æºåˆ†æ
python3 visualize.py --csv custom_results.csv --output custom_analysis/
```

---

## ğŸ¯ ç‰¹å®šåœºæ™¯æµ‹è¯•ç¤ºä¾‹

### ç¤ºä¾‹10ï¼šå‹åŠ›æµ‹è¯•

```bash
# æé™å¹¶å‘å‹åŠ›æµ‹è¯•
python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name random \
  --num-prompts 2000 \
  --random-input-len 1024 \
  --random-output-len 512 \
  --request-rate inf \
  --max-concurrency 64 \
  --save-result \
  --result-filename "stress_test.json"
```

### ç¤ºä¾‹11ï¼šç¨³å®šæ€§æµ‹è¯•

```bash
# é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•
python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 5000 \
  --request-rate 10 \
  --max-concurrency 8 \
  --save-result \
  --metadata test_type=stability duration=long
```

### ç¤ºä¾‹12ï¼šA/Bå¯¹æ¯”æµ‹è¯•

```bash
# æµ‹è¯•é…ç½®A
python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 500 \
  --request-rate 15 \
  --max-concurrency 8 \
  --save-result \
  --result-filename "config_a.json" \
  --metadata config=A gpu_memory=24GB

# æµ‹è¯•é…ç½®B  
python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name sharegpt \
  --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
  --num-prompts 500 \
  --request-rate 15 \
  --max-concurrency 16 \
  --save-result \
  --result-filename "config_b.json" \
  --metadata config=B gpu_memory=24GB
```

---

## ğŸ” ç»“æœè§£è¯»ç¤ºä¾‹

### ç¤ºä¾‹13ï¼šæ€§èƒ½æŒ‡æ ‡åˆ†æ

**å…¸å‹æµ‹è¯•è¾“å‡ºè§£è¯»**ï¼š
```
============= Serving Benchmark Result =============
Successful requests:                1000        # âœ… æ‰€æœ‰è¯·æ±‚éƒ½æˆåŠŸ
Benchmark duration (s):            50.25       # â±ï¸ æ€»æµ‹è¯•æ—¶é—´
Total input tokens:                 512000      # ğŸ“¥ è¾“å…¥tokenæ€»æ•°
Total generated tokens:             256000      # ğŸ“¤ è¾“å‡ºtokenæ€»æ•°
Request throughput (req/s):         19.90       # ğŸš€ è¯·æ±‚ååé‡
Request goodput (req/s):            18.50       # âœ¨ æœ‰æ•ˆååé‡ (æ»¡è¶³SLA)
Output token throughput (tok/s):    5094.04     # ğŸ“Š è¾“å‡ºtokenååé‡
Total Token throughput (tok/s):     15282.11    # ğŸ“ˆ æ€»tokenååé‡

------------------ Time to First Token ------------------
Mean TTFT (ms):                     125.30      # ğŸ“Š å¹³å‡é¦–tokenæ—¶é—´
Median TTFT (ms):                   120.50      # ğŸ“Š ä¸­ä½æ•°é¦–tokenæ—¶é—´
P99 TTFT (ms):                      180.20      # ğŸ“Š 99%åˆ†ä½æ•°

--------------- Time per Output Token (excl. 1st token) ---------------
Mean TPOT (ms):                     25.40       # ğŸ“Š å¹³å‡æ¯tokenæ—¶é—´
Median TPOT (ms):                   24.80       # ğŸ“Š ä¸­ä½æ•°æ¯tokenæ—¶é—´
P99 TPOT (ms):                      35.60       # ğŸ“Š 99%åˆ†ä½æ•°
```

**æ€§èƒ½è¯„ä¼°æ ‡å‡†**ï¼š
- **ä¼˜ç§€**ï¼šTTFT < 100ms, TPOT < 20ms
- **è‰¯å¥½**ï¼šTTFT < 200ms, TPOT < 40ms  
- **å¯æ¥å—**ï¼šTTFT < 500ms, TPOT < 80ms
- **éœ€ä¼˜åŒ–**ï¼šTTFT > 500ms, TPOT > 80ms

---

## ğŸ› ï¸ æ•…éšœæ’é™¤ç¤ºä¾‹

### ç¤ºä¾‹14ï¼šå¸¸è§é—®é¢˜è¯Šæ–­

```bash
# 1. è¿æ¥æµ‹è¯•
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your_model_name",
    "prompt": "Hello",
    "max_tokens": 10
  }'

# 2. ç®€å•åŠŸèƒ½æµ‹è¯•
python3 benchmark_serving.py \
  --backend vllm \
  --model your_model_name \
  --dataset-name random \
  --num-prompts 1 \
  --random-input-len 10 \
  --random-output-len 5

# 3. é€æ­¥å¢åŠ è´Ÿè½½æµ‹è¯•
for rate in 1 2 5 10 20; do
  echo "Testing rate: $rate req/s"
  python3 benchmark_serving.py \
    --backend vllm \
    --model your_model_name \
    --dataset-name random \
    --num-prompts 20 \
    --request-rate $rate \
    --random-input-len 256 \
    --random-output-len 128
done
```

---

## ğŸ“‹ æœ€ä½³å®è·µæ¸…å•

### âœ… æµ‹è¯•å‰å‡†å¤‡
- [ ] ç¡®è®¤vLLMæœåŠ¡æ­£å¸¸è¿è¡Œ
- [ ] éªŒè¯æ¨¡å‹åŠ è½½æˆåŠŸ
- [ ] æ£€æŸ¥ç³»ç»Ÿèµ„æºå……è¶³
- [ ] å‡†å¤‡åˆé€‚çš„æµ‹è¯•æ•°æ®é›†

### âœ… æµ‹è¯•æ‰§è¡Œ
- [ ] ä»å°è§„æ¨¡æµ‹è¯•å¼€å§‹
- [ ] é€æ­¥å¢åŠ è´Ÿè½½
- [ ] è®°å½•æµ‹è¯•ç¯å¢ƒä¿¡æ¯
- [ ] ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœ

### âœ… ç»“æœåˆ†æ
- [ ] å…³æ³¨å…³é”®æ€§èƒ½æŒ‡æ ‡
- [ ] åˆ†ææ€§èƒ½ç“¶é¢ˆ
- [ ] å¯¹æ¯”ä¸åŒé…ç½®ç»“æœ
- [ ] ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š

### âœ… ä¼˜åŒ–æ”¹è¿›
- [ ] åŸºäºç»“æœè°ƒæ•´é…ç½®
- [ ] é‡å¤æµ‹è¯•éªŒè¯æ”¹è¿›
- [ ] å»ºç«‹æ€§èƒ½åŸºçº¿
- [ ] æŒç»­ç›‘æ§æ€§èƒ½å˜åŒ–

é€šè¿‡è¿™äº›å®æˆ˜ç¤ºä¾‹ï¼Œåˆå­¦è€…å¯ä»¥å¿«é€ŸæŒæ¡vLLMåŸºå‡†æµ‹è¯•æ¡†æ¶çš„ä½¿ç”¨æ–¹æ³•ï¼Œå¹¶å»ºç«‹èµ·ç³»ç»Ÿçš„æ€§èƒ½æµ‹è¯•æµç¨‹ã€‚
