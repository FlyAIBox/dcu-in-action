# ğŸ¯ ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ

## ğŸ“‹ å¸¸ç”¨æµ‹è¯•åœºæ™¯ç¤ºä¾‹

### 1. å¿«é€ŸåŠŸèƒ½éªŒè¯æµ‹è¯•

**ç›®æ ‡**: éªŒè¯ç³»ç»ŸåŸºæœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸

```bash
# æœ€ç®€å•çš„æµ‹è¯•å‘½ä»¤
python benchmark_serving.py \
    --backend vllm \
    --model /path/to/your/model \
    --dataset-name random \
    --num-prompts 5 \
    --random-input-len 50 \
    --random-output-len 20

# é¢„æœŸç»“æœ: 5ä¸ªè¯·æ±‚å…¨éƒ¨æˆåŠŸï¼Œè·å¾—åŸºç¡€æ€§èƒ½æ•°æ®
```

**é€‚ç”¨åœºæ™¯**:
- æ–°éƒ¨ç½²æœåŠ¡çš„åŠŸèƒ½éªŒè¯
- ä»£ç ä¿®æ”¹åçš„å›å½’æµ‹è¯•
- å¿«é€Ÿæ£€æŸ¥æœåŠ¡çŠ¶æ€

### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•

**ç›®æ ‡**: å»ºç«‹ç³»ç»Ÿæ€§èƒ½åŸºå‡†çº¿

```bash
# æ ‡å‡†åŸºå‡†æµ‹è¯•
python benchmark_serving.py \
    --backend vllm \
    --model /path/to/your/model \
    --dataset-name random \
    --num-prompts 1000 \
    --random-input-len 512 \
    --random-output-len 128 \
    --save-result \
    --result-dir ./benchmarks \
    --metadata model_version=v1.0 hardware=8xDCU

# é¢„æœŸç»“æœ: è·å¾—1000ä¸ªè¯·æ±‚çš„è¯¦ç»†æ€§èƒ½æ•°æ®
```

**å…³é”®å‚æ•°è¯´æ˜**:
- `--num-prompts 1000`: è¶³å¤Ÿçš„æ ·æœ¬é‡ç¡®ä¿ç»Ÿè®¡æ„ä¹‰
- `--save-result`: ä¿å­˜è¯¦ç»†ç»“æœç”¨äºåç»­åˆ†æ
- `--metadata`: æ·»åŠ å…ƒæ•°æ®ä¾¿äºç»“æœç®¡ç†

### 3. å¹¶å‘å‹åŠ›æµ‹è¯•

**ç›®æ ‡**: æµ‹è¯•ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸‹çš„è¡¨ç°

```bash
# æ¸è¿›å¼å¹¶å‘æµ‹è¯•
for concurrency in 10 20 50 100 200; do
    echo "Testing with concurrency: $concurrency"
    python benchmark_serving.py \
        --backend vllm \
        --model /path/to/your/model \
        --dataset-name random \
        --num-prompts 500 \
        --request-rate 50 \
        --max-concurrency $concurrency \
        --random-input-len 256 \
        --random-output-len 64 \
        --save-result \
        --result-filename "concurrency_${concurrency}.json"
done

# åˆ†æä¸åŒå¹¶å‘åº¦ä¸‹çš„æ€§èƒ½å˜åŒ–
```

**åˆ†æè¦ç‚¹**:
- è§‚å¯Ÿååé‡éšå¹¶å‘åº¦çš„å˜åŒ–
- æ‰¾åˆ°æ€§èƒ½æ‹ç‚¹ (æœ€ä¼˜å¹¶å‘åº¦)
- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

### 4. çœŸå®åœºæ™¯æ¨¡æ‹Ÿæµ‹è¯•

**ç›®æ ‡**: ä½¿ç”¨çœŸå®æ•°æ®æ¨¡æ‹Ÿç”¨æˆ·ä½¿ç”¨åœºæ™¯

```bash
# ä¸‹è½½ShareGPTæ•°æ®é›†
wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json

# çœŸå®åœºæ™¯æµ‹è¯•
python benchmark_serving.py \
    --backend vllm \
    --model /path/to/your/model \
    --dataset-name sharegpt \
    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
    --num-prompts 200 \
    --request-rate 5 \
    --save-result \
    --save-detailed

# é¢„æœŸç»“æœ: æ›´æ¥è¿‘çœŸå®ä½¿ç”¨åœºæ™¯çš„æ€§èƒ½æ•°æ®
```

**ä¼˜åŠ¿**:
- è¾“å…¥é•¿åº¦åˆ†å¸ƒæ›´çœŸå®
- å¯¹è¯å†…å®¹æ›´è´´è¿‘å®é™…ä½¿ç”¨
- èƒ½å‘ç°åˆæˆæ•°æ®æµ‹è¯•ä¸­é—æ¼çš„é—®é¢˜

### 5. é•¿æ–‡æœ¬å¤„ç†æµ‹è¯•

**ç›®æ ‡**: æµ‹è¯•ç³»ç»Ÿå¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›

```bash
# é•¿æ–‡æœ¬æµ‹è¯•
python benchmark_serving.py \
    --backend vllm \
    --model /path/to/your/model \
    --dataset-name random \
    --num-prompts 100 \
    --random-input-len 4096 \
    --random-output-len 1024 \
    --request-rate 2 \
    --max-concurrency 10

# é¢„æœŸç»“æœ: äº†è§£é•¿æ–‡æœ¬å¤„ç†çš„æ€§èƒ½ç‰¹å¾
```

**æ³¨æ„äº‹é¡¹**:
- é•¿æ–‡æœ¬ä¼šæ˜¾è‘—å¢åŠ å†…å­˜ä½¿ç”¨
- éœ€è¦è°ƒæ•´æ¨¡å‹çš„max_model_lenå‚æ•°
- å¯èƒ½éœ€è¦é™ä½å¹¶å‘åº¦é¿å…OOM

### 6. æœåŠ¡è´¨é‡ (SLO) æµ‹è¯•

**ç›®æ ‡**: éªŒè¯ç³»ç»Ÿæ˜¯å¦æ»¡è¶³æœåŠ¡çº§åˆ«ç›®æ ‡

```bash
# SLOæµ‹è¯•
python benchmark_serving.py \
    --backend vllm \
    --model /path/to/your/model \
    --dataset-name random \
    --num-prompts 1000 \
    --request-rate 20 \
    --goodput ttft:100 tpot:50 e2el:2000 \
    --random-input-len 512 \
    --random-output-len 128

# é¢„æœŸç»“æœ: è·å¾—æ»¡è¶³SLOçš„è¯·æ±‚æ¯”ä¾‹ (goodput)
```

**SLOé…ç½®è¯´æ˜**:
- `ttft:100`: é¦–æ¬¡å“åº”æ—¶é—´ < 100ms
- `tpot:50`: æ¯tokenç”Ÿæˆæ—¶é—´ < 50ms  
- `e2el:2000`: ç«¯åˆ°ç«¯å»¶è¿Ÿ < 2000ms

## ğŸ† æœ€ä½³å®è·µæŒ‡å—

### 1. æµ‹è¯•ç¯å¢ƒå‡†å¤‡

#### ç¡¬ä»¶ç¯å¢ƒ
```bash
# æ£€æŸ¥DCUçŠ¶æ€
rocm-smi

# è®¾ç½®DCUç¯å¢ƒ
export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export NCCL_MAX_NCHANNELS=16
export NCCL_P2P_LEVEL=SYS

# æ£€æŸ¥å†…å­˜å’Œå­˜å‚¨
free -h
df -h
```

#### è½¯ä»¶ç¯å¢ƒ
```bash
# åˆ›å»ºç‹¬ç«‹çš„æµ‹è¯•ç¯å¢ƒ
conda create -n benchmark python=3.10
conda activate benchmark

# å®‰è£…ä¾èµ–
pip install vllm transformers datasets aiohttp tqdm numpy pandas

# éªŒè¯å®‰è£…
python -c "import vllm; print('vLLM installed successfully')"
```

### 2. æµ‹è¯•ç­–ç•¥è®¾è®¡

#### åˆ†å±‚æµ‹è¯•ç­–ç•¥
```
1. å†’çƒŸæµ‹è¯• (Smoke Test)
   â””â”€â”€ 5-10ä¸ªè¯·æ±‚ï¼ŒéªŒè¯åŸºæœ¬åŠŸèƒ½

2. åŠŸèƒ½æµ‹è¯• (Functional Test)  
   â””â”€â”€ 100-200ä¸ªè¯·æ±‚ï¼ŒéªŒè¯å„é¡¹åŠŸèƒ½

3. æ€§èƒ½æµ‹è¯• (Performance Test)
   â””â”€â”€ 1000+è¯·æ±‚ï¼Œå»ºç«‹æ€§èƒ½åŸºå‡†

4. å‹åŠ›æµ‹è¯• (Stress Test)
   â””â”€â”€ é«˜å¹¶å‘ã€é•¿æ—¶é—´æµ‹è¯•

5. ç¨³å®šæ€§æµ‹è¯• (Stability Test)
   â””â”€â”€ 24å°æ—¶æŒç»­è¿è¡Œæµ‹è¯•
```

#### å‚æ•°é€‰æ‹©åŸåˆ™
```bash
# è¾“å…¥é•¿åº¦é€‰æ‹©
--random-input-len 512    # çŸ­æ–‡æœ¬åœºæ™¯
--random-input-len 1024   # ä¸­ç­‰é•¿åº¦åœºæ™¯  
--random-input-len 2048   # é•¿æ–‡æœ¬åœºæ™¯

# è¾“å‡ºé•¿åº¦é€‰æ‹©
--random-output-len 64    # ç®€çŸ­å›å¤
--random-output-len 128   # æ ‡å‡†å›å¤
--random-output-len 256   # è¯¦ç»†å›å¤

# è¯·æ±‚æ•°é‡é€‰æ‹©
--num-prompts 10          # å¿«é€ŸéªŒè¯
--num-prompts 100         # åŠŸèƒ½æµ‹è¯•
--num-prompts 1000        # æ€§èƒ½åŸºå‡†
--num-prompts 10000       # å‹åŠ›æµ‹è¯•
```

### 3. ç»“æœåˆ†ææ–¹æ³•

#### æ€§èƒ½æŒ‡æ ‡è§£è¯»
```python
# å»¶è¿ŸæŒ‡æ ‡åˆ†æ
def analyze_latency_metrics(results):
    """åˆ†æå»¶è¿ŸæŒ‡æ ‡çš„æ–¹æ³•"""
    
    # 1. æŸ¥çœ‹å¹³å‡å€¼å’Œä¸­ä½æ•°çš„å·®å¼‚
    ttft_mean = results['mean_ttft_ms']
    ttft_median = results['median_ttft_ms']
    
    if ttft_mean > ttft_median * 1.5:
        print("âš ï¸  TTFTå­˜åœ¨é•¿å°¾å»¶è¿Ÿé—®é¢˜")
    
    # 2. åˆ†æP99æŒ‡æ ‡
    ttft_p99 = results['p99_ttft_ms']
    if ttft_p99 > ttft_mean * 3:
        print("âš ï¸  ç³»ç»Ÿå­˜åœ¨ä¸¥é‡çš„æ€§èƒ½ä¸ç¨³å®š")
    
    # 3. è¯„ä¼°ç”¨æˆ·ä½“éªŒ
    if ttft_median < 100:
        print("âœ… ç”¨æˆ·ä½“éªŒä¼˜ç§€")
    elif ttft_median < 200:
        print("âœ… ç”¨æˆ·ä½“éªŒè‰¯å¥½")
    else:
        print("âŒ ç”¨æˆ·ä½“éªŒéœ€è¦ä¼˜åŒ–")
```

#### è¶‹åŠ¿åˆ†æ
```bash
# æ‰¹é‡æµ‹è¯•è„šæœ¬ç¤ºä¾‹
#!/bin/bash
for batch_size in 1 2 4 8 16 32 64; do
    echo "Testing batch size: $batch_size"
    python benchmark_serving.py \
        --backend vllm \
        --model /path/to/model \
        --dataset-name random \
        --num-prompts $batch_size \
        --save-result \
        --result-filename "batch_${batch_size}.json"
done

# åˆ†ææ‰¹æ¬¡å¤§å°å¯¹æ€§èƒ½çš„å½±å“
python analyze_batch_results.py
```

### 4. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜1: è¿æ¥è¶…æ—¶
```bash
# ç—‡çŠ¶: requests.exceptions.ConnectTimeout
# åŸå› : vLLMæœåŠ¡æœªå¯åŠ¨æˆ–ç«¯å£ä¸æ­£ç¡®
# è§£å†³: 
curl http://localhost:8000/v1/models  # éªŒè¯æœåŠ¡çŠ¶æ€
netstat -tlnp | grep 8000            # æ£€æŸ¥ç«¯å£å ç”¨
```

#### é—®é¢˜2: å†…å­˜ä¸è¶³ (OOM)
```bash
# ç—‡çŠ¶: CUDA out of memory
# åŸå› : æ¨¡å‹å¤ªå¤§æˆ–å¹¶å‘åº¦è¿‡é«˜
# è§£å†³:
--gpu-memory-utilization 0.8         # é™ä½GPUå†…å­˜ä½¿ç”¨ç‡
--max-concurrency 10                 # é™åˆ¶å¹¶å‘æ•°
--max-model-len 2048                 # å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦
```

#### é—®é¢˜3: æ€§èƒ½å¼‚å¸¸æ³¢åŠ¨
```bash
# ç—‡çŠ¶: æ€§èƒ½æŒ‡æ ‡ä¸ç¨³å®šï¼Œæ–¹å·®å¾ˆå¤§
# åŸå› : ç³»ç»Ÿè´Ÿè½½ã€ç½‘ç»œæŠ–åŠ¨ã€èµ„æºç«äº‰
# è§£å†³:
# 1. ç³»ç»Ÿé¢„çƒ­
for i in {1..10}; do
    curl -s http://localhost:8000/v1/completions \
        -d '{"model":"test","prompt":"hello","max_tokens":10}' \
        -H "Content-Type: application/json" > /dev/null
done

# 2. ç›‘æ§ç³»ç»Ÿèµ„æº
htop &
rocm-smi -l 1 &

# 3. å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
for i in {1..5}; do
    python benchmark_serving.py ... --result-filename "run_${i}.json"
done
```

### 5. è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

#### å®Œæ•´çš„è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹
```bash
#!/bin/bash
# automated_benchmark.sh

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é…ç½®å‚æ•°
MODEL_PATH="/path/to/your/model"
RESULT_DIR="./results/$(date +%Y%m%d_%H%M%S)"
mkdir -p $RESULT_DIR

# 1. ç¯å¢ƒæ£€æŸ¥
echo "ğŸ” æ£€æŸ¥æµ‹è¯•ç¯å¢ƒ..."
python -c "import vllm, torch; print(f'vLLM: OK, PyTorch: {torch.__version__}')"
curl -s http://localhost:8000/v1/models > /dev/null || {
    echo "âŒ vLLMæœåŠ¡æœªå¯åŠ¨"
    exit 1
}

# 2. å†’çƒŸæµ‹è¯•
echo "ğŸš€ æ‰§è¡Œå†’çƒŸæµ‹è¯•..."
python benchmark_serving.py \
    --backend vllm \
    --model $MODEL_PATH \
    --dataset-name random \
    --num-prompts 5 \
    --result-filename "$RESULT_DIR/smoke_test.json"

# 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
echo "ğŸ“Š æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•..."
python benchmark_serving.py \
    --backend vllm \
    --model $MODEL_PATH \
    --dataset-name random \
    --num-prompts 1000 \
    --save-result \
    --result-filename "$RESULT_DIR/benchmark.json"

# 4. å¹¶å‘æµ‹è¯•
echo "âš¡ æ‰§è¡Œå¹¶å‘æµ‹è¯•..."
for concurrency in 10 20 50; do
    python benchmark_serving.py \
        --backend vllm \
        --model $MODEL_PATH \
        --dataset-name random \
        --num-prompts 200 \
        --max-concurrency $concurrency \
        --result-filename "$RESULT_DIR/concurrency_${concurrency}.json"
done

# 5. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
echo "ğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š..."
python generate_report.py $RESULT_DIR

echo "âœ… æµ‹è¯•å®Œæˆï¼ç»“æœä¿å­˜åœ¨: $RESULT_DIR"
```

é€šè¿‡è¿™äº›ç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼Œä½ å¯ä»¥ç³»ç»Ÿæ€§åœ°è¿›è¡Œå¤§æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•ï¼Œè·å¾—å¯é çš„æ€§èƒ½æ•°æ®ï¼Œå¹¶æŒç»­ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚
