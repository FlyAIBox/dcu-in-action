# æµ·å…‰DCUå¤§æ¨¡å‹æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·

## ğŸ“– é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæµ·å…‰DCUä¼˜åŒ–çš„vLLMå¤§æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°å¤§æ¨¡å‹åœ¨DCUç¡¬ä»¶ä¸Šçš„æ¨ç†æ€§èƒ½ã€‚è¯¥å·¥å…·æä¾›äº†å…¨é¢çš„æ€§èƒ½æŒ‡æ ‡åˆ†æï¼Œæ”¯æŒå¤šç§æ¨ç†åç«¯å’Œæ•°æ®é›†ï¼Œæ˜¯å¤§æ¨¡å‹æ¨ç†å‹æµ‹çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

### ğŸ¯ ä¸»è¦åŠŸèƒ½

- **å¤šåç«¯æ”¯æŒ**: vLLMã€TGIã€OpenAI APIã€TensorRT-LLMç­‰
- **å¤šæ•°æ®é›†æ”¯æŒ**: ShareGPTã€Randomã€Sonnetã€HuggingFaceç­‰
- **å…¨é¢æ€§èƒ½åˆ†æ**: TTFTã€TPOTã€ITLã€E2ELç­‰å…³é”®æŒ‡æ ‡
- **å¹¶å‘æ§åˆ¶**: æ”¯æŒè¯·æ±‚é€Ÿç‡æ§åˆ¶å’Œæœ€å¤§å¹¶å‘é™åˆ¶
- **DCUä¼˜åŒ–**: é’ˆå¯¹æµ·å…‰DCUç¡¬ä»¶çš„ä¸“é—¨ä¼˜åŒ–
- **ç»“æœå¯¼å‡º**: æ”¯æŒJSONã€CSVç­‰å¤šç§æ ¼å¼è¾“å‡º

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **benchmark_serving.py** - ä¸»æµ‹è¯•è„šæœ¬
   - å‘½ä»¤è¡Œå‚æ•°è§£æ
   - æµ‹è¯•æµç¨‹æ§åˆ¶
   - æ€§èƒ½æŒ‡æ ‡è®¡ç®—
   - ç»“æœè¾“å‡ºç®¡ç†

2. **backend_request_func.py** - åç«¯é€šä¿¡æ¨¡å—
   - ç»Ÿä¸€çš„è¯·æ±‚æ¥å£
   - å¼‚æ­¥HTTPé€šä¿¡
   - æµå¼å“åº”å¤„ç†
   - æ€§èƒ½æ•°æ®æ”¶é›†

3. **benchmark_dataset.py** - æ•°æ®é›†å¤„ç†æ¨¡å—
   - å¤šç§æ•°æ®é›†æ”¯æŒ
   - æ•°æ®é‡‡æ ·å’Œé¢„å¤„ç†
   - å¤šæ¨¡æ€æ•°æ®å¤„ç†

4. **benchmark_utils.py** - å·¥å…·å‡½æ•°æ¨¡å—
   - ç»“æœæ ¼å¼è½¬æ¢
   - JSONåºåˆ—åŒ–å¤„ç†

## ğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡

### å»¶è¿ŸæŒ‡æ ‡ (Latency Metrics)

- **TTFT (Time To First Token)**: é¦–æ¬¡tokenæ—¶é—´
  - å®šä¹‰: ä»å‘é€è¯·æ±‚åˆ°æ¥æ”¶ç¬¬ä¸€ä¸ªtokençš„æ—¶é—´
  - é‡è¦æ€§: åæ˜ ç”¨æˆ·æ„ŸçŸ¥çš„å“åº”é€Ÿåº¦
  - å•ä½: æ¯«ç§’ (ms)

- **TPOT (Time Per Output Token)**: æ¯tokenæ—¶é—´
  - å®šä¹‰: ç”Ÿæˆæ¯ä¸ªè¾“å‡ºtokençš„å¹³å‡æ—¶é—´
  - é‡è¦æ€§: åæ˜ ç”Ÿæˆé€Ÿåº¦çš„ç¨³å®šæ€§
  - å•ä½: æ¯«ç§’ (ms)

- **ITL (Inter-Token Latency)**: è¿­ä»£å»¶è¿Ÿ
  - å®šä¹‰: ç›¸é‚»ä¸¤ä¸ªtokenä¹‹é—´çš„æ—¶é—´é—´éš”
  - é‡è¦æ€§: åæ˜ æµå¼è¾“å‡ºçš„æµç•…åº¦
  - å•ä½: æ¯«ç§’ (ms)

- **E2EL (End-to-End Latency)**: ç«¯åˆ°ç«¯å»¶è¿Ÿ
  - å®šä¹‰: ä»è¯·æ±‚å‘é€åˆ°å®Œæ•´å“åº”æ¥æ”¶çš„æ€»æ—¶é—´
  - é‡è¦æ€§: åæ˜ æ•´ä½“å¤„ç†æ—¶é—´
  - å•ä½: æ¯«ç§’ (ms)

### ååé‡æŒ‡æ ‡ (Throughput Metrics)

- **Request Throughput**: è¯·æ±‚ååé‡
  - å®šä¹‰: æ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°é‡
  - å•ä½: è¯·æ±‚/ç§’ (req/s)

- **Output Token Throughput**: è¾“å‡ºååé‡
  - å®šä¹‰: æ¯ç§’ç”Ÿæˆçš„tokenæ•°é‡
  - å•ä½: token/ç§’ (tok/s)

- **Total Token Throughput**: æ€»tokenååé‡
  - å®šä¹‰: æ¯ç§’å¤„ç†çš„æ€»tokenæ•°é‡(è¾“å…¥+è¾“å‡º)
  - å•ä½: token/ç§’ (tok/s)

### æœåŠ¡è´¨é‡æŒ‡æ ‡ (Quality of Service)

- **Goodput**: è‰¯å¥½ååé‡
  - å®šä¹‰: æ»¡è¶³SLO(æœåŠ¡çº§åˆ«ç›®æ ‡)çš„è¯·æ±‚æ¯”ä¾‹
  - é‡è¦æ€§: åæ˜ æœåŠ¡è´¨é‡çš„ç¨³å®šæ€§

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install vllm transformers datasets aiohttp tqdm numpy pandas

# è®¾ç½®DCUç¯å¢ƒå˜é‡
export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
```

### 2. å¯åŠ¨vLLMæœåŠ¡

```bash
# ä½¿ç”¨æä¾›çš„æœåŠ¡å¯åŠ¨è„šæœ¬
bash server.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨
vllm serve /path/to/your/model \
    --trust-remote-code \
    --dtype float16 \
    --max-model-len 32768 \
    -tp 8 \
    --gpu-memory-utilization 0.9 \
    --port 8000
```

### 3. è¿è¡ŒåŸºå‡†æµ‹è¯•

#### åŸºç¡€æµ‹è¯•
```bash
python benchmark_serving.py \
    --backend vllm \
    --model /path/to/your/model \
    --dataset-name random \
    --num-prompts 100 \
    --random-input-len 512 \
    --random-output-len 128
```

#### é«˜çº§æµ‹è¯•é…ç½®
```bash
python benchmark_serving.py \
    --backend vllm \
    --model /path/to/your/model \
    --dataset-name sharegpt \
    --dataset-path /path/to/sharegpt.json \
    --num-prompts 1000 \
    --request-rate 10 \
    --max-concurrency 50 \
    --save-result \
    --result-dir ./results
```

### 4. æ‰¹é‡æµ‹è¯•

```bash
# ä½¿ç”¨æä¾›çš„æ‰¹é‡æµ‹è¯•è„šæœ¬
bash test.sh
```

## ğŸ“‹ å‘½ä»¤è¡Œå‚æ•°è¯¦è§£

### åŸºç¡€å‚æ•°

- `--backend`: æ¨ç†åç«¯é€‰æ‹© (vllm, openai, tgiç­‰)
- `--model`: æ¨¡å‹åç§°æˆ–è·¯å¾„
- `--host`: æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 127.0.0.1)
- `--port`: æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)

### æ•°æ®é›†å‚æ•°

- `--dataset-name`: æ•°æ®é›†ç±»å‹ (sharegpt, random, sonnet, hf)
- `--dataset-path`: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
- `--num-prompts`: æµ‹è¯•è¯·æ±‚æ•°é‡ (é»˜è®¤: 1000)

### æ€§èƒ½æ§åˆ¶å‚æ•°

- `--request-rate`: è¯·æ±‚å‘é€é€Ÿç‡ (req/s, é»˜è®¤: inf)
- `--max-concurrency`: æœ€å¤§å¹¶å‘æ•°
- `--burstiness`: è¯·æ±‚çªå‘æ€§å› å­ (é»˜è®¤: 1.0)

### è¾“å‡ºæ§åˆ¶å‚æ•°

- `--save-result`: ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
- `--result-dir`: ç»“æœä¿å­˜ç›®å½•
- `--percentile-metrics`: ç™¾åˆ†ä½æ•°æŒ‡æ ‡é€‰æ‹©
- `--metric-percentiles`: ç™¾åˆ†ä½æ•°å€¼é€‰æ‹©

## ğŸ“ˆ ç»“æœè§£è¯»

### æ§åˆ¶å°è¾“å‡ºç¤ºä¾‹

```
=============== Serving Benchmark Result ===============
Successful requests:                     1000
Benchmark duration (s):                 45.23
Total input tokens:                      512000
Total generated tokens:                  128000
Request throughput (req/s):              22.11
Output token throughput (tok/s):         2830.45
Total Token throughput (tok/s):          14152.25

----------------------- Time to First Token -----------------------
Mean TTFT (ms):                          125.34
Median TTFT (ms):                        118.67
P99 TTFT (ms):                          245.89

----------------------- Time per Output Token ----------------------
Mean TPOT (ms):                          35.67
Median TPOT (ms):                        32.45
P99 TPOT (ms):                          78.23
```

### æ€§èƒ½è¯„ä¼°æ ‡å‡†

#### ä¼˜ç§€æ€§èƒ½æŒ‡æ ‡å‚è€ƒå€¼ (ä»…ä¾›å‚è€ƒ)

- **TTFT**: < 100ms (ä¼˜ç§€), < 200ms (è‰¯å¥½), > 500ms (éœ€ä¼˜åŒ–)
- **TPOT**: < 50ms (ä¼˜ç§€), < 100ms (è‰¯å¥½), > 200ms (éœ€ä¼˜åŒ–)
- **ååé‡**: æ ¹æ®ç¡¬ä»¶é…ç½®å’Œæ¨¡å‹å¤§å°è€Œå®š

## ğŸ”§ é«˜çº§é…ç½®

### DCUä¼˜åŒ–é…ç½®

```bash
# NCCLé€šä¿¡ä¼˜åŒ–
export NCCL_MAX_NCHANNELS=16
export NCCL_MIN_NCHANNELS=16
export NCCL_P2P_LEVEL=SYS

# NUMAç»‘å®š
export VLLM_NUMA_BIND=0
```

### æœåŠ¡è´¨é‡ç›®æ ‡ (SLO) é…ç½®

```bash
python benchmark_serving.py \
    --goodput ttft:100 tpot:50 e2el:1000 \
    # å…¶ä»–å‚æ•°...
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. æµ‹è¯•å‰å‡†å¤‡
- ç¡®ä¿DCUé©±åŠ¨å’Œè¿è¡Œæ—¶ç¯å¢ƒæ­£ç¡®å®‰è£…
- é¢„çƒ­æ¨¡å‹æœåŠ¡ï¼Œé¿å…å†·å¯åŠ¨å½±å“
- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

### 2. æµ‹è¯•å‚æ•°é€‰æ‹©
- ä»å°æ‰¹é‡å¼€å§‹ï¼Œé€æ­¥å¢åŠ è´Ÿè½½
- é€‰æ‹©ä»£è¡¨æ€§çš„è¾“å…¥è¾“å‡ºé•¿åº¦ç»„åˆ
- è€ƒè™‘çœŸå®ä¸šåŠ¡åœºæ™¯çš„è¯·æ±‚æ¨¡å¼

### 3. ç»“æœåˆ†æ
- å…³æ³¨P99ç­‰é«˜ç™¾åˆ†ä½æ•°æŒ‡æ ‡
- åˆ†æä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½å˜åŒ–è¶‹åŠ¿
- ç»“åˆç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µè¿›è¡Œç»¼åˆè¯„ä¼°

## ğŸ› å¸¸è§é—®é¢˜

### Q: æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°è¿æ¥è¶…æ—¶
A: æ£€æŸ¥vLLMæœåŠ¡æ˜¯å¦æ­£å¸¸å¯åŠ¨ï¼Œè°ƒæ•´AIOHTTP_TIMEOUTè®¾ç½®

### Q: å†…å­˜ä¸è¶³é”™è¯¯
A: é™ä½--gpu-memory-utilizationå‚æ•°æˆ–å‡å°‘å¹¶å‘æ•°

### Q: æ€§èƒ½æŒ‡æ ‡å¼‚å¸¸
A: æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½ï¼Œç¡®è®¤DCUè®¾å¤‡é…ç½®

## ğŸ“š å‚è€ƒèµ„æ–™

- [vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [æµ·å…‰DCUå¼€å‘æŒ‡å—](https://developer.hygon.cn/)
- [å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æœ€ä½³å®è·µ](https://github.com/vllm-project/vllm)

## ğŸ“ åˆå­¦è€…å®Œæ•´æ•™ç¨‹

### ç¬¬ä¸€æ­¥: ç¯å¢ƒæ­å»º

1. **å®‰è£…åŸºç¡€ä¾èµ–**
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (æ¨è)
conda create -n vllm-benchmark python=3.10
conda activate vllm-benchmark

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6
pip install vllm transformers datasets aiohttp tqdm numpy pandas
```

2. **é…ç½®DCUç¯å¢ƒ**
```bash
# æ£€æŸ¥DCUè®¾å¤‡
rocm-smi

# è®¾ç½®ç¯å¢ƒå˜é‡
export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export NCCL_MAX_NCHANNELS=16
export NCCL_MIN_NCHANNELS=16
export NCCL_P2P_LEVEL=SYS
```

### ç¬¬äºŒæ­¥: å‡†å¤‡æ¨¡å‹

1. **ä¸‹è½½æ¨¡å‹**
```bash
# ä½¿ç”¨HuggingFace Hubä¸‹è½½
huggingface-cli download microsoft/DialoGPT-medium --local-dir ./models/DialoGPT-medium

# æˆ–ä½¿ç”¨git lfs
git lfs clone https://huggingface.co/microsoft/DialoGPT-medium ./models/DialoGPT-medium
```

2. **éªŒè¯æ¨¡å‹**
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("./models/DialoGPT-medium")
model = AutoModelForCausalLM.from_pretrained("./models/DialoGPT-medium")
print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
```

### ç¬¬ä¸‰æ­¥: å¯åŠ¨vLLMæœåŠ¡

1. **ä¿®æ”¹æœåŠ¡å¯åŠ¨è„šæœ¬**
```bash
# ç¼–è¾‘ server.sh
vim server.sh

# ä¿®æ”¹æ¨¡å‹è·¯å¾„
vllm serve ./models/DialoGPT-medium \
    --trust-remote-code \
    --dtype float16 \
    --max-model-len 2048 \
    -tp 4 \
    --gpu-memory-utilization 0.9 \
    --port 8000
```

2. **å¯åŠ¨æœåŠ¡**
```bash
bash server.sh
```

3. **éªŒè¯æœåŠ¡**
```bash
# æµ‹è¯•APIè¿é€šæ€§
curl http://localhost:8000/v1/models

# å‘é€æµ‹è¯•è¯·æ±‚
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "./models/DialoGPT-medium",
    "prompt": "Hello, how are you?",
    "max_tokens": 50
  }'
```

### ç¬¬å››æ­¥: è¿è¡ŒåŸºå‡†æµ‹è¯•

1. **ç®€å•æµ‹è¯•**
```bash
# æœ€åŸºç¡€çš„æµ‹è¯•å‘½ä»¤
python benchmark_serving.py \
    --backend vllm \
    --model ./models/DialoGPT-medium \
    --dataset-name random \
    --num-prompts 10 \
    --random-input-len 100 \
    --random-output-len 50
```

2. **æŸ¥çœ‹ç»“æœ**
```
=============== Serving Benchmark Result ===============
Successful requests:                     10
Benchmark duration (s):                 5.23
Total input tokens:                      1000
Total generated tokens:                  500
Request throughput (req/s):              1.91
Output token throughput (tok/s):         95.60
Total Token throughput (tok/s):          286.81

----------------------- Time to First Token -----------------------
Mean TTFT (ms):                          125.34
Median TTFT (ms):                        118.67
P99 TTFT (ms):                          245.89
```

### ç¬¬äº”æ­¥: ç†è§£æ€§èƒ½æŒ‡æ ‡

1. **å»¶è¿ŸæŒ‡æ ‡è§£è¯»**
- **TTFT < 200ms**: ç”¨æˆ·æ„Ÿè§‰å“åº”å¾ˆå¿«
- **TTFT 200-500ms**: å¯æ¥å—çš„å“åº”é€Ÿåº¦
- **TTFT > 500ms**: ç”¨æˆ·ä¼šæ„Ÿè§‰æ˜æ˜¾å»¶è¿Ÿ

2. **ååé‡æŒ‡æ ‡è§£è¯»**
- **Request Throughput**: ç³»ç»Ÿæ¯ç§’èƒ½å¤„ç†å¤šå°‘ä¸ªè¯·æ±‚
- **Token Throughput**: ç³»ç»Ÿæ¯ç§’èƒ½ç”Ÿæˆå¤šå°‘ä¸ªtoken
- **æ•°å€¼è¶Šé«˜è¡¨ç¤ºæ€§èƒ½è¶Šå¥½**

### ç¬¬å…­æ­¥: é«˜çº§æµ‹è¯•é…ç½®

1. **å‹åŠ›æµ‹è¯•**
```bash
python benchmark_serving.py \
    --backend vllm \
    --model ./models/DialoGPT-medium \
    --dataset-name random \
    --num-prompts 1000 \
    --request-rate 10 \
    --max-concurrency 50 \
    --random-input-len 512 \
    --random-output-len 128
```

2. **çœŸå®æ•°æ®æµ‹è¯•**
```bash
# ä¸‹è½½ShareGPTæ•°æ®é›†
wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json

# ä½¿ç”¨çœŸå®å¯¹è¯æ•°æ®æµ‹è¯•
python benchmark_serving.py \
    --backend vllm \
    --model ./models/DialoGPT-medium \
    --dataset-name sharegpt \
    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
    --num-prompts 100
```

### ç¬¬ä¸ƒæ­¥: ç»“æœåˆ†æå’Œä¼˜åŒ–

1. **æ€§èƒ½ç“¶é¢ˆè¯†åˆ«**
```bash
# ç›‘æ§ç³»ç»Ÿèµ„æº
htop
rocm-smi

# åˆ†ææ—¥å¿—
tail -f /var/log/vllm.log
```

2. **ä¼˜åŒ–å»ºè®®**
- **å†…å­˜ä¸è¶³**: é™ä½ `--gpu-memory-utilization`
- **å»¶è¿Ÿè¿‡é«˜**: å‡å°‘ `--max-model-len` æˆ–å¢åŠ GPUæ•°é‡
- **ååé‡ä½**: è°ƒæ•´ `--max-concurrency` å’Œæ‰¹å¤„ç†å¤§å°

### å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

1. **CUDA/HIPé”™è¯¯**
```bash
# æ£€æŸ¥é©±åŠ¨
rocm-smi
export HIP_VISIBLE_DEVICES=0,1,2,3
```

2. **å†…å­˜ä¸è¶³**
```bash
# é™ä½å†…å­˜ä½¿ç”¨
--gpu-memory-utilization 0.8
--max-model-len 1024
```

3. **è¿æ¥è¶…æ—¶**
```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export AIOHTTP_TIMEOUT=3600
```

è¿™ä¸ªæ•™ç¨‹å°†å¸®åŠ©åˆå­¦è€…ä»é›¶å¼€å§‹æŒæ¡å¤§æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•çš„å®Œæ•´æµç¨‹ã€‚
