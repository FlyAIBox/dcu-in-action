#!/usr/bin/env python3
"""
åŸºäºvLLMçš„é«˜æ€§èƒ½æ¨ç†æœåŠ¡å™¨
æ”¯æŒå¤šæ¨¡å‹å¹¶å‘æ¨ç†å’ŒREST APIæ¥å£
"""

import asyncio
import time
import json
from typing import List, Dict, Optional
from dataclasses import dataclass

try:
    from vllm import LLM, SamplingParams
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…vLLM: pip install vllm")
    exit(1)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    name: str
    model_path: str
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.9
    max_model_len: int = 2048
    dtype: str = "float16"

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.95
    stream: bool = False

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    response: str
    model: str
    usage: Dict[str, int]
    timing: Dict[str, float]

class BatchRequest(BaseModel):
    """æ‰¹é‡è¯·æ±‚æ¨¡å‹"""
    prompts: List[str]
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.95

class vLLMServer:
    def __init__(self, models_config: List[ModelConfig]):
        """
        åˆå§‹åŒ–vLLMæœåŠ¡å™¨
        
        Args:
            models_config: æ¨¡å‹é…ç½®åˆ—è¡¨
        """
        self.models_config = models_config
        self.engines = {}
        self.app = FastAPI(title="æµ·å…‰DCU vLLMæ¨ç†æœåŠ¡", version="1.0.0")
        
        self._setup_routes()
    
    async def _load_models(self):
        """å¼‚æ­¥åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        for config in self.models_config:
            try:
                print(f"   åŠ è½½æ¨¡å‹: {config.name}")
                
                # é…ç½®å¼•æ“å‚æ•°
                engine_args = AsyncEngineArgs(
                    model=config.model_path,
                    tensor_parallel_size=config.tensor_parallel_size,
                    gpu_memory_utilization=config.gpu_memory_utilization,
                    max_model_len=config.max_model_len,
                    dtype=config.dtype,
                    trust_remote_code=True
                )
                
                # åˆ›å»ºå¼‚æ­¥å¼•æ“
                engine = AsyncLLMEngine.from_engine_args(engine_args)
                self.engines[config.name] = engine
                
                print(f"   âœ… {config.name} åŠ è½½æˆåŠŸ")
                
            except Exception as e:
                print(f"   âŒ {config.name} åŠ è½½å¤±è´¥: {e}")
    
    def _setup_routes(self):
        """è®¾ç½®APIè·¯ç”±"""
        
        @self.app.on_event("startup")
        async def startup_event():
            await self._load_models()
        
        @self.app.get("/")
        async def root():
            return {
                "message": "æµ·å…‰DCU vLLMæ¨ç†æœåŠ¡",
                "models": list(self.engines.keys()),
                "status": "running"
            }
        
        @self.app.get("/models")
        async def list_models():
            """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
            models_info = []
            for name, engine in self.engines.items():
                config = next(c for c in self.models_config if c.name == name)
                models_info.append({
                    "name": name,
                    "model_path": config.model_path,
                    "tensor_parallel_size": config.tensor_parallel_size,
                    "max_model_len": config.max_model_len
                })
            return {"models": models_info}
        
        @self.app.post("/chat/{model_name}")
        async def chat(model_name: str, request: ChatRequest) -> ChatResponse:
            """å•æ¬¡å¯¹è¯æ¥å£"""
            if model_name not in self.engines:
                raise HTTPException(status_code=404, detail=f"æ¨¡å‹ {model_name} æœªæ‰¾åˆ°")
            
            engine = self.engines[model_name]
            
            # è®¾ç½®é‡‡æ ·å‚æ•°
            sampling_params = SamplingParams(
                temperature=request.temperature,
                top_p=request.top_p,
                max_tokens=request.max_tokens
            )
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            try:
                # å¼‚æ­¥ç”Ÿæˆ
                results = await engine.generate(request.message, sampling_params, request_id=f"chat_{time.time()}")
                
                # æå–ç»“æœ
                output = results.outputs[0]
                response_text = output.text
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                end_time = time.time()
                inference_time = end_time - start_time
                
                return ChatResponse(
                    response=response_text,
                    model=model_name,
                    usage={
                        "prompt_tokens": len(results.prompt_token_ids),
                        "completion_tokens": len(output.token_ids),
                        "total_tokens": len(results.prompt_token_ids) + len(output.token_ids)
                    },
                    timing={
                        "inference_time": inference_time,
                        "tokens_per_second": len(output.token_ids) / inference_time
                    }
                )
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"æ¨ç†å¤±è´¥: {str(e)}")
        
        @self.app.post("/batch/{model_name}")
        async def batch_inference(model_name: str, request: BatchRequest):
            """æ‰¹é‡æ¨ç†æ¥å£"""
            if model_name not in self.engines:
                raise HTTPException(status_code=404, detail=f"æ¨¡å‹ {model_name} æœªæ‰¾åˆ°")
            
            engine = self.engines[model_name]
            
            # è®¾ç½®é‡‡æ ·å‚æ•°
            sampling_params = SamplingParams(
                temperature=request.temperature,
                top_p=request.top_p,
                max_tokens=request.max_tokens
            )
            
            start_time = time.time()
            
            try:
                # æ‰¹é‡ç”Ÿæˆ
                results = []
                for i, prompt in enumerate(request.prompts):
                    result = await engine.generate(prompt, sampling_params, request_id=f"batch_{i}_{time.time()}")
                    results.append(result)
                
                end_time = time.time()
                
                # æ•´ç†ç»“æœ
                responses = []
                total_tokens = 0
                
                for result in results:
                    output = result.outputs[0]
                    responses.append({
                        "prompt": result.prompt,
                        "response": output.text,
                        "tokens": len(output.token_ids)
                    })
                    total_tokens += len(output.token_ids)
                
                return {
                    "results": responses,
                    "model": model_name,
                    "timing": {
                        "total_time": end_time - start_time,
                        "average_tokens_per_second": total_tokens / (end_time - start_time)
                    }
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"æ‰¹é‡æ¨ç†å¤±è´¥: {str(e)}")
        
        @self.app.get("/health")
        async def health_check():
            """å¥åº·æ£€æŸ¥æ¥å£"""
            return {
                "status": "healthy",
                "models_loaded": len(self.engines),
                "timestamp": time.time()
            }
    
    def run(self, host: str = "0.0.0.0", port: int = 8000):
        """å¯åŠ¨æœåŠ¡å™¨"""
        print(f"ğŸš€ å¯åŠ¨vLLMæ¨ç†æœåŠ¡...")
        print(f"   åœ°å€: http://{host}:{port}")
        print(f"   æ–‡æ¡£: http://{host}:{port}/docs")
        
        uvicorn.run(self.app, host=host, port=port)

class HighPerformanceInference:
    """é«˜æ€§èƒ½æ¨ç†å·¥å…·ç±»"""
    
    def __init__(self, model_name: str, **kwargs):
        """
        åˆå§‹åŒ–é«˜æ€§èƒ½æ¨ç†å¼•æ“
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            **kwargs: vLLMå¼•æ“å‚æ•°
        """
        default_kwargs = {
            "tensor_parallel_size": 1,
            "gpu_memory_utilization": 0.9,
            "dtype": "float16",
            "trust_remote_code": True
        }
        default_kwargs.update(kwargs)
        
        print(f"ğŸ”„ åˆå§‹åŒ–vLLMå¼•æ“: {model_name}")
        self.llm = LLM(model=model_name, **default_kwargs)
        print("âœ… å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def generate(self, prompts, **sampling_kwargs):
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompts: å•ä¸ªæç¤ºæˆ–æç¤ºåˆ—è¡¨
            **sampling_kwargs: é‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœ
        """
        default_sampling = {
            "temperature": 0.7,
            "top_p": 0.95,
            "max_tokens": 512
        }
        default_sampling.update(sampling_kwargs)
        
        sampling_params = SamplingParams(**default_sampling)
        
        # ç¡®ä¿promptsæ˜¯åˆ—è¡¨
        if isinstance(prompts, str):
            prompts = [prompts]
        
        start_time = time.time()
        outputs = self.llm.generate(prompts, sampling_params)
        end_time = time.time()
        
        # å¤„ç†ç»“æœ
        results = []
        total_tokens = 0
        
        for output in outputs:
            generated_text = output.outputs[0].text
            output_tokens = len(output.outputs[0].token_ids)
            total_tokens += output_tokens
            
            results.append({
                "prompt": output.prompt,
                "generated_text": generated_text,
                "tokens": output_tokens
            })
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time = end_time - start_time
        tokens_per_second = total_tokens / total_time
        
        print(f"âš¡ ç”Ÿæˆå®Œæˆ:")
        print(f"   æ€»æ—¶é—´: {total_time:.2f}s")
        print(f"   æ€»tokens: {total_tokens}")
        print(f"   ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} tokens/s")
        
        return results if len(results) > 1 else results[0]
    
    def benchmark(self, test_prompts=None, iterations=3):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if test_prompts is None:
            test_prompts = [
                "ä»‹ç»ä¸€ä¸‹æµ·å…‰DCUçš„æŠ€æœ¯ç‰¹ç‚¹",
                "è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†",
                "Pythonç¼–ç¨‹è¯­è¨€çš„ä¼˜åŠ¿æœ‰å“ªäº›ï¼Ÿ",
                "æè¿°äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
                "ä»€ä¹ˆæ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Ÿ"
            ]
        
        print(f"ğŸƒ å¼€å§‹vLLMæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print(f"   æµ‹è¯•æç¤ºæ•°é‡: {len(test_prompts)}")
        print(f"   æµ‹è¯•è½®æ•°: {iterations}")
        
        all_results = []
        
        for i in range(iterations):
            print(f"\nè½®æ¬¡ {i+1}/{iterations}:")
            
            start_time = time.time()
            results = self.generate(test_prompts, max_tokens=256)
            end_time = time.time()
            
            total_time = end_time - start_time
            total_tokens = sum(r["tokens"] for r in results)
            tokens_per_second = total_tokens / total_time
            
            all_results.append({
                "time": total_time,
                "tokens": total_tokens,
                "tps": tokens_per_second
            })
            
            print(f"   æ—¶é—´: {total_time:.2f}s")
            print(f"   tokens: {total_tokens}")
            print(f"   é€Ÿåº¦: {tokens_per_second:.1f} tokens/s")
        
        # è®¡ç®—å¹³å‡å€¼
        avg_time = sum(r["time"] for r in all_results) / len(all_results)
        avg_tokens = sum(r["tokens"] for r in all_results) / len(all_results)
        avg_tps = sum(r["tps"] for r in all_results) / len(all_results)
        
        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ (å¹³å‡å€¼):")
        print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}s")
        print(f"   å¹³å‡tokens: {avg_tokens:.0f}")
        print(f"   å¹³å‡é€Ÿåº¦: {avg_tps:.1f} tokens/s")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="vLLMé«˜æ€§èƒ½æ¨ç†")
    parser.add_argument("--mode", choices=["server", "client", "benchmark"], default="client", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--model", default="Qwen/Qwen-7B-Chat", help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--host", default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--prompt", type=str, help="æµ‹è¯•æç¤º")
    
    args = parser.parse_args()
    
    if args.mode == "server":
        # å¯åŠ¨æœåŠ¡å™¨æ¨¡å¼
        config = ModelConfig(
            name="default",
            model_path=args.model,
            tensor_parallel_size=1
        )
        server = vLLMServer([config])
        server.run(host=args.host, port=args.port)
        
    elif args.mode == "client":
        # å®¢æˆ·ç«¯æ¨ç†æ¨¡å¼
        engine = HighPerformanceInference(args.model)
        
        if args.prompt:
            result = engine.generate(args.prompt)
            print(f"è¾“å…¥: {result['prompt']}")
            print(f"è¾“å‡º: {result['generated_text']}")
        else:
            # äº¤äº’æ¨¡å¼
            print("ğŸ¤– vLLMäº¤äº’å¼æ¨ç† (è¾“å…¥'exit'é€€å‡º)")
            while True:
                prompt = input("\nğŸ‘¤ è¾“å…¥: ").strip()
                if prompt.lower() in ['exit', 'quit', 'é€€å‡º']:
                    break
                
                result = engine.generate(prompt)
                print(f"ğŸ¤– è¾“å‡º: {result['generated_text']}")
    
    elif args.mode == "benchmark":
        # åŸºå‡†æµ‹è¯•æ¨¡å¼
        engine = HighPerformanceInference(args.model)
        engine.benchmark()

if __name__ == "__main__":
    main() 