#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLaMA Factoryæ¨¡å‹è®­ç»ƒè„šæœ¬
ä½œè€…: DCUå®æˆ˜é¡¹ç›®ç»„
"""

import os
import sys
import yaml
import json
import torch
import argparse
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ LLaMA Factoryè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../LLaMA-Factory/src'))

try:
    from llamafactory.train.tuner import run_exp
    from llamafactory.extras.logging import get_logger
except ImportError:
    print("é”™è¯¯ï¼šæ— æ³•å¯¼å…¥LLaMA Factoryæ¨¡å—")
    print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…LLaMA Factoryå¹¶åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œ")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('training.log')
    ]
)
logger = logging.getLogger(__name__)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, config_path: Optional[str] = None, **kwargs):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            **kwargs: ç›´æ¥ä¼ å…¥çš„é…ç½®å‚æ•°
        """
        self.config = self._load_config(config_path, **kwargs)
        self.start_time = None
        self.training_process = None
        
    def _load_config(self, config_path: Optional[str], **kwargs) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        config = {}
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        if config_path and os.path.exists(config_path):
            logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            with open(config_path, 'r', encoding='utf-8') as f:
                if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                    config = yaml.safe_load(f)
                else:
                    config = json.load(f)
        
        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
        config.update({k: v for k, v in kwargs.items() if v is not None})
        
        # è®¾ç½®é»˜è®¤å€¼
        default_config = {
            "model_name_or_path": "THUDM/chatglm3-6b",
            "dataset": "alpaca_zh",
            "template": "chatglm3",
            "finetuning_type": "lora",
            "output_dir": "./saves/default_train",
            "stage": "sft",
            "do_train": True,
            "per_device_train_batch_size": 2,
            "gradient_accumulation_steps": 4,
            "lr_scheduler_type": "cosine",
            "logging_steps": 10,
            "save_steps": 1000,
            "learning_rate": 5e-5,
            "num_train_epochs": 3,
            "max_grad_norm": 1.0,
            "val_size": 0.1,
            "evaluation_strategy": "steps",
            "eval_steps": 500,
            "load_best_model_at_end": True,
            "plot_loss": True
        }
        
        for key, value in default_config.items():
            if key not in config:
                config[key] = value
        
        return config
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®å‚æ•°"""
        logger.info("éªŒè¯è®­ç»ƒé…ç½®...")
        
        required_fields = ["model_name_or_path", "dataset", "output_dir"]
        for field in required_fields:
            if field not in self.config:
                logger.error(f"ç¼ºå°‘å¿…è¦é…ç½®: {field}")
                return False
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•
        output_dir = Path(self.config["output_dir"])
        if output_dir.exists() and any(output_dir.iterdir()):
            logger.warning(f"è¾“å‡ºç›®å½•éç©º: {output_dir}")
            response = input("ç»§ç»­è®­ç»ƒå°†è¦†ç›–ç°æœ‰æ–‡ä»¶ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]: ")
            if response.lower() != 'y':
                logger.info("è®­ç»ƒå·²å–æ¶ˆ")
                return False
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        try:
            if torch.cuda.is_available():
                logger.info(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
                for i in range(torch.cuda.device_count()):
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    logger.info(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                logger.warning("æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        except ImportError:
            logger.warning("PyTorchæœªå®‰è£…")
            return False
        
        logger.info("é…ç½®éªŒè¯é€šè¿‡")
        return True
    
    def prepare_training_args(self) -> list:
        """å‡†å¤‡è®­ç»ƒå‚æ•°"""
        args = [
            "--stage", str(self.config["stage"]),
            "--model_name_or_path", str(self.config["model_name_or_path"]),
            "--dataset", str(self.config["dataset"]),
            "--template", str(self.config.get("template", "default")),
            "--finetuning_type", str(self.config["finetuning_type"]),
            "--output_dir", str(self.config["output_dir"]),
            "--overwrite_output_dir",
            "--per_device_train_batch_size", str(self.config["per_device_train_batch_size"]),
            "--gradient_accumulation_steps", str(self.config["gradient_accumulation_steps"]),
            "--lr_scheduler_type", str(self.config["lr_scheduler_type"]),
            "--logging_steps", str(self.config["logging_steps"]),
            "--save_steps", str(self.config["save_steps"]),
            "--learning_rate", str(self.config["learning_rate"]),
            "--num_train_epochs", str(self.config["num_train_epochs"]),
            "--max_grad_norm", str(self.config["max_grad_norm"]),
            "--val_size", str(self.config["val_size"]),
            "--evaluation_strategy", str(self.config["evaluation_strategy"]),
            "--eval_steps", str(self.config["eval_steps"]),
        ]
        
        # å¯é€‰å‚æ•°
        optional_bool_args = [
            "do_train", "load_best_model_at_end", "plot_loss", 
            "use_unsloth", "use_rslora"
        ]
        
        for arg in optional_bool_args:
            if self.config.get(arg, False):
                args.append(f"--{arg}")
        
        # LoRAç‰¹å®šå‚æ•°
        if self.config["finetuning_type"] == "lora":
            lora_params = [
                "lora_target", "lora_rank", "lora_alpha", "lora_dropout",
                "modules_to_save", "loraplus_lr_ratio"
            ]
            
            for param in lora_params:
                if param in self.config:
                    args.extend([f"--{param}", str(self.config[param])])
        
        # é‡åŒ–å‚æ•°
        if "quantization_bit" in self.config:
            args.extend(["--quantization_bit", str(self.config["quantization_bit"])])
        
        # å…¶ä»–å¯é€‰å‚æ•°
        other_params = [
            "max_samples", "cutoff_len", "warmup_ratio", "weight_decay",
            "adam_beta1", "adam_beta2", "adam_epsilon", "max_source_length",
            "max_target_length", "report_to"
        ]
        
        for param in other_params:
            if param in self.config:
                args.extend([f"--{param}", str(self.config[param])])
        
        return args
    
    def save_config(self):
        """ä¿å­˜è®­ç»ƒé…ç½®"""
        config_file = os.path.join(self.config["output_dir"], "training_config.yaml")
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
        logger.info(f"è®­ç»ƒé…ç½®å·²ä¿å­˜: {config_file}")
    
    def start_training(self) -> bool:
        """å¼€å§‹è®­ç»ƒ"""
        if not self.validate_config():
            return False
        
        self.save_config()
        
        # æ£€æŸ¥LLaMA Factoryæ˜¯å¦å®‰è£…
        try:
            import llamafactory
            logger.info(f"LLaMA Factoryç‰ˆæœ¬: {llamafactory.__version__}")
        except ImportError:
            logger.error("LLaMA Factoryæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œå®‰è£…è„šæœ¬")
            return False
        
        args = self.prepare_training_args()
        
        logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        logger.info(f"è®­ç»ƒå‚æ•°: {' '.join(args)}")
        
        self.start_time = time.time()
        
        try:
            # å¯¼å…¥å¹¶è¿è¡Œè®­ç»ƒ
            from llamafactory.train.tuner import run_exp
            
            # æ•è·è®­ç»ƒè¿‡ç¨‹
            logger.info("æ­£åœ¨å¯åŠ¨è®­ç»ƒè¿›ç¨‹...")
            run_exp(args)
            
            training_time = time.time() - self.start_time
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼è€—æ—¶: {training_time/3600:.2f} å°æ—¶")
            
            # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            self._generate_training_report(training_time)
            
            return True
            
        except KeyboardInterrupt:
            logger.warning("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            return False
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _generate_training_report(self, training_time: float):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report_file = os.path.join(self.config["output_dir"], "training_report.md")
        
        report_content = f"""
# è®­ç»ƒæŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **è®­ç»ƒå¼€å§‹æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.start_time))}
- **è®­ç»ƒç»“æŸæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **è®­ç»ƒè€—æ—¶**: {training_time/3600:.2f} å°æ—¶
- **æ¨¡å‹**: {self.config['model_name_or_path']}
- **æ•°æ®é›†**: {self.config['dataset']}
- **å¾®è°ƒç±»å‹**: {self.config['finetuning_type']}

## è®­ç»ƒé…ç½®
```yaml
{yaml.dump(self.config, default_flow_style=False, allow_unicode=True)}
```

## è¾“å‡ºæ–‡ä»¶
- æ¨¡å‹æƒé‡: `{self.config['output_dir']}/adapter_model.bin`
- é…ç½®æ–‡ä»¶: `{self.config['output_dir']}/adapter_config.json`
- è®­ç»ƒæ—¥å¿—: `{self.config['output_dir']}/trainer_log.jsonl`
- æŸå¤±æ›²çº¿: `{self.config['output_dir']}/training_loss.png`

## åç»­æ­¥éª¤
1. æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½
2. æ¨¡å‹åˆå¹¶ï¼šå°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
3. æ¨¡å‹éƒ¨ç½²ï¼šéƒ¨ç½²åˆ°æ¨ç†ç¯å¢ƒ

## ä½¿ç”¨ç¤ºä¾‹
```bash
# æ¨¡å‹æ¨ç†
llamafactory-cli chat \\
    --model_name_or_path {self.config['model_name_or_path']} \\
    --adapter_name_or_path {self.config['output_dir']} \\
    --template {self.config.get('template', 'default')}

# æ¨¡å‹åˆå¹¶
llamafactory-cli export \\
    --model_name_or_path {self.config['model_name_or_path']} \\
    --adapter_name_or_path {self.config['output_dir']} \\
    --template {self.config.get('template', 'default')} \\
    --finetuning_type {self.config['finetuning_type']} \\
    --export_dir ./merged_model
```
        """
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def create_sample_config():
    """åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    config = {
        "model_name_or_path": "THUDM/chatglm3-6b",
        "dataset": "alpaca_zh",
        "template": "chatglm3",
        "finetuning_type": "lora",
        "output_dir": "./saves/ChatGLM3-6B/lora/sample_train",
        
        # LoRAé…ç½®
        "lora_target": "query_key_value",
        "lora_rank": 64,
        "lora_alpha": 16,
        "lora_dropout": 0.1,
        "modules_to_save": "embed_tokens,lm_head",
        
        # è®­ç»ƒå‚æ•°
        "stage": "sft",
        "do_train": True,
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "lr_scheduler_type": "cosine",
        "logging_steps": 10,
        "save_steps": 1000,
        "learning_rate": 5e-5,
        "num_train_epochs": 3,
        "max_grad_norm": 1.0,
        "quantization_bit": 4,
        "use_unsloth": True,
        "val_size": 0.1,
        "evaluation_strategy": "steps",
        "eval_steps": 500,
        "load_best_model_at_end": True,
        "plot_loss": True,
        "report_to": "none"
    }
    
    config_file = "sample_training_config.yaml"
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    return config_file

def main():
    parser = argparse.ArgumentParser(description="LLaMA Factory æ¨¡å‹è®­ç»ƒå·¥å…·")
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument("--config", help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„ (YAML/JSON)")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--model", dest="model_name_or_path", help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--dataset", help="æ•°æ®é›†åç§°")
    parser.add_argument("--template", help="å¯¹è¯æ¨¡æ¿")
    parser.add_argument("--output_dir", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--finetuning_type", choices=["lora", "qlora", "full"], help="å¾®è°ƒç±»å‹")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, dest="per_device_train_batch_size", help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, help="å­¦ä¹ ç‡")
    parser.add_argument("--num_epochs", type=int, dest="num_train_epochs", help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--max_samples", type=int, help="æœ€å¤§æ ·æœ¬æ•°")
    
    # LoRAå‚æ•°
    parser.add_argument("--lora_rank", type=int, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, help="LoRA dropout")
    parser.add_argument("--lora_target", help="LoRAç›®æ ‡æ¨¡å—")
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--quantization_bit", type=int, choices=[4, 8], help="é‡åŒ–ä½æ•°")
    parser.add_argument("--create_config", action="store_true", help="åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶")
    parser.add_argument("--dry_run", action="store_true", help="ä»…éªŒè¯é…ç½®ï¼Œä¸æ‰§è¡Œè®­ç»ƒ")
    
    args = parser.parse_args()
    
    # åˆ›å»ºç¤ºä¾‹é…ç½®
    if args.create_config:
        create_sample_config()
        return
    
    # è½¬æ¢å‚æ•°ä¸ºå­—å…¸
    config_args = {k: v for k, v in vars(args).items() 
                   if v is not None and k not in ['config', 'create_config', 'dry_run']}
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ModelTrainer(args.config, **config_args)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    logger.info("è®­ç»ƒé…ç½®:")
    for key, value in trainer.config.items():
        logger.info(f"  {key}: {value}")
    
    # ä»…éªŒè¯é…ç½®
    if args.dry_run:
        if trainer.validate_config():
            logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        else:
            logger.error("âŒ é…ç½®éªŒè¯å¤±è´¥")
        return
    
    # å¼€å§‹è®­ç»ƒ
    success = trainer.start_training()
    
    if success:
        logger.info("ğŸ‰ è®­ç»ƒä»»åŠ¡å®Œæˆï¼")
        sys.exit(0)
    else:
        logger.error("âŒ è®­ç»ƒä»»åŠ¡å¤±è´¥ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main() 