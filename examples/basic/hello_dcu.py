#!/usr/bin/env python3
"""
DCUç¯å¢ƒéªŒè¯ç¨‹åº
éªŒè¯DCUè®¾å¤‡æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œå¹¶å±•ç¤ºåŸºæœ¬çš„DCUæ“ä½œ
"""

import torch
import time
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from common.dcu.device_manager import DCUDeviceManager
from common.utils.logger import get_logger

logger = get_logger(__name__)

def check_dcu_availability():
    """æ£€æŸ¥DCUè®¾å¤‡å¯ç”¨æ€§"""
    print("ğŸ” æ£€æŸ¥DCUè®¾å¤‡å¯ç”¨æ€§...")
    
    # æ£€æŸ¥PyTorchæ˜¯å¦æ”¯æŒCUDAï¼ˆDCUï¼‰
    if not torch.cuda.is_available():
        print("âŒ DCUè®¾å¤‡ä¸å¯ç”¨ï¼è¯·æ£€æŸ¥é©±åŠ¨å®‰è£…ã€‚")
        return False
    
    device_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {device_count} ä¸ªDCUè®¾å¤‡")
    
    # æ˜¾ç¤ºæ¯ä¸ªè®¾å¤‡çš„ä¿¡æ¯
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        print(f"   DCU {i}: {props.name}")
        print(f"   - æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
        print(f"   - è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    
    return True

def test_basic_operations():
    """æµ‹è¯•åŸºæœ¬çš„DCUæ“ä½œ"""
    print("\nğŸ§ª æµ‹è¯•åŸºæœ¬DCUæ“ä½œ...")
    
    device = torch.device('cuda:0')
    
    # åˆ›å»ºå¼ é‡
    print("   åˆ›å»ºå¼ é‡...")
    a = torch.randn(1000, 1000, device=device)
    b = torch.randn(1000, 1000, device=device)
    
    # çŸ©é˜µä¹˜æ³•
    print("   æ‰§è¡ŒçŸ©é˜µä¹˜æ³•...")
    start_time = time.time()
    c = torch.mm(a, b)
    torch.cuda.synchronize()  # ç­‰å¾…DCUæ“ä½œå®Œæˆ
    end_time = time.time()
    
    print(f"   âœ… çŸ©é˜µä¹˜æ³•å®Œæˆï¼Œè€—æ—¶: {(end_time - start_time)*1000:.2f} ms")
    print(f"   ç»“æœå½¢çŠ¶: {c.shape}")
    print(f"   ç»“æœå‡å€¼: {c.mean().item():.4f}")

def test_memory_management():
    """æµ‹è¯•æ˜¾å­˜ç®¡ç†"""
    print("\nğŸ’¾ æµ‹è¯•æ˜¾å­˜ç®¡ç†...")
    
    device = torch.device('cuda:0')
    
    # æ˜¾ç¤ºåˆå§‹æ˜¾å­˜çŠ¶æ€
    allocated = torch.cuda.memory_allocated(device) / 1024**3
    cached = torch.cuda.memory_reserved(device) / 1024**3
    print(f"   åˆå§‹æ˜¾å­˜ - å·²åˆ†é…: {allocated:.2f} GB, å·²ç¼“å­˜: {cached:.2f} GB")
    
    # åˆ†é…å¤§é‡æ˜¾å­˜
    print("   åˆ†é…æ˜¾å­˜...")
    tensors = []
    for i in range(10):
        tensor = torch.randn(1000, 1000, device=device)
        tensors.append(tensor)
    
    allocated = torch.cuda.memory_allocated(device) / 1024**3
    cached = torch.cuda.memory_reserved(device) / 1024**3
    print(f"   åˆ†é…åæ˜¾å­˜ - å·²åˆ†é…: {allocated:.2f} GB, å·²ç¼“å­˜: {cached:.2f} GB")
    
    # æ¸…ç†æ˜¾å­˜
    print("   æ¸…ç†æ˜¾å­˜...")
    del tensors
    torch.cuda.empty_cache()
    
    allocated = torch.cuda.memory_allocated(device) / 1024**3
    cached = torch.cuda.memory_reserved(device) / 1024**3
    print(f"   æ¸…ç†åæ˜¾å­˜ - å·²åˆ†é…: {allocated:.2f} GB, å·²ç¼“å­˜: {cached:.2f} GB")

def test_multi_device():
    """æµ‹è¯•å¤šè®¾å¤‡æ“ä½œ"""
    device_count = torch.cuda.device_count()
    if device_count < 2:
        print("\nâš ï¸  åªæœ‰ä¸€ä¸ªDCUè®¾å¤‡ï¼Œè·³è¿‡å¤šè®¾å¤‡æµ‹è¯•")
        return
    
    print(f"\nğŸ”— æµ‹è¯•å¤šè®¾å¤‡æ“ä½œï¼ˆ{device_count}ä¸ªè®¾å¤‡ï¼‰...")
    
    # åœ¨ä¸åŒè®¾å¤‡ä¸Šåˆ›å»ºå¼ é‡
    tensors = []
    for i in range(min(device_count, 4)):  # æœ€å¤šæµ‹è¯•4ä¸ªè®¾å¤‡
        device = torch.device(f'cuda:{i}')
        tensor = torch.randn(500, 500, device=device)
        tensors.append(tensor)
        print(f"   åœ¨DCU {i}ä¸Šåˆ›å»ºå¼ é‡: {tensor.shape}")
    
    # è®¾å¤‡é—´æ•°æ®ä¼ è¾“
    if len(tensors) >= 2:
        print("   æµ‹è¯•è®¾å¤‡é—´æ•°æ®ä¼ è¾“...")
        start_time = time.time()
        tensor_copy = tensors[0].to('cuda:1')
        torch.cuda.synchronize()
        end_time = time.time()
        print(f"   âœ… æ•°æ®ä¼ è¾“å®Œæˆï¼Œè€—æ—¶: {(end_time - start_time)*1000:.2f} ms")

def benchmark_performance():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    device = torch.device('cuda:0')
    sizes = [512, 1024, 2048, 4096]
    
    print("   çŸ©é˜µå¤§å° | è€—æ—¶(ms) | GFLOPS")
    print("   ---------|----------|--------")
    
    for size in sizes:
        # åˆ›å»ºéšæœºçŸ©é˜µ
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        # é¢„çƒ­
        for _ in range(3):
            torch.mm(a, b)
        torch.cuda.synchronize()
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        for _ in range(10):
            c = torch.mm(a, b)
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10 * 1000  # ms
        flops = 2 * size**3  # çŸ©é˜µä¹˜æ³•çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°
        gflops = flops / (avg_time / 1000) / 1e9
        
        print(f"   {size:8d} | {avg_time:8.2f} | {gflops:7.1f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ DCU-in-Action ç¯å¢ƒéªŒè¯ç¨‹åº")
    print("=" * 50)
    
    # æ£€æŸ¥DCUå¯ç”¨æ€§
    if not check_dcu_availability():
        sys.exit(1)
    
    try:
        # åŸºæœ¬æ“ä½œæµ‹è¯•
        test_basic_operations()
        
        # æ˜¾å­˜ç®¡ç†æµ‹è¯•
        test_memory_management()
        
        # å¤šè®¾å¤‡æµ‹è¯•
        test_multi_device()
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        benchmark_performance()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DCUç¯å¢ƒå·¥ä½œæ­£å¸¸ã€‚")
        print("\nğŸ“š æ¥ä¸‹æ¥æ‚¨å¯ä»¥ï¼š")
        print("   1. è¿è¡Œè®­ç»ƒç¤ºä¾‹: cd examples/training && python train_example.py")
        print("   2. å°è¯•æ¨¡å‹å¾®è°ƒ: cd examples/finetuning && python finetune_example.py")
        print("   3. å¯åŠ¨æ¨ç†æœåŠ¡: cd examples/inference && python inference_server.py")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"DCUæµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main() 