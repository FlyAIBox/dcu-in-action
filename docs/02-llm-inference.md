# æµ·å…‰DCUå¤§æ¨¡å‹æ¨ç†å®æˆ˜æ•™ç¨‹

## ğŸ“‹ ç›®å½•
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ¨ç†æ¡†æ¶é€‰æ‹©](#æ¨ç†æ¡†æ¶é€‰æ‹©)
- [æ¨¡å‹éƒ¨ç½²](#æ¨¡å‹éƒ¨ç½²)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å®é™…åº”ç”¨](#å®é™…åº”ç”¨)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## ğŸš€ ç¯å¢ƒå‡†å¤‡

### 1. DCUç¯å¢ƒæ£€æŸ¥

åœ¨å¼€å§‹æ¨ç†ä¹‹å‰ï¼Œç¡®ä¿DCUç¯å¢ƒæ­£ç¡®é…ç½®ï¼š

```bash
# æ£€æŸ¥DCUè®¾å¤‡çŠ¶æ€
hy-smi

# æŸ¥çœ‹DTKç‰ˆæœ¬
dtk-config --version

# éªŒè¯PyTorch+ROCmç¯å¢ƒ
python -c "import torch; print(f'DCUå¯ç”¨: {torch.cuda.is_available()}')"
python -c "import torch; print(f'DCUæ•°é‡: {torch.cuda.device_count()}')"
```

### 2. ä¾èµ–å®‰è£…

```bash
# å®‰è£…æ¨ç†ç›¸å…³ä¾èµ–
pip install transformers>=4.35.0
pip install accelerate>=0.24.0
pip install bitsandbytes>=0.41.0
pip install optimum>=1.15.0

# å®‰è£…æ¨ç†åŠ é€Ÿæ¡†æ¶
pip install vllm  # é«˜æ€§èƒ½æ¨ç†å¼•æ“
pip install text-generation-webui  # Webç•Œé¢
```

---

## ğŸ› ï¸ æ¨ç†æ¡†æ¶é€‰æ‹©

### 1. TransformersåŸç”Ÿæ¨ç†

é€‚ç”¨äºå°è§„æ¨¡æ¨¡å‹å’Œå®éªŒåœºæ™¯ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "THUDM/chatglm3-6b"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# æ¨ç†
prompt = "ä»‹ç»ä¸€ä¸‹æµ·å…‰DCUåŠ é€Ÿå¡çš„ä¼˜åŠ¿"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 2. vLLMé«˜æ€§èƒ½æ¨ç†

é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒå’Œé«˜å¹¶å‘åœºæ™¯ï¼š

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–vLLMå¼•æ“
llm = LLM(
    model="Qwen/Qwen-7B-Chat",
    tensor_parallel_size=2,  # å¤šå¡å¹¶è¡Œ
    gpu_memory_utilization=0.9,
    dtype="float16"
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=512
)

# æ‰¹é‡æ¨ç†
prompts = [
    "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
    "ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€",
    "æè¿°æœºå™¨å­¦ä¹ çš„åº”ç”¨"
]

outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    print(f"è¾“å…¥: {output.prompt}")
    print(f"è¾“å‡º: {output.outputs[0].text}")
    print("-" * 50)
```

### 3. ä½¿ç”¨APIæœåŠ¡

éƒ¨ç½²æ¨ç†APIæœåŠ¡ï¼š

```python
# FastAPIæ¨ç†æœåŠ¡
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

# å…¨å±€åŠ è½½æ¨¡å‹
model = None
tokenizer = None

class ChatRequest(BaseModel):
    message: str
    max_tokens: int = 512
    temperature: float = 0.7

@app.on_event("startup")
async def load_model():
    global model, tokenizer
    model_name = "baichuan-inc/Baichuan2-7B-Chat"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

@app.post("/chat")
async def chat(request: ChatRequest):
    inputs = tokenizer(request.message, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_tokens,
            temperature=request.temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}

# å¯åŠ¨å‘½ä»¤: uvicorn inference_api:app --host 0.0.0.0 --port 8000
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ··åˆç²¾åº¦æ¨ç†

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ä½¿ç”¨FP16ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/DialoGPT-medium",
    torch_dtype=torch.float16,
    device_map="auto"
)

# ä½¿ç”¨BF16ç²¾åº¦ï¼ˆæ¨èï¼‰
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/DialoGPT-medium",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

### 2. é‡åŒ–æ¨ç†

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# 4bité‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-chat-hf",
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 3. KVç¼“å­˜ä¼˜åŒ–

```python
from transformers import AutoModelForCausalLM

# å¯ç”¨KVç¼“å­˜ä¼˜åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "gpt2-medium",
    torch_dtype=torch.float16,
    device_map="auto",
    use_cache=True  # å¯ç”¨KVç¼“å­˜
)

# å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œä½¿ç”¨past_key_values
past_key_values = None
for i in range(5):  # å¤šè½®å¯¹è¯
    inputs = tokenizer(f"ç¬¬{i+1}è½®å¯¹è¯å†…å®¹", return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            past_key_values=past_key_values,
            use_cache=True
        )
    past_key_values = outputs.past_key_values
```

### 4. æ‰¹å¤„ç†ä¼˜åŒ–

```python
def batch_inference(model, tokenizer, prompts, batch_size=4):
    results = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i+batch_size]
        
        # æ‰¹é‡ç¼–ç 
        inputs = tokenizer(
            batch, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=512
        ).to("cuda")
        
        # æ‰¹é‡æ¨ç†
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=True,
                temperature=0.7
            )
        
        # è§£ç ç»“æœ
        batch_results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        results.extend(batch_results)
    
    return results
```

---

## ğŸ“± å®é™…åº”ç”¨åœºæ™¯

### 1. èŠå¤©æœºå™¨äºº

```python
class ChatBot:
    def __init__(self, model_name="THUDM/chatglm3-6b"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        self.chat_history = []
    
    def chat(self, user_input):
        # æ„å»ºå¯¹è¯å†å²
        self.chat_history.append(f"ç”¨æˆ·: {user_input}")
        context = "\n".join(self.chat_history[-10:])  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
        
        # ç”Ÿæˆå›å¤
        inputs = self.tokenizer(context, return_tensors="pt").to("cuda")
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        assistant_reply = response.split("åŠ©æ‰‹:")[-1].strip()
        
        self.chat_history.append(f"åŠ©æ‰‹: {assistant_reply}")
        return assistant_reply

# ä½¿ç”¨ç¤ºä¾‹
bot = ChatBot()
while True:
    user_input = input("ç”¨æˆ·: ")
    if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
        break
    reply = bot.chat(user_input)
    print(f"åŠ©æ‰‹: {reply}")
```

### 2. æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class DocumentQA:
    def __init__(self, model_name="THUDM/chatglm3-6b"):
        # åŠ è½½é—®ç­”æ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å‘é‡æ¨¡å‹
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.documents = []
        self.embeddings = None
        self.index = None
    
    def add_documents(self, docs):
        self.documents.extend(docs)
        # è®¡ç®—æ–‡æ¡£å‘é‡
        doc_embeddings = self.embedding_model.encode(docs)
        
        if self.embeddings is None:
            self.embeddings = doc_embeddings
        else:
            self.embeddings = np.vstack([self.embeddings, doc_embeddings])
        
        # æ„å»ºFAISSç´¢å¼•
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(self.embeddings.astype('float32'))
    
    def search_documents(self, query, top_k=3):
        query_embedding = self.embedding_model.encode([query])
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        relevant_docs = [self.documents[i] for i in indices[0]]
        return relevant_docs
    
    def answer_question(self, question):
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.search_documents(question)
        context = "\n".join(relevant_docs)
        
        # æ„å»ºæç¤º
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
        
        # ç”Ÿæˆç­”æ¡ˆ
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.3,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return answer.split("å›ç­”ï¼š")[-1].strip()

# ä½¿ç”¨ç¤ºä¾‹
qa_system = DocumentQA()
documents = [
    "æµ·å…‰DCUæ˜¯ä¸­å›½è‡ªä¸»ç ”å‘çš„GPUåŠ é€Ÿè®¡ç®—èŠ¯ç‰‡...",
    "PyTorchæ˜¯Facebookå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶...",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯..."
]
qa_system.add_documents(documents)

question = "ä»€ä¹ˆæ˜¯æµ·å…‰DCUï¼Ÿ"
answer = qa_system.answer_question(question)
print(f"é—®é¢˜: {question}")
print(f"ç­”æ¡ˆ: {answer}")
```

### 3. ä»£ç ç”ŸæˆåŠ©æ‰‹

```python
class CodeGenerator:
    def __init__(self, model_name="microsoft/CodeGPT-small-py"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
    
    def generate_code(self, description, language="python"):
        prompt = f"""# ä»»åŠ¡æè¿°ï¼š{description}
# ç¼–ç¨‹è¯­è¨€ï¼š{language}
# ä»£ç å®ç°ï¼š

"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.2,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                stop_strings=["# ä»»åŠ¡æè¿°", "```"]
            )
        
        code = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return code.split("# ä»£ç å®ç°ï¼š")[-1].strip()

# ä½¿ç”¨ç¤ºä¾‹
code_gen = CodeGenerator()
description = "å®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"
code = code_gen.generate_code(description)
print(f"ç”Ÿæˆçš„ä»£ç ï¼š\n{code}")
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### 1. å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### é”™è¯¯1ï¼šCUDA out of memory
```bash
# é”™è¯¯ä¿¡æ¯
RuntimeError: CUDA out of memory. Tried to allocate XX GB

# è§£å†³æ–¹æ¡ˆ
1. å‡å°‘æ‰¹æ¬¡å¤§å°
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
3. å¯ç”¨æ··åˆç²¾åº¦
4. ä½¿ç”¨æ¨¡å‹é‡åŒ–
5. å¢åŠ è™šæ‹Ÿå†…å­˜
```

#### é”™è¯¯2ï¼šæ¨¡å‹åŠ è½½å¤±è´¥
```bash
# é”™è¯¯ä¿¡æ¯
OSError: Can't load the model

# è§£å†³æ–¹æ¡ˆ
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
3. éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
4. æ¸…é™¤ç¼“å­˜åé‡æ–°ä¸‹è½½
```

#### é”™è¯¯3ï¼šDCUè®¾å¤‡ä¸å¯ç”¨
```bash
# é”™è¯¯ä¿¡æ¯
AssertionError: DCU is not available

# è§£å†³æ–¹æ¡ˆ
1. æ£€æŸ¥DCUé©±åŠ¨å®‰è£…
2. éªŒè¯DTKç¯å¢ƒå˜é‡
3. é‡å¯DCUæœåŠ¡
4. æ£€æŸ¥è®¾å¤‡æƒé™
```

### 2. æ€§èƒ½è°ƒä¼˜å»ºè®®

```python
# æ€§èƒ½ç›‘æ§å·¥å…·
import time
import psutil
import torch

class PerformanceMonitor:
    def __init__(self):
        self.start_time = None
        self.end_time = None
    
    def start(self):
        torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
        torch.cuda.synchronize()  # åŒæ­¥
        self.start_time = time.time()
    
    def end(self):
        torch.cuda.synchronize()
        self.end_time = time.time()
    
    def get_stats(self):
        # è®¡ç®—ç”¨æ—¶
        inference_time = self.end_time - self.start_time
        
        # GPUå†…å­˜ä½¿ç”¨
        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        gpu_memory_max = torch.cuda.max_memory_allocated() / 1024**3
        
        # CPUå’Œç³»ç»Ÿå†…å­˜
        cpu_percent = psutil.cpu_percent()
        memory_info = psutil.virtual_memory()
        
        return {
            "inference_time": f"{inference_time:.3f}s",
            "gpu_memory_current": f"{gpu_memory:.2f}GB",
            "gpu_memory_peak": f"{gpu_memory_max:.2f}GB",
            "cpu_usage": f"{cpu_percent:.1f}%",
            "system_memory": f"{memory_info.percent:.1f}%"
        }

# ä½¿ç”¨ç¤ºä¾‹
monitor = PerformanceMonitor()
monitor.start()

# æ‰§è¡Œæ¨ç†ä»»åŠ¡
# your_inference_code_here()

monitor.end()
stats = monitor.get_stats()
print("æ€§èƒ½ç»Ÿè®¡:", stats)
```

### 3. è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# è®¾ç½®ç¯å¢ƒå˜é‡è¿›è¡Œè°ƒè¯•
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥æ‰§è¡Œ
os.environ['TORCH_USE_CUDA_DSA'] = '1'    # æ£€æµ‹å†…å­˜é”™è¯¯

# ä½¿ç”¨torch.autograd.profileråˆ†ææ€§èƒ½
with torch.autograd.profiler.profile(use_cuda=True) as prof:
    # æ‰§è¡Œæ¨ç†ä»£ç 
    output = model.generate(**inputs)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨ç†æ¡†æ¶ | æ¨¡å‹å¤§å° | æ¨ç†é€Ÿåº¦ | å†…å­˜å ç”¨ | å¹¶å‘æ”¯æŒ |
|----------|----------|----------|----------|----------|
| Transformers | 7B | 50 tokens/s | 14GB | ä½ |
| vLLM | 7B | 150 tokens/s | 16GB | é«˜ |
| TensorRT-LLM | 7B | 200 tokens/s | 12GB | ä¸­ |
| é‡åŒ–ç‰ˆæœ¬ | 7B | 120 tokens/s | 8GB | ä¸­ |

---

## ğŸ¯ æœ€ä½³å®è·µ

1. **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚å¤§å°çš„æ¨¡å‹
2. **ç²¾åº¦å¹³è¡¡**ï¼šåœ¨ç²¾åº¦å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹
3. **æ‰¹å¤„ç†**ï¼šå……åˆ†åˆ©ç”¨æ‰¹å¤„ç†æé«˜ååé‡
4. **ç¼“å­˜ç­–ç•¥**ï¼šåˆç†ä½¿ç”¨KVç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
5. **ç›‘æ§è°ƒä¼˜**ï¼šæŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡å¹¶ä¼˜åŒ–

---

## ğŸ“š å‚è€ƒèµ„æº

- [æµ·å…‰DCUå¼€å‘è€…æ–‡æ¡£](https://developer.sourcefind.cn/)
- [Transformerså®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [vLLMé¡¹ç›®æ–‡æ¡£](https://vllm.readthedocs.io/)
- [PyTorchæ€§èƒ½è°ƒä¼˜æŒ‡å—](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)

---

*æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿æäº¤å»ºè®®å’Œæ”¹è¿›ï¼*
