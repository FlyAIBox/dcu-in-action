#!/bin/bash

# Dockerå®¹å™¨å…¥å£è„šæœ¬
# ç”¨äºå¯åŠ¨DCUå¼€å‘ç¯å¢ƒçš„å„ç§æœåŠ¡

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
show_welcome() {
    cat << 'EOF'
================================================================
          ğŸš€ æµ·å…‰DCUåŠ é€Ÿå¡å®æˆ˜ç¯å¢ƒ ğŸš€
================================================================
   
   ç¯å¢ƒä¿¡æ¯:
   - åŸºç¡€é•œåƒ: DCU PyTorch 2.1.0
   - Python: 3.10
   - DTK: 25.04
   - é¡¹ç›®ç›®å½•: /workspace/dcu-in-action
   
   å¯ç”¨æœåŠ¡:
   - Jupyter Lab: http://localhost:8888
   - FastAPIæœåŠ¡: http://localhost:8000
   - Gradioç•Œé¢: http://localhost:7860
   
   å¿«é€Ÿå¼€å§‹:
   - æ£€æŸ¥ç¯å¢ƒ: python examples/llm-inference/simple_test.py
   - ç›‘æ§DCU: python scripts/utils/monitor_performance.py monitor
   - å¯åŠ¨æ¨ç†: python examples/llm-inference/vllm_server.py
   
================================================================
EOF
}

# æ£€æŸ¥DCUç¯å¢ƒ
check_dcu_env() {
    log_info "æ£€æŸ¥DCUç¯å¢ƒ..."
    
    # æ£€æŸ¥DCUè®¾å¤‡
    if command -v hy-smi >/dev/null 2>&1; then
        log_success "hy-smi å¯ç”¨"
        hy-smi || log_warning "æ— æ³•è·å–DCUä¿¡æ¯ï¼Œå¯èƒ½éœ€è¦è®¾å¤‡æƒé™"
    else
        log_warning "hy-smi ä¸å¯ç”¨"
    fi
    
    # æ£€æŸ¥PyTorch DCUæ”¯æŒ
    python -c "
import torch
print('PyTorchç‰ˆæœ¬:', torch.__version__)
print('DCUå¯ç”¨:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('DCUæ•°é‡:', torch.cuda.device_count())
    for i in range(torch.cuda.device_count()):
        print(f'DCU {i}:', torch.cuda.get_device_name(i))
else:
    print('æ³¨æ„: DCUä¸å¯ç”¨ï¼Œå¯èƒ½éœ€è¦æ˜ å°„è®¾å¤‡æˆ–è®¾ç½®æƒé™')
"
}

# è®¾ç½®æƒé™
setup_permissions() {
    log_info "è®¾ç½®æ–‡ä»¶æƒé™..."
    
    # ç¡®ä¿è„šæœ¬å¯æ‰§è¡Œ
    find /workspace/dcu-in-action/scripts -name "*.sh" -exec chmod +x {} \;
    find /workspace/dcu-in-action/examples -name "*.py" -exec chmod +x {} \;
    
    log_success "æƒé™è®¾ç½®å®Œæˆ"
}

# å¯åŠ¨JupyteræœåŠ¡
start_jupyter() {
    if [ "$START_JUPYTER" = "true" ]; then
        log_info "å¯åŠ¨Jupyter Lab..."
        cd /workspace/dcu-in-action
        nohup jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root \
            --NotebookApp.token='' --NotebookApp.password='' \
            --notebook-dir=/workspace/dcu-in-action > /tmp/jupyter.log 2>&1 &
        log_success "Jupyter Lab å·²å¯åŠ¨ï¼Œè®¿é—®: http://localhost:8888"
    fi
}

# å¯åŠ¨ç›‘æ§æœåŠ¡
start_monitoring() {
    if [ "$START_MONITOR" = "true" ]; then
        log_info "å¯åŠ¨DCUç›‘æ§æœåŠ¡..."
        cd /workspace/dcu-in-action
        nohup python scripts/utils/monitor_performance.py monitor -i 5 -j > /tmp/monitor.log 2>&1 &
        log_success "DCUç›‘æ§æœåŠ¡å·²å¯åŠ¨"
    fi
}

# å¯åŠ¨æ¨ç†æœåŠ¡
start_inference_server() {
    if [ "$START_INFERENCE" = "true" ] && [ -n "$MODEL_NAME" ]; then
        log_info "å¯åŠ¨æ¨ç†æœåŠ¡: $MODEL_NAME"
        cd /workspace/dcu-in-action
        nohup python examples/llm-inference/vllm_server.py \
            --mode server --model "$MODEL_NAME" \
            --host 0.0.0.0 --port 8000 > /tmp/inference.log 2>&1 &
        log_success "æ¨ç†æœåŠ¡å·²å¯åŠ¨ï¼Œè®¿é—®: http://localhost:8000"
    fi
}

# è¿è¡Œç¯å¢ƒæ£€æŸ¥
run_env_check() {
    if [ "$RUN_ENV_CHECK" = "true" ]; then
        log_info "è¿è¡Œç¯å¢ƒæ£€æŸ¥..."
        cd /workspace/dcu-in-action
        bash scripts/setup/check_dcu_environment.sh
    fi
}

# ä¸»å‡½æ•°
main() {
    show_welcome
    
    # æ£€æŸ¥DCUç¯å¢ƒ
    check_dcu_env
    
    # è®¾ç½®æƒé™
    setup_permissions
    
    # è¿è¡Œç¯å¢ƒæ£€æŸ¥
    run_env_check
    
    # å¯åŠ¨æœåŠ¡
    start_jupyter
    start_monitoring
    start_inference_server
    
    # æ ¹æ®ä¼ å…¥çš„å‚æ•°æ‰§è¡Œä¸åŒæ“ä½œ
    case "${1:-interactive}" in
        "jupyter")
            log_info "å¯åŠ¨Jupyteræ¨¡å¼..."
            cd /workspace/dcu-in-action
            exec jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root \
                --NotebookApp.token='' --NotebookApp.password='' \
                --notebook-dir=/workspace/dcu-in-action
            ;;
        "bash")
            log_info "å¯åŠ¨Bashæ¨¡å¼..."
            cd /workspace/dcu-in-action
            exec /bin/bash
            ;;
        "train")
            log_info "å¯åŠ¨è®­ç»ƒæ¨¡å¼..."
            cd /workspace/dcu-in-action
            if [ -n "$2" ]; then
                exec python "$2" "${@:3}"
            else
                log_error "è¯·æŒ‡å®šè®­ç»ƒè„šæœ¬è·¯å¾„"
                exit 1
            fi
            ;;
        "inference")
            log_info "å¯åŠ¨æ¨ç†æ¨¡å¼..."
            cd /workspace/dcu-in-action
            MODEL_NAME=${2:-"Qwen/Qwen-7B-Chat"}
            exec python examples/llm-inference/vllm_server.py \
                --mode server --model "$MODEL_NAME" --host 0.0.0.0 --port 8000
            ;;
        "monitor")
            log_info "å¯åŠ¨ç›‘æ§æ¨¡å¼..."
            cd /workspace/dcu-in-action
            exec python scripts/utils/monitor_performance.py monitor
            ;;
        "interactive"|*)
            log_info "å¯åŠ¨äº¤äº’æ¨¡å¼..."
            cd /workspace/dcu-in-action
            
            # æ˜¾ç¤ºå¯ç”¨å‘½ä»¤
            echo ""
            log_info "å¯ç”¨å‘½ä»¤:"
            echo "  python examples/llm-inference/simple_test.py     # ç¯å¢ƒæµ‹è¯•"
            echo "  python examples/llm-inference/chatglm_inference.py --mode chat  # èŠå¤©æµ‹è¯•"
            echo "  python scripts/utils/monitor_performance.py monitor  # DCUç›‘æ§"
            echo "  jupyter lab --ip=0.0.0.0 --port=8888 --allow-root   # å¯åŠ¨Jupyter"
            echo ""
            
            # è¿›å…¥bash
            exec /bin/bash
            ;;
    esac
}

# ä¿¡å·å¤„ç†
trap 'log_info "æ­£åœ¨åœæ­¢æœåŠ¡..."; pkill -f jupyter; pkill -f python; exit 0' SIGTERM SIGINT

# æ‰§è¡Œä¸»å‡½æ•°
main "$@" 